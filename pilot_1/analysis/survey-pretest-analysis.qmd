---
title: "survey-pretest-analysis"
format: html
bibliography: mocareferences.bib
citation-location: document
reference-location: section
editor: visual
---

```{r}
#| label: Data Fetching. Load R packages
#| include: false 

library(osfr) 
library(dplyr)
library(ez) 
library(tidyverse) 
library(gt)
library(gtsummary)
library(kableExtra)
library(stringr)
library(ggridges)
library(lubridate)  # for date handling
library(psych) # for factor analysis 
library(pheatmap) # for factor analysis heat map 
library(corrplot) # for correlation matrix 
library(broom) # for correlation tests
library(purrr) # for correlation tests 
library(likert) # for likert plots 
library(factoextra)
library(ggfortify)
library(car) 
library(GPArotation) # for factor analysis 
# for mlr
# library(GGally)
# library(car)
# library(MASS)
# library(glmnet)

```

```{r}
#| label: Prolific demographics data
#| include: false 

## add which round of the pilot it was by reading in demographics files so we know completion dates

#read in prolific demographics files 
pretest_demo_df <- read_csv("prolific_export_685c48bc2d211110b035b997.csv") %>%
  mutate(Age = as.numeric(Age))

#merge files (if multiple)
merged_prolific_demographics_df <- pretest_demo_df
  #bind_rows(pretest_demo_df) 

#clean df
cleaned_prolific_demographics_df <- merged_prolific_demographics_df %>%
# make name the same as in data csvs so merging is easier
rename("prolific_subject_id" = "Participant id") %>%
# filter out people who didn't finish
filter(Status != "RETURNED") %>%
#rename "Started at" to make grepl function easier to use later on
rename("start" = "Started at") %>%
# make time more readable but keep the seconds version for analysis stuff later
mutate(`time_taken_overall_minutes` = as.numeric(`Time taken`) / 60) %>%
#remove the no code and unknown code people 
  filter(`Completion code` == "CVWBBA5S")

str(cleaned_prolific_demographics_df)

```

```{r}
#| label: Retrieve data for the specific date of the willingness pretest from our OSF project. Loading into a data folder, which  we manually create
#| include: false 
# 
# # list all files
# files <- osf_retrieve_node("x32pv") %>%
#   osf_ls_files(n_max = 1000)
# 
# # originally it wasn't downloading: 
# #csv 1ftd0hhjnr.csv which has subject id 5f82567f1e720a3842580d23
# #csv fn1vmsaka7 which has subject id 66d8837
# #csv zxynjls3ql.csv which has subject id 5ffb8c5b6... 
# 
# # filter for the correct date
# files_filtered <- files %>%
#   mutate(date_created = map_chr(meta, ~ as.character(.x$attributes$date_created)),
#     date_created = as_date(date_created)
#   ) %>%
#   filter(date_created %in% as_date(c("2025-09-16", "2025-09-29", "2025-09-30")))

# download files for the correct date 
#osf_download(files_filtered, path = "data", conflicts = "skip")
```

```{r}
#| label: Bind csv's
#| include: false 
# 
#  osf_csv_filenames <- list.files(path = "data/")
#  osf_csv_filenames_filtered <- osf_csv_filenames[osf_csv_filenames %in% files_filtered$name]

# LOAD IN DATA FROM FOLDER IN THIS R PROJ SINCE RETRIEVAL NODE IS WEIRD 

file_names <- list.files("osfstorage-archive/", pattern = "\\.csv$", full.names = FALSE)

 raw_dfs_tibbles <- map(file_names, ~read_csv(file.path("osfstorage-archive/", .)))
 rawwwwwww_df <- bind_rows(raw_dfs_tibbles)

```

```{r}
#| label: Verifying that we have all data 
#| include: false 

# verify that we have the data for all the people by making sure that the data df has as many subs as the demo df

# difference <- setdiff(
#   cleaned_prolific_demographics_df$prolific_subject_id,
#   rawwwwwww_df$prolific_subject_id
# )
# 
# if (length(difference) > 0) {
#   print("ALERT: MISSING DATA FROM PARTICIPANTS:")
#   print(difference)
# }

#664287debe4750708ea0ecf7 has our url as their completion code 
#6173c0bb92e149b4127563fe has their submission id as completion code 
# the rest are completely normal, idk why they're not downloading:
# not sure what is going on with "5dceca2541afe801a32e27ac", 

#qeg3nvqqbc.csv which has "646792a49a85f35e7a7f5169",
#omkd7wc2nm.csv which has "5e07d174da6bad1342bafab2", 
#08yq5tsl91.csv which has "67ac21373a76570ef0a5c0d3" , 
#4y1akeuxu9.csv which has "5dac4dad5e53cc001499f1f6", 
#9vtg72o38k.csv which has "6296bef3248bf76cc113926b"

```

```{r}
#| label: Merge prolific demos + create a condition type column
#| include: false 

raw_df <- rawwwwwww_df %>%
  # inner join with the demographics df from pilot 2 so we can eliminate pilot 1 subjects
  right_join(cleaned_prolific_demographics_df, by = "prolific_subject_id") %>%
  # remove unnecessary syntax from topicChoiceAsString for readability later
  mutate(topicChoiceAsString = gsub('\\{"row":"', '', topicChoiceAsString)) %>%
  mutate(topicChoiceAsString = gsub('"}', '', topicChoiceAsString)) %>%
  # fill condition column and row and topic strings
  group_by(prolific_subject_id) %>%
  fill(topicChoiceAsString, .direction = "downup") %>%
  ungroup() 

# number of subjects in data 
total_num_subjects_in_data <- nrow(raw_df %>%
  group_by(prolific_subject_id) %>%
  summarize(total_num_subjects_in_data = n()))

```

```{r}
#| label: Clean the survey df
#| include: false

surveys_cleaning1_df <- raw_df %>%
  # Filter for only surveys
  filter(trial_type == "survey") %>%
  # Remove control interventions (these are coded as surveys)
  filter(phase != "control-intervention") %>%
  # Remove the matrix question titles because they mess with separating questions from answers
  # openmindedness and unwillingness reason
  mutate(across(everything(), ~ gsub(',"openmindedness":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"openmindedness":', '', .))) %>%
  mutate(across(everything(), ~ gsub(',"unwillingness-reason":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"unwillingness-reason":', '', .))) %>%
  # republican/democrat (pre-intervention)
  mutate(across(everything(), ~ gsub(',"rating-republicans":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub(',"rating-democrats":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-republicans":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans":null', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats":null', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-republicans":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats":', '', .))) %>%
  # republican/democrat (post-intervention)
  mutate(across(everything(), ~ gsub('"rating-republicans-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-republicans-post":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats-post":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans-post":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats-post":', '', .))) %>%
  # the name of the question in 2.1 run of the experiment
  mutate(across(everything(), ~ gsub('topicChoice":', '', .))) %>%
  # the name of the question in future runs of the experiment
  mutate(across(everything(), ~ gsub('topic":', '', .))) %>%
  mutate(across(everything(), ~ gsub(',null,', '', .))) 

  
surveys_cleaning2_df <- surveys_cleaning1_df %>%
  # Remove weird characters but don't remove quotations because we need them for separating questions
  mutate(across(everything(), ~ gsub('}', '', .))) %>%
  mutate(across(everything(), ~ gsub('\\{', '', .))) %>%
  mutate(across(everything(), ~ gsub('\\[', '', .))) %>%
  mutate(across(everything(), ~ gsub('\\]', '', .))) %>%
  # Make each question its own row
  separate_rows(response, sep = ',"') %>%
  # Remove any leftover quotation marks
  mutate(across(everything(), ~ gsub('"', '', .))) %>%
  # Move answers to a new column
  separate(response, into = c("question", "answer"), sep = ":", extra = "merge", fill = "right") %>%
  # Remove all remaining questions that were answered with null (like the slider questions, because the slider 'placeholder' questions are where people actually answer the slider questions)
  filter(!is.na(answer) & answer != 'null' & answer != 'null,null')

```

```{r}
#| label: Reverse code necessary items 
#| include: false 

# reverse code open-mindedness items 

surveys_cleaning3_df <- surveys_cleaning2_df %>%
    mutate(answer = ifelse(
     grepl("@R@", question),
     8 - as.numeric(answer),
     (answer)
   ))

```

```{r}
#| label: Remove unnecessary columns 
#| include: false 

surveys_df <- surveys_cleaning3_df %>%
  select(-c(stimulus, trial_type, trial_index, time_elapsed, internal_node_id, study_id, session_id, unwillingness_yes_rows, "Submission id", Status, "Custom study tncs accepted at", "Reviewed at", "Archived at", "Completion code", "Total approvals"))

willingness <- surveys_df %>%
  filter(question == "commitment-check")
```

```{r}
#| label: Create a df that flags people who failed the attention check
#| include: false

#see who passes the check
attention_check <- surveys_df %>%
  filter(question == "attention-check-slider-pre-placeholder") %>%
  mutate(flag = case_when(
    sum(answer == "0.00") == 0 ~ "fail (single)", # single fail means we exclude their data but we still have to pay them on prolific 
    sum(answer == "0.00") == 1 ~ "pass", #good subject :3
    TRUE ~ NA_character_
  )) %>%
  ungroup() %>%
  select(flag, question, answer, prolific_subject_id) 

#get list of all subject id's so we can see which subjects didn't even answer the question (the sliders aren't recorded in our df if the subject doesn't move them. they naturally sit at 50.) 
all_ids <- surveys_df %>%
  distinct(prolific_subject_id)

#filter for the question of interest
attention_ids <- surveys_df %>%
  filter(question == "attention-check-slider-pre-placeholder") %>%
  distinct(prolific_subject_id) 

#see who's missing
missing_ids <- setdiff(all_ids$prolific_subject_id, attention_ids$prolific_subject_id)

#make a table for these missing people 
missing_rows <- tibble(
  flag = "fail (single)",
  question = NA_character_,
  answer = NA_character_,
  prolific_subject_id = missing_ids,
  time_taken_overall_minutes = NA_real_
)

# combine those who answered and those who didn't answer
attention_check <- bind_rows(attention_check, missing_rows)

# extract value for text 
total_failed_attention_checks <- attention_check %>%
  # excluding people for both fails
  filter(flag == 'fail (single)') %>%
  summarize(total_failed_attention_checks = n_distinct(prolific_subject_id)) %>%
  pull(total_failed_attention_checks)

#extract list of ids so we can remove the data
failed_attention_check_ids <- attention_check %>%
  filter(flag == "fail (single)") %>%
  distinct(prolific_subject_id) %>%
  pull(prolific_subject_id)
```

```{r}
#| label: Remove subs who failed attention check 
#| include: false

# remove bad subs
raw_df <- raw_df %>%
  filter(!(prolific_subject_id %in% failed_attention_check_ids))
    
surveys_df <- surveys_df %>%
  filter(!prolific_subject_id %in% failed_attention_check_ids) 

total_num_clean_subjects_in_data <- nrow(raw_df %>%
  group_by(prolific_subject_id) %>%
  summarize(total_num_clean_subjects_in_data = n()))

```

```{r}
#| label: Print surveys_df csv (and txt for Claude)
#| echo: false 

write.csv(surveys_df, "pretest_survey_data.csv", row.names = FALSE)


write.table(surveys_df, file = "pretest_survey_data.txt", sep = "\t", row.names = FALSE, quote = FALSE)

```

## Introduction

Existing literature indicates that a complex interplay of psychological factors—including fear of social isolation (Spiral of Silence; [@NoelleNeumann1974]; [@Chen2018]; [@Haug2025]), identity protection (Social Identity Theory; [@Tajfel1979]; [@Hunter1996]), communication anxiety (Communication Apprehension; [@Beatty1987]; [@Schulenberg2023]), and emotional dysregulation [@Masters2019; @Aldao2010]—converge with sociocultural influences like opinion climate [@Haug2025], misinformation [@Lewandowsky2017], and cognitive biases [@Kahneman2011] to deter open dialogue. Following from these ideas, researchers have suggested that people avoid controversial discussions primarily due to perceived threats to their social standing, personal identity, and emotional well-being [@Chen2018; @Tajfel1979]. This avoidance is thought to often be a coping mechanism, reinforced by group dynamics and the spread of misinformation [@Lewandowsky2017], ultimately hindering constructive discourse and critical thinking [@NoelleNeumann1974; @Haug2025]. Understanding this phenomenon is critical for fostering democratic citizenship, promoting critical thinking, and cultivating inclusive learning environments [@Duarte2015].

These problems can be combated on systemic and individual levels. Previous research indicates that interventions on the individual level can include reducing communication apprehension [@Beatty1987; @Schulenberg2023], providing tools for difficult conversations, and building skills such as emotional regulation [@Gross1998], respectful dissent [@Rehg2022], sensitivity towards others [@Decety2006], and open-minded perspective-taking [@Galinsky2008]. In a future experiment, we will be focusing on the individual level through one-on-one human-chatbot conversations. These conversations are about controversial topics with strong moral components, which are thought to evoke many of the factors that play into individual unwillingness to converse [@Aldao2010; @Tajfel1979]. By curating a space where users are led to cultivate and practice the individual-level interventions which should increase users’ competencies in navigating difficult conversations, we hope to increase users’ willingness to participate in these difficult conversations. Before we can perform this intervention, though, we need to properly measure individuals' reasons for not engaging in these conversations about controversial topics. Towards this end, we designed a 19-item questionnaire that seeks to capture the primary concerns that contribute to an individual's reluctance to engage in conversations on controversial topics. In this survey pretest, we will assess whether our questionnaire succeeds in this endeavor.

### Limitations

The literature clearly indicates that while individual skills like emotional regulation [@Gross1998], communication competence [@Spitzberg1983], and conflict management [@Rahim2011] are vital for engaging in difficult conversations, their effective application is deeply intertwined with the social environment [@Edmondson1999; @Newman2017]. Thus, by not directly intervening on the systemic level, which includes aspects such as creating a psychologically safe environment [@Edmondson1999], challenging negative norms, and promoting inclusivity [@Carmeli2010], we are missing pieces of the puzzle that significantly influence individual willingness, especially “psychological safety.”

Psychological safety—the belief that one can speak openly and truthfully about problems without fear of reprisal—is built upon three core pillars: care, consistency, and normalizing mistakes [@Newman2017]. Psychological safety is suggested to largely relate to group culture and norms in a conversation [@Edmondson2014]. Through developing greater abilities to navigate difficult conversations, users of our intervention may influence future group climates, which, in turn, could contribute to each member’s sense of psychological safety [@Carmeli2010]. This idea goes hand in hand with the finding that widespread individual avoidance can contribute to the formation of a silent, unsupportive opinion climate [@NoelleNeumann1974; @Chen2018]. However, the literature suggests that psychological safety is largely curated through systemic changes, especially for creating an environment where people from marginalized communities can experience the same sense of psychological safety as conventionally celebrated voices [@Edmondson2014; @Roberts2022]. Future research could explore applications of chatbots for systemic interventions.

Our primary goal, though, is to focus on the direct effects of individual interventions. By focusing solely on the individual level, perhaps users will grow in their abilities to navigate these difficult conversations even in spaces with poor group cultures or norms.

## Methods

### Materials

We used custom software written with the jsPsych framework (@jspsychcite) to create the study, which participants viewed and responded to via their personal laptops or desktops. The javascript code for the experiment and RStudio code for data analysis can both be found at (<https://github.com/jodeleeuw/URSI-LLM-Convo-Bot/tree/main/pilot_1).> Pre-registrations and all data for this study and previous pilots are available on the Open Science Framework at <https://osf.io/x32pv.>

### Participants and Procedure

Through Prolific, we gathered data from `r total_num_subjects_in_data` subjects. To remain consistent with the demographic that our main experiment will target, we restricted our study to Prolific users who currently reside in the states, fluently speak English, and reported the United States as their nationality and country of birth. Per Prolific's harmful content criteria, we also restricted our study to participants who have consented to participating in studies with harmful content. Participants received a compensation of one dollar and fifty cents. We excluded data from `r total_failed_attention_checks` subjects for failing our attention check question. After applying this exclusion criteria, `r total_num_clean_subjects_in_data` subjects remain.

Upon agreeing to participate in this experiment, participants were brought to the experiment webpage where they were asked to complete our questionnaire. The questionnaire was composed of questions to ascertain which of our six topics the participant was most unwilling to discuss, the participant's topic-specific open-mindedness, the participant's willingness to converse with others about the topic, and the concerns that negatively impact the user's willingness. After completing the questionnaire, participants were sent back to Prolific.

```{r}
#| label: Find time taken specifically on the surveys (by subject and overall)
#| echo: false 

#finding each subject's time taken on the survey specifically
surveys_df <- surveys_df %>%
  group_by(prolific_subject_id) %>%
  mutate(
    time_minutes_survey_only = sum(
      unique(as.numeric(rt)) / 60000),
      na.rm = TRUE
    )

#finding overall mean, min, max of how long participants took 
time_stats <- surveys_df %>%
  ungroup() %>%
  summarise(
    Mean = round(mean(time_minutes_survey_only, na.rm = TRUE), 1),
    Min = round(min(time_minutes_survey_only, na.rm = TRUE), 1),
    Max = round(max(time_minutes_survey_only, na.rm = TRUE), 1)
  ) 

mean_survey_time <- time_stats$Mean
min_survey_time <- time_stats$Min
max_survey_time <- time_stats$Max
```

```{r}
#| label: WIP - tbl-basic-demographics-table
#| echo: false
#| tbl-cap: Table of basic demographics.

#reloading package because functions from other packages were overriding??
library(gtsummary) 

#we already read in demographics earlier 

#create a demographics table from prolific screening demo data which we loaded onto the raw df arlier
survey_demographics_df <- surveys_df %>%
filter(question == "commitment-check") %>% # picking random question so there's only one row per subject
select(prolific_subject_id, Age, Sex, Ethnicity, "Ethnicity simplified", "Highest education level completed", "U.s. political affiliation", "Political spectrum (us)", "Religious affiliation", "Employment status", time_taken_overall_minutes) %>%
  mutate(Age = as.numeric(Age))

#ensure that these are factors
survey_demographics_df$`Highest education level completed` <- factor(survey_demographics_df$`Highest education level completed`)
survey_demographics_df$`U.s. political affiliation` <- factor(survey_demographics_df$`U.s. political affiliation`)
survey_demographics_df$`Political spectrum (us)` <- factor(survey_demographics_df$`Political spectrum (us)`)

# define the levels in categorical variables so that they are organized in the table
survey_demographics_df$`U.s. political affiliation` <- fct_relevel(
  survey_demographics_df$`U.s. political affiliation`,
  "Republican",
  "Democrat",
  "Independent",
)


survey_demographics_df$`Highest education level completed` <- fct_relevel(
  survey_demographics_df$`Highest education level completed`,
  "High school diploma/A-levels",
  "Technical/community college",
  "Undergraduate degree (BA/BSc/other)",
  "Graduate degree (MA/MSc/MPhil/other)",
)


survey_demographics_df$`Political spectrum (us)` <- fct_relevel(
  survey_demographics_df$`Political spectrum (us)`,
"Conservative",
"Liberal",
)

survey_demographics_df <- survey_demographics_df %>%
  mutate(across(where(is.factor), ~ droplevels(.)))

survey_demographics_df$time_taken_overall_minutes <- round(
  as.numeric(survey_demographics_df$time_taken_overall_minutes), 
  1
)

demographics_table <- tbl_summary(survey_demographics_df,
 by = NULL,
  label = list(
    `time_taken_overall_minutes` = "Time taken (minutes)",
    Age = "Age (years)",
    Sex = "Sex",
    `Ethnicity simplified` = "Ethnicity",
    `U.s. political affiliation` = "Political affiliation",
    `Highest education level completed` = "Education level",
    `religious-affiliation` = "Religious affiliation",
    `Political spectrum (us)` = "Political ideology"
  ),
  statistic = NULL,
#list(
  #  all_continuous() ~ "{mean} ({sd})",
  #  all_categorical() ~ "{n} ({p}%)"
#),
  type = list(
    `time_taken_overall_minutes` = "continuous",
    `Age` = "continuous",
    `gender-identification` = "categorical",
   `Sex` = "categorical",
    `Ethnicity simplified` = "categorical",
    `U.s. political affiliation` = "categorical",
    `Highest education level completed` = "categorical",
    `religious-affiliation` = "categorical",
    `Political spectrum (us)` = "categorical"
  ),
  digits = list(
    all_continuous() ~ 2,
    all_categorical() ~ 0
  ),
 include = c(
'time_taken_overall_minutes',
'Age',
'Sex',
'Ethnicity simplified',
'U.s. political affiliation',
'Highest education level completed',
'Religious affiliation',
'Political spectrum (us)'
))
 # %>%  modify_header(label = "**whatever label i want**")

# Print the table
as_gt(demographics_table)

```

### Survey design

#### Supporting Literature

We developed our willingness questions based on existing literature about the concerns reasons that contribute to a person's unwillingness to participate in conversations, especially conversations about divisive topics.

Elisabeth Noelle-Neumann’s Spiral of Silence theory, developed in the 1960s and ’70s, offers a foundational understanding of how individuals’ willingness to express opinions on controversial public issues is influenced by their perception of those opinions as popular or unpopular [@NoelleNeumann1974; @metaSpiralSilence]. This theory suggests that the prevailing opinion held by the majority in a specific social setting, such as a classroom or public forum, profoundly influences an individual’s willingness to participate in discussions. If one’s view is perceived as opposed to the majority, individuals are less likely to express it, a phenomenon directly linked to the Spiral of Silence [@metaSpiralSilence; @ExpSpiralSilence]. Individual psychological factors, such as communication apprehension and fear of conflict, are not isolated but are significantly influenced by the prevailing opinion climate.

The theory’s key elements revolve around a fundamental human apprehension: the fear of social isolation, which is triggered by the belief that others will consider the individual not merely mistaken, but morally deficient [@ExpSpiralSilence; @metaSpiralSilence]. Individuals thus tend to refrain from publicly stating their views on controversial matters when they perceive that doing so would attract criticism, scorn, laughter, or other signs of disapproval.

The Social Identity Theory (SIT) suggests that when controversial issues are discussed, individuals may experience “identity threat” if their ideas and feelings about these issues are deeply connected to their identity or in-group, such as their race [@Tajfel1979; @Hunter1996]. Identity threat is defined as “experiences appraised as indicating potential harm to the value, meanings, or enactment of an identity” (in e.g. [@Steele1997]).

The accentuation of differences between in-groups and out-groups, coupled with the desire for positive distinctiveness, can intensify intergroup hostility and conflict, especially when competition or perceived threats exist [@Tajfel1979; @Turner1987]. This can lead to negative perceptions of out-groups, even bordering on dehumanization, where fewer human attributes are ascribed to the “other” [@Haslam2006; @Fiske2002], and beliefs that opponents are “wicked and untrustworthy”—making constructive dialogue exceedingly difficult, as the “other” is seen as an adversary rather than a conversational partner.

This strong in-group cohesion, while providing social support and validation, can paradoxically foster communication siloing. SIT highlights that people identify with in-groups to enhance their self-esteem and perceive their in-group favorably. This desire for “positive distinctiveness” motivates them to accentuate differences with out-groups. When controversial topics arise, challenging an in-group’s perspective can be perceived as an “identity threat,” leading to defensive behaviors like derogating or distancing from out-group views. This process also contributes to “group polarization,” where discussions with like-minded peers strengthen original beliefs and lead to more extreme stances [@Sunstein2009; @Moscovici1976].

Communication Apprehension (CA) is a broad term referring to an individual’s “fear or anxiety associated with either real or anticipated communication with another person or persons” [@McCroskey1977; @Beatty1987]. It is thought to be a fundamental psychological response to evaluation. Research indicates that approximately 70 % of individuals in the U.S. experience CA when giving a speech, with 20 % having severe CA that leads to avoidance of oral communication in personal, public, and professional settings [@McCroskey1977; @Eilert2025]. A significant driver of communication avoidance is the fear of conflict. People often avoid communication when it is likely to lead to an unpleasant outcome, such as offending someone or entering into conflict. This avoidance is primarily driven by a desire to minimize perceived threats to self-esteem and well-being, including the fear of rejection, disapproval, criticism, humiliation, or loss of security [@Beatty1987; @Eilert2025].

CA suggests that the fear of conflict can lead people to systematically avoid “zero-sum situations”—where one party’s gains are offset by another’s losses—even when such avoidance is costly. This belief can erode interpersonal trust and increase hostility. Moreover, avoiding necessary difficult conversations leads to “conflict debt,” which breeds resentment and allows unaddressed concerns to escalate into crises.

Individuals with high CA experience significant fear and anxiety when faced with communication. This internal discomfort, along with a fear of negative outcomes like conflict or rejection, leads them to actively avoid communication. This avoidance, while providing immediate relief from anxiety, prevents the individual from experiencing that the feared outcomes might not occur or that they can cope with the anxiety. This creates a negative reinforcement loop: avoidance reduces immediate discomfort, but simultaneously strengthens the underlying fear and the avoidance habit, making it harder to engage in future difficult conversations.

The concepts of emotional avoidance, emotional dysregulation, and affective forecasting reveal that avoidance is not just about external threats, but also an internal struggle. Emotional avoidance refers to the deliberate effort to suppress, ignore, or distract oneself from experiencing unpleasant feelings such as fear, sadness, anger, or vulnerability. Common tactics include denial, suppression, distraction, busyness, and steering clear of situations, people, or places that might evoke uncomfortable emotions.

Emotional dysregulation (ED) is defined as the inability to regulate the intensity and quality of emotions to generate an appropriate emotional response. Emotional regulation strategies involve the ability to recognize, evaluate, modify, and manage emotions in a personal and socially acceptable way to maintain mental control over strong feelings and achieve adaptive functioning [@Gross1998; @Aldao2010]. A meta-analysis recently found maladaptive emotion regulation to be strongly associated with internalizing symptoms, aggression, and poorer social functioning [@SalazarKampf2023].

Affective forecasting, or hedonic forecasting, is the prediction of how one will feel in the future. Research consistently demonstrates that people are poor predictors of their future well-being. They tend to overestimate the impact and duration of negative emotions in response to loss or difficult situations [@Wilson2000; @Levine2012; @Liu2022]. This inaccuracy occurs because individuals often focus more on what they will lose than on what will stay the same (focalism), fail to envision how their own coping skills will lessen their unhappiness (immune neglect), and fail to envision how they might develop new values (adaptation) [@Wilson2000; @Levine2012].

Thus, avoidance is also an internal struggle to manage uncomfortable feelings and a flawed prediction of future emotional states, making the perceived "cost" of engagement seem higher than it might be.

If a significant portion of a population struggles with emotional regulation and relies on avoidance as a primary coping mechanism for unpleasant feelings, this has implications beyond individual mental health. When collective emotional dysregulation is widespread, it can hinder a society's capacity for constructive dialogue on complex, emotionally charged issues. Individuals less equipped to handle the inherent discomfort of disagreement may resort to withdrawal, aggression, or rigid adherence to existing beliefs, rather than engaging in nuanced discussion. This suggests that the inability to manage emotions constructively during controversial discussions can lead to societal fragmentation, increased polarization, and a diminished capacity to find common ground or collectively problem-solve issues that require navigating diverse, often conflicting, emotional responses.

Based on this existing research, we developed a Likert-scale questionnaire which lists 19 concerns that might contribute to users' unwillingness to engage in a conversation about a topic that they are unwilling to discuss with others. We also created two questions which ask users to rate, on a scale from 0 to 100, their willingness to "have a conversation with someone who strongly disagrees with \[them\] about \[conversation topic\]" and "to have a conversation with someone who would push against \[their\] views about \[conversation topic\]," where a score of 0 is complete unwillingness, a score of 50 is neutral, and a score of 100 is absolute willingness to converse.

In our full-scale experiment, we intend to administer this questionnaire both before and after our intervention to assess which concerns, if any, our intervention can mitigate. To ensure that this survey captures all of the primary reasons that contribute to unwillingness and can be targeted on the individual level, this pretest study administered the questionnaire once and with a free-response question at the end: "Think back to the last time you chose to not participate in a conversation about \${topic} or another topic you found uncomfortable to discuss. In the space below, please describe why you decided to not engage. Consider whether your reasons align with any of the potential concerns listed in this survey, or if you had different concerns."

## Results

### Topic choice

On average, participants completed the questionnaire in `r mean_survey_time` minutes (`r min_survey_time`, `r max_survey_time`).

```{r}
#| label: fig-topic-ratings
#| echo: false
#| fig-cap: In the pre-intervention survey, each subject used a 7-pt scale to rate how comfortable they would be discussing each of the following topics. One of the subject's most strongly rated topics were randomly selected as the topic that the subject would focus on for the remainder of the experiment. The scale went from strongly uncomfortable (1), to moderately uncomfortable, a little uncomfortable, neutral (4), a little comfortable, moderately comfortable, and strongly comfortable (7).

# create figure/table showing distribution of ratings on topics
topic_ratings_df <- surveys_df %>%
  filter(phase == "pre-convo-survey-initial") %>%
  filter(question != "commitment-check") %>%
  mutate(answer = as.numeric(answer)) %>%
  select(prolific_subject_id, question, answer, topicChoiceAsString)

topic_labels <- c( 
  "mandating vaccines in the U.S." = 
    "Mandating vaccines",
  "human euthanasia in the U.S." = 
    "Human euthanasia",
  "deportation policies for undocumented immigrants in the U.S." = 
    "Deportation policies",
  "transgender athletes in U.S. sports" = 
    "Transgender athletes in sports",
  "the role of the U.S. government in healthcare" = 
    "Government in healthcare",
  "U.S. public policy on climate change" = 
    "Climate change policies",
  "the use of A.I. in traditionally human-run spaces (e.g., medicine, art, driving)" = 
    "A.I. in human-run spaces",
  "the role of capitalism in the U.S. economy" = 
    "Capitalism in economy",
  "the role of the U.S. in the Israeli-Palestinian conflict" = 
    "U.S. in Israeli-Palestinian conflict",
  "police conduct and accountability in the U.S." =
    "Police conduct and accountability"
)

topic_ratings_plot <- ggplot(topic_ratings_df, aes(x = answer, y = question, fill = question)) +
  geom_density_ridges(bandwidth = 0.931) + # using the bandwidth that it automatically picks 
  # give the qualitative meanings of 1-7
  scale_x_continuous(breaks = seq(1, 7, by = 1)) +
   scale_y_discrete(labels = topic_labels) +
  # make it look nicer
  theme_ridges() +      
  # remove the legend
  theme(legend.position = "none") +
  labs(title = "Ridgeline Plot of Ratings by Topic",
       x = "Rating (1 = Strongly uncomfortable, 7 = Strongly comfortable)",
       y = "Contentious Topic")

print(topic_ratings_plot)
```

```{r}
#| label: fig-topic-ratings-only-chosen-topics
#| echo: false
#| fig-cap: In the pre-intervention survey, each subject used a 7-pt scale to rate how comfortable they would be discussing each of the following topics. One of the subject's most strongly rated topics were randomly selected as the topic that the subject would focus on for the remainder of the experiment. The scale went from strongly uncomfortable (1), to moderately uncomfortable, a little uncomfortable, neutral (4), a little comfortable, moderately comfortable, and strongly comfortable (7).

# create figure/table showing distribution of ratings on chosen topics only

topic_choice_rating <- topic_ratings_df %>%
  group_by(prolific_subject_id) %>%
  filter(question == topicChoiceAsString)

topic_choice_ratings_plot <- ggplot(topic_choice_rating, aes(x = answer, y = question, fill = question)) +
  geom_density_ridges(bandwidth = 0.931) + # using the bandwidth that it automatically picks 
  # give the qualitative meanings of 1-7
  scale_x_continuous(breaks = seq(1, 7, by = 1)) +
   scale_y_discrete(labels = topic_labels) +
  # make it look nicer
  theme_ridges() +      
  # remove the legend
  theme(legend.position = "none") +
  labs(title = "Ridgeline Plot of Ratings by Topic",
       x = "Rating (1 = Strongly uncomfortable, 7 = Strongly comfortable)",
       y = "Contentious Topic")

print(topic_choice_ratings_plot)
```

Figure \@ref(fig: fig-topic-ratings) indicates that, on average, subjects appeared to show a little to moderate willingness to discuss our ten controversial topics, though subjects exhibited some additional discomfort towards the topics of transgender athletes in sports and the U.S. in the Israeli-Palestinian conflict. We had intended to select topics that participants were generally less comfortable discussing than indicated by these results, as we want to understand why Americans are unwilling to discuss the topics that they are a little to strongly unwilling to discuss. However, Figure \@ref(fig: fig-topic-ratings-only-chosen-topics) indicates that, on average for most of our topics, participants were indeed a little to strongly unwilling to discuss their topic choice. Only three of the seven topics---climate change policy, A.I. in human-run spaces, and government in healthcare---still garnered many participants who were more willing than we had intended. It seems participants were a little to strongly willing to discuss many of the ten topics, but, for each participant, there tended to be at least one topic that the participant was a little to strongly unwilling to discuss.

```{r}
#| label: fig-bar-chart-topic-choice
#| echo: false 
#| fig-cap: Bar chart showing the number of participants in each topic

topic_votes_plot <- ggplot(topic_choice_rating, aes(x = answer, fill = topicChoiceAsString)) + 
  geom_bar() + 
  xlab("Willingness (1 = Strongly Unwilling, 7 = Strongly Willing)") +
  ylab("Number of participants")

topic_votes_plot
```

### Average willingness

```{r}
#| label: fig-avg-willingness-by-topic
#| echo: false
#| fig-cap: Ridgeline plot of willingness to engage in conversation about topic by topic. Participants only rated their willingness for their topic choice. Participants' willingness values were calculated through averaging the responses to the questions, "Using the slider, please rate your willingness to have a conversation with someone who strongly disagrees with you about [topic], where a score of 0 is absolute unwillingness to converse and 100 is absolute willingness to converse. A score of 50 is neutral," and "Using the slider, please rate your willingness to have a conversation with someone who strongly disagrees with you about [topic], where a score of 0 is absolute unwillingness to converse and 100 is absolute willingness to converse. A score of 50 is neutral."

#make avg willingness df by taking mean of both slides for each participant 

avg_willingness_df <- surveys_df %>%
  filter(question == "slider1-placeholder" | question == "slider2-placeholder") %>% 
  group_by(prolific_subject_id, topicChoiceAsString) %>%
  summarise(avg_willingness = mean(as.numeric(answer), na.rm = TRUE)) %>%
  ungroup()

topic_choice_ratings_plot <- ggplot(avg_willingness_df, aes(x = avg_willingness, y = topicChoiceAsString, fill = topicChoiceAsString)) +
  geom_density_ridges(bandwidth = 0.931) + # using the bandwidth that it automatically picks 
  scale_x_continuous() +
   scale_y_discrete(labels = topic_labels) +
  # make it look nicer
  theme_ridges() +      
  # remove the legend
  theme(legend.position = "none") +
  labs(title = "Ridgeline Plot of Willingness to Engage by Topic",
       x = "Rating (0 = Strongly unwilling, 100 = Strongly willing)",
       y = "Contentious Topic")

topic_choice_ratings_plot

```

```{r}
#| label: fig-box-plot-willingness 
#| echo: false
#| fig-cap: Distribution of average willingness ratings by topic from 0 (completely unwilling) to 100 (completely willing), where 50 is neutral.


avg_willingness_box_plot <- ggplot(avg_willingness_df, aes(x = avg_willingness, y = as.factor(topicChoiceAsString), fill = as.factor(topicChoiceAsString))) +
  geom_boxplot() + 
  labs(title = "Boxplots of Willingness to Engage by Topic",
       x = "Rating (0 = Strongly unwilling, 100 = Strongly willing)",
       y = "Contentious Topic") + 
  guides(fill = "none")

avg_willingness_box_plot

```

```{r}
#| label: Diff btwn two avg willingness questions 
#| echo: false 


will_df <- surveys_df %>%
  filter(question == "slider1-placeholder" | question == "slider2-placeholder") %>%
  pivot_wider(
    names_from = question,
    values_from = answer
  ) %>%
  mutate(
    diff_willingness = as.numeric(`slider1-placeholder`) - as.numeric(`slider2-placeholder`)
  ) %>%
  select(prolific_subject_id, topicChoiceAsString, diff_willingness)

# there are some p big differences for a handful of people, so it's good that we have both questions

```

### Willingness concerns

```{r}
#| label: fig-unwillingness-reason-likert-bar-plot
#| echo: false
#| fig-cap: Likert bar plot displaying subjects' ratings of how much certain concerns contribute to their unwillingness to converse. The nineteen potential concerns were derived from existing literature and pretested in a pilot experiment. On a scale from 0 to 7, where a rating of 0 meant participant's willingness is not at all impacted by the concern and a rating of 7 meant participant's willingness is strongly impacted by the concern, participants rated the potential concerns' effects on their willingness to converse.

# only works though if all "1" to "7" ratings are in the df

wide_unwill_reasons_df_1 <- surveys_df %>%
  filter(grepl("unwillingness-reason", question)) %>%
  filter(phase == "follow-up-unwillingness-concerns-survey") %>%
  select(question, answer, prolific_subject_id) %>%
  group_by(prolific_subject_id) %>%
  mutate(answer = as.numeric(answer)) %>%
  pivot_wider(id_cols = prolific_subject_id, names_from = question, values_from = answer) %>%
  ungroup() %>%
  rename("Might experience negative feelings about self" = "unwillingness-reason-The conversation could negatively affect how I feel about myself during and/or afterward (e.g.,  fear, sadness, anger, vulnerability)", "Might be offended" = "unwillingness-reason-I might take offense to what someone says",
  "Opinion is unpopular amongst group" = "unwillingness-reason-I would feel like my opinion is unpopular amongst the group.",
  "Doesn't participate in these conversations often" = "unwillingness-reason-I don’t often participate in conversations like these",
  "Might struggle to explain views or seem uninformed" = "unwillingness-reason-I might struggle to explain my views to the others and/or come across as uninformed",
  "Others might be unproductive or disrespectful" = "unwillingness-reason-I would not trust the others to keep the conversation productive and respectful (e.g., keeping emotions in check, refraining from making hostile remarks, not dominating the conversation)",
  "Views wouldn't change" = "unwillingness-reason-It would be a waste of time because my views wouldn't change",
  "Doesn't want to listen to others" = "unwillingness-reason-I would not want to listen to the others discuss their views",
  "Might be criticized" = "unwillingness-reason-Someone could criticize or show disapproval of my views",
  "Experience negative feelings about the world" = "unwillingness-reason-The conversation could negatively affect how I feel about the world during and/or afterward",
  "Conversation might be awkward or tense" = "unwillingness-reason-The conversation could become awkward or tense",
  "Might feel embarrassed" = "unwillingness-reason-I could feel humiliated or embarrassed in the conversation",
  "Might offend someone" = "unwillingness-reason-Someone might take offense to what I say",
  "Might struggle to remain productive and respectful" = "unwillingness-reason-I might struggle to remain productive and respectful",
  "Others won't make effort to listen" = "unwillingness-reason-The others might not make a full effort to hear and understand me",
  "Identity might be challenged" = "unwillingness-reason-Someone could challenge ideas or beliefs that play an important role in making me who I am",
  "Might feel invalidated" = "unwillingness-reason-I might feel disempowered, unheard, or invalidated",
  "Might incur social reprecussions" = "unwillingness-reason-I could incur social repercussions (e.g., being excluded by others in the future, putting a strain on my relationships, negative changes to my reputation)",
  "Opinion is unpopular in general" = "unwillingness-reason-My opinion is unpopular in general."
) %>%
  mutate(across(everything(), ~ replace_na(.x, 1))) # making the concerns that participants weren't even worried about into a 1 so that they can be represented in the likert graph as "not at all"


wide_unwill_reasons_df_2 <- wide_unwill_reasons_df_1 %>%
  select(-prolific_subject_id) %>%
  mutate(across(everything(), ~ factor(.x, levels = 1:7,
     labels = c("Not at all", "Slightly", "Somewhat",
                "Moderately", "Quite a bit", "Very much", "Extremely"),
     ordered = TRUE))) 

unwillingness_reason_likert <- likert(as.data.frame(wide_unwill_reasons_df_2))

# for the future when comparing pre and post, might want to use this example line: df1_likert <- likert(items=df1[,3:4], grouping=df1[,2])

plot(unwillingness_reason_likert, legend.position="right")

```

```{r}
#| label: fig-sliders-density
#| echo: false
#| fig-cap: Density plot displaying subjects' self-reported willingness to converse about their topic with somebody who disagrees with them and/or will challenge their views. Subjects used a scale from 0 to 100, where a rating of 0 meant subject was absolutely unwilling and a rating of 100 meant subject was absolutely willing. Subjects were asked this question in two ways; this density plot shows the mean of the two ratings. 

sliders_df <- surveys_df %>%
  filter(question == "slider1-placeholder" | question == "slider2-placeholder") %>%
  group_by(prolific_subject_id) %>%
  summarize(mean_willingness = round(mean(as.numeric(answer), na.rm = TRUE), 1))
  
sliders_density <- ggplot(sliders_df, aes(x = mean_willingness)) + #, color = condition.x)) +
  geom_density() +  
  labs(x = "Mean Willingness Rating", y = "Density of Rating", title = "Average Willingness to Converse about Selected Topic")  #, color = "Group") +
 # scale_color_manual(values = c("control free reflection" = "darkseagreen", "selection bot" = "cornflowerblue", "combination bot" = "navyblue"), 
 #                    labels = c("control free reflection" = "Control group", "selection bot" = "Experimental group: Selection bot", "combination bot" = "Experimental group: Combination bot"))

print(sliders_density)

```

```{r}
#| label: Basic info abt concerns
#| echo: false

concern_map <- c("Might experience negative feelings about self" = "unwillingness-reason-The conversation could negatively affect how I feel about myself during and/or afterward (e.g.,  fear, sadness, anger, vulnerability)", "Might be offended" = "unwillingness-reason-I might take offense to what someone says",
  "Opinion is unpopular amongst group" = "unwillingness-reason-I would feel like my opinion is unpopular amongst the group.",
  "Doesn't participate in these conversations often" = "unwillingness-reason-I don’t often participate in conversations like these",
  "Might struggle to explain views or seem uninformed" = "unwillingness-reason-I might struggle to explain my views to the others and/or come across as uninformed",
  "Others might be unproductive or disrespectful" = "unwillingness-reason-I would not trust the others to keep the conversation productive and respectful (e.g., keeping emotions in check, refraining from making hostile remarks, not dominating the conversation)",
  "Views wouldn't change" = "unwillingness-reason-It would be a waste of time because my views wouldn't change",
  "Doesn't want to listen to others" = "unwillingness-reason-I would not want to listen to the others discuss their views",
  "Might be criticized" = "unwillingness-reason-Someone could criticize or show disapproval of my views",
  "Experience negative feelings about the world" = "unwillingness-reason-The conversation could negatively affect how I feel about the world during and/or afterward",
  "Conversation might be awkward or tense" = "unwillingness-reason-The conversation could become awkward or tense",
  "Might feel embarrassed" = "unwillingness-reason-I could feel humiliated or embarrassed in the conversation",
  "Might offend someone" = "unwillingness-reason-Someone might take offense to what I say",
  "Might struggle to remain productive and respectful" = "unwillingness-reason-I might struggle to remain productive and respectful",
  "Others won't make effort to listen" = "unwillingness-reason-The others might not make a full effort to hear and understand me",
  "Identity might be challenged" = "unwillingness-reason-Someone could challenge ideas or beliefs that play an important role in making me who I am",
  "Might feel invalidated" = "unwillingness-reason-I might feel disempowered, unheard, or invalidated",
  "Might incur social reprecussions" = "unwillingness-reason-I could incur social repercussions (e.g., being excluded by others in the future, putting a strain on my relationships, negative changes to my reputation)",
  "Opinion is unpopular in general" = "unwillingness-reason-My opinion is unpopular in general."
)

percent_concerned_df <- surveys_df %>%
  filter(phase == "willingness-concerns-first-survey") %>%
    # Keep only the columns needed
    select(question, answer) %>%
    # Group by the question text
    group_by(question) %>%
    # Summarize the data for each question
    summarise(
        # Calculate the count of "Yes" answers for this question
        N_yes = sum(answer == "2", na.rm = TRUE),
        # Calculate the percentage
        percent_yes = (N_yes / n()) * 100,
        .groups = 'drop' # Ungroup after summarizing
    ) %>%
    # Select and rename the final columns
    select(
        concern = question, 
        percent_yes
    ) %>%
    # Format to 1 decimal place
    mutate(
        percent_yes = round(percent_yes, 1)
    ) %>%
    # Order the results from highest to lowest percentage
    arrange(desc(percent_yes))

```

### Open-mindedness

```{r}
#| label: fig-openmindedness-density
#| echo: false
#| fig-cap: Scatterplot with line of equality of pre- and post-conversation mean ratings of subjects' open-mindedness when discussing their topic. Subjects rated their open-mindedness on a scale from 0 to 7, where a rating of 0 meant subject was not open-minded and a rating of 7 meant subject was open-minded.

openmindedness_df <- surveys_df %>%
  filter(grepl("openmindedness", question)) %>%
  mutate(answer = as.numeric(answer)) 

openmindedness_df_mean <- openmindedness_df %>%
  group_by(prolific_subject_id) %>%
  summarize(mean_openmindedness = mean(as.numeric(answer))) %>%
  ungroup()

openmindedness_density <- ggplot(openmindedness_df_mean, aes(x = mean_openmindedness)) + #, color = condition.x)) +
  geom_density() +  
  labs(x = "Mean Open-mindedness Rating", y = "Density of Rating", title = "Average Open-mindedness about Selected Topic")  #, color = "Group") +
 # scale_color_manual(values = c("control free reflection" = "darkseagreen", "selection bot" = "cornflowerblue", "combination bot" = "navyblue"), 
 #                    labels = c("control free reflection" = "Control group", "selection bot" = "Experimental group: Selection bot", "combination bot" = "Experimental group: Combination bot"))

print(openmindedness_density)

```

```{r}
#| label: fig-openmindedness-likert-bar-plot
#| echo: false
#| fig-cap: Likert bar plot displaying subjects' ratings of five questions about their open-mindedness. The five questions were derived from @elnakouri2024 and modified for application to this experiment. On a scale from 0 to 7, where a rating of 0 meant subject strongly disagreed and a rating of 7 meant strongly agree, subjects rated how true the five statements were for them. Three of the statements were negatively worded in the survey but have been worded positively and reverse-coded in this plot for clarity.

# only works though if all "1" to "7" ratings are in the df

wide_for_likert_openmindedness_df_with_sub_labels <- openmindedness_df %>%
  select(question, answer, prolific_subject_id) %>%
  pivot_wider(names_from = question, values_from = answer) %>%
  ungroup() %>%
  rename("Has patience for opposing arguments" = "openmindedness...have little patience for arguments that I disagree with @R@", 
"Does not avoid opposing messages" = "openmindedness...avoid messages that I disagree with @R@", 
"Believes it is important to pay attention to all politics ideas" = "openmindedness...believe it is a waste of time to pay attention to certain political ideas about $topic @R@",
"Is open to other political viewpoints" = "openmindedness...am open to considering other political viewpoints about $topic",
"Considers as many different opinions as possible" = "openmindedness...consider as many different opinions as possible about $topic") 

wide_for_likert_openmindedness_df <- wide_for_likert_openmindedness_df_with_sub_labels %>%
  select(-prolific_subject_id) %>%
  mutate(across(everything(), ~ factor(.x, levels = 1:7,
     labels = c("Strongly Disagree", "Moderately Disagree", "Somewhat Disagree",
                "Neutral", "Somewhat Agree", "Moderately Agree", "Strongly Agree"),
     ordered = TRUE)))

### create a vector of the all 1-7 values. defining again here in case stuff gets moved around 

all_likert_values <- c("Strongly Disagree", "Moderately Disagree", "Somewhat Disagree", "Neutral", "Somewhat Agree", "Moderately Agree", "Strongly Agree")
### get the column names from  existing df
openmindedness_column_names <- colnames(wide_for_likert_openmindedness_df)
### create 7 new rows
new_openmindedness_likert_rows <- lapply(all_likert_values, function(response) {
  as.list(setNames(rep(response, length(openmindedness_column_names)), openmindedness_column_names))
}) %>%
  bind_rows()
### add the new rows to the existing dataframe
wide_for_likert_openmindedness_df <- bind_rows(wide_for_likert_openmindedness_df, new_openmindedness_likert_rows)


# standardize all columns to use the same factor object
wide_for_likert_openmindedness_df <- wide_for_likert_openmindedness_df %>%
  mutate(across(everything(), ~ factor(.x, levels = all_likert_values, ordered = TRUE)))

openmindedness_likert <- likert(as.data.frame(wide_for_likert_openmindedness_df))

# for the future when comparing pre and post, might want to use this example line: df1_likert <- likert(items=df1[,3:4], grouping=df1[,2]) 

plot(openmindedness_likert, legend.position="right")
```

### Free response

```{r}
#| label: Making free response csv for chat analysis 
#| echo: false

free_response_df <- surveys_df %>%
  filter(question == "willingness-free-response") %>%
  ungroup() %>%
  select(prolific_subject_id, answer, topicChoiceAsString, Age, Sex, Ethnicity, "Ethnicity simplified", "Highest education level completed", "U.s. political affiliation", "Political spectrum (us)", "Religious affiliation", "Employment status", "Student status") %>%
  rename(
    #Condition = condition,
    'Focus topic' = 'topicChoiceAsString',
    'Free response' = 'answer'
    )

write.csv(free_response_df, "pretest_free_response_data.csv", row.names = FALSE)
```

```{r}
#| label: Df for 20 manually reviewed free responses 
#| echo: false 

# I used random.org to randomly select 20 numbers from 1 to 202, which will correspond to the row number in the free_response df

# 1. 129 
# 2. 179
# 3. 164
# 4. 199
# 5. 184
# 6. 116
# 7. 160
# 8. 38
# 9. 60
# 10. 76
# 11. 165
# 12. 1
# 13. 83
# 14. 106
# 15. 70
# 16. 33
# 17. 15
# 18. 46
# 19. 82
# 20. 81

free_response_manual_df <- free_response_df %>%
  filter(prolific_subject_id == "662938efd711c3639bd68aa3" |# 1. 129 
           prolific_subject_id == "675de3bd64119cef93edfc75" |# 2. 179
           prolific_subject_id == "672eda41e28ba46f845249df" |# 3. 164
           prolific_subject_id == "65c10e1bba59f80070501946" |# 4. 199
           prolific_subject_id == "64012a2623351894f2917605" |# 5. 184
           prolific_subject_id == "667610590a062036fb381ccd" |# 6. 116
           prolific_subject_id == "60712d937752fb8780e89951" |# 7. 160
           prolific_subject_id == "6624638357b56eb340c8f109" |# 8. 38
           prolific_subject_id == "660460162a2b3e5c554cf140" |# 9. 60
           prolific_subject_id == "609f58e8af0f5f3acedd9806" |# 10. 76
           prolific_subject_id == "646e4572cbed17f6bbacec12" |# 11. 165
           prolific_subject_id == "5dd86784dcec8782b85cb5bb" |# 12. 1
           prolific_subject_id == "66b979589d208d3ec691d89b" |# 13. 83
           prolific_subject_id == "66961b0829c07b563688c86d" |# 14. 106
           prolific_subject_id == "678ff81c8c6b56f93355e0bb" |# 15. 70
           prolific_subject_id == "63671bfc81a5eca4218b6417" |# 16. 33
           prolific_subject_id == "666efed91a31f3c82021f11d" |# 17. 15
           prolific_subject_id == "67335e8a302aac96120cc104" |# 18. 46
           prolific_subject_id == "671bf57baf2c2374636a8c4d" |# 19. 82
           prolific_subject_id == "6724260700cede758eb0c062" # 20. 81
           )

```

```{r}
#| label: tbl-free-response-examples-table
#| echo: false
#| tbl-cap: Table showing examples of subjects' responses to free response question, which asked subjects "Think back to the last time you chose to not participate in a conversation about ${topic} or another topic you found uncomfortable to discuss. In the space below, please describe why you decided to not engage. Consider whether your reasons align with any of the potential concerns listed in this survey, or if you had different concerns."

# This table should show a couple very typical responses, which, based on LLM analysis, should probably be consistent with our nineteen items 

# free_response_for_table <- free_response_df %>%
#   filter(prolific_subject_id == [select people we want to show as examples]) %>%
#   select("Free response", "Focus topic")

#kable(free_response_for_table, "html")

# %>%
#   kable_styling() %>%
#   column_spec(2, width = "5em") %>%
#   column_spec(c(3,4,5,5), width = "40em")
```

We conducted two primary analyses. First, we assessed whether our 19-item survey question accurately captures Americans' most pressing willingness concerns. Towards this end, we had LLMs analyze the free-response data to determine whether users felt that there were concerns contributing to their unwillingness other than the 19 items that we provided and whether any of these additional concerns were common in the data.

Specifically, we fed the following prompt to ChatGPT5 and Gemini 2.5 Flash: "You are a research assistant analyzing qualitative data from a survey pretest about the reasons why Americans are unwilling to engage in conversations on controversial topics. The file pretest_free_response_data.csv contains participants’ open-ended responses to the following question: “Think back to the last time you chose to not participate in a conversation about \${topic} or another topic you found uncomfortable to discuss. In the space below, please describe why you decided to not engage. Consider whether your reasons align with any of the potential concerns listed in this survey, or if you had different concerns.” Participants previously rated nineteen potential concerns, which were:

\[1\] "unwillingness-reason-I could feel humiliated or embarrassed in the conversation" \[2\] "unwillingness-reason-I could incur social repercussions (e.g., being excluded by others in the future, putting a strain on my relationships, negative changes to my reputation)" \[3\] "unwillingness-reason-I don’t often participate in conversations like these" \[4\] "unwillingness-reason-I might feel disempowered, unheard, or invalidated" \[5\] "unwillingness-reason-I might struggle to explain my views to the others and/or come across as uninformed" \[6\] "unwillingness-reason-I might struggle to remain productive and respectful" \[7\] "unwillingness-reason-I might take offense to what someone says" \[8\] "unwillingness-reason-I would feel like my opinion is unpopular amongst the group." \[9\] "unwillingness-reason-I would not trust the others to keep the conversation productive and respectful (e.g., keeping emotions in check, refraining from making hostile remarks, not dominating the conversation)" \[10\] "unwillingness-reason-I would not want to listen to the others discuss their views" \[11\] "unwillingness-reason-It would be a waste of time because my views wouldn't change" \[12\] "unwillingness-reason-My opinion is unpopular in general." \[13\] "unwillingness-reason-Someone could challenge ideas or beliefs that play an important role in making me who I am" \[14\] "unwillingness-reason-Someone could criticize or show disapproval of my views" \[15\] "unwillingness-reason-Someone might take offense to what I say" \[16\] "unwillingness-reason-The conversation could become awkward or tense" \[17\] "unwillingness-reason-The conversation could negatively affect how I feel about myself during and/or afterward (e.g., fear, sadness, anger, vulnerability)" \[18\] "unwillingness-reason-The conversation could negatively affect how I feel about the world during and/or afterward" \[19\] "unwillingness-reason-The others might not make a full effort to hear and understand me"

Your goal is to identify new or distinct concerns that appear in these free responses that are not captured by the nineteen items.

Please do the following: 1) Load and read the CSV file of free responses. Each row corresponds to one participant. 2) Analyze all responses to find recurring themes or concerns that are not already included in the existing nineteen. Do not just search for key words; you should manually review each free response. 3) For each new concern you identify, provide: A descriptive name for the concern. A brief summary of what it captures. 2–3 representative excerpts or paraphrased examples from participants’ responses (including the subject id).

Optional: any relevant notes or nuances (e.g., emotional tone, context, overlaps with existing concerns).

If no new concerns emerge, state that explicitly and explain why the responses align with existing categories."

We also manually analyzed ten percent of the data to validate the LLMs' assessments.

A total of six unique uncaptured concerns were identified by the LLMs. Gemini 2.5 Flash identified two uncaptured concerns: perceived lack of knowledge and lack of personal relevance. ChatGPT-5 identified five uncaptured concerns: professional consequences, public/permanent record and online exposure, context/setting constraints, lack of personal relevance, and low emotional/cognitive bandwith. These findings are consistent with the manual review that we performed on a random subset of 20 responses (\~10% of sample), in which potential missing items were professional consequences, perceived lack of knowledge, and additional emphasis on straining personal relationships.

Concerningly, both Gemini and ChatGPT misreported many of the representative examples of the uncaptured concerns that it identified, citing participant IDs that did not exist or misquoting existing participants. When we sent follow-up questions and requests to amend these errors, both LLMs responded with more errors and no real examples. All but one of the six representative examples provided by Gemini did not exist. ChatGPT mostly failsified examples when discussing professional consequences and the public/permanent record concerns. Beacuse of this, we manually combed through subjects' free responses for key words related to the concerns identified by Gemini, as well as the professional consequences concern and the public/permanent record concern identified by ChatGPT. We found a handful of responses in support of the concerns identified by Gemini. We found some excerpts in support of the professional concerns identified by ChatGPT but only identified one excerpt in support of the public-permanent records concern. Hence, we will not consider the public/permanent record concern.

```{r}
#| label: tbl-knowledge-free-response
#| echo: false
#| fig-cap: Table of example free responses in which participants expressed avoidance of conversations because they felt that the topic was too complex or that they did not know enough about the topic to engage in a conversation about it.


fr_knowledge <- free_response_df %>%
  filter(prolific_subject_id == "67ac21373a76570ef0a5c0d3" | prolific_subject_id == "66b979589d208d3ec691d89b" | prolific_subject_id == "5d3695969749cc00165c222f") %>%
  select("Free response", "Focus topic")

kable(fr_knowledge, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")

```

Topic complexity/lack of knowledge: Gemini identified lack of knowledge as an uncaptured concern. These participants reported that they avoided conversations because they did not know enough about the topic to engage in a conversation about it. With items like "I might struggle to explain my views to the others and/or come across as uninformed," we sought to capture feelings of inadequacy or embarrassment that might result from a lack of knowledge and thus drive the fear of participating in the conversation. However, our nineteen items did not directly identify the pragmatic concern that the subject feels they do not know enough about the topic to have a productive conversation. We could capture this with something like "I would feel like the topic is too complex or requires more knowledge than I have."

```{r}
#| label: tbl-relevance-free-response
#| echo: false
#| fig-cap: Table of example free responses in which participants expressed avoidance of conversations because they felt that they did not know enough about the topic to engage in a productive conversation about it with people who hold differing opinions. 

#6101d290cc9593237a346123

fr_relevance <- free_response_df %>%
  filter(prolific_subject_id == "5d3695969749cc00165c222f" | prolific_subject_id == "66293ab7b323f5f7ce785c38" | prolific_subject_id == "5b5a822e1ad8270001c4f28d") %>%
  select("Free response", "Focus topic")

kable(fr_relevance, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")

```

Perceived lack of relevance: Both Gemini and ChatGPT identified perceived lack of relevance as an uncaptured concern. These participants reported avoiding conversations because they felt that the topic was not personally relevant to them, that it was not their place to comment because they did not have first-hand experiences or were not affected by the topic, that they did not have anything to add to the conversation because the topic isn't relevant to them, or that they simply did not care enough or have strong enough opinions to engage. This is distinct from not participating because of fear or being uninformed — it’s a disinterest or intentional ethical boundary. I don't think this is the type of concern that we're necessarily looking to change through MOCA, so I'm not sure that it's relevant for our purposes; however, in the scope of trying to capture people's reasons for being unwilling, it is worth including items like: "I don't have first-hand experience with the topic and/or it isn't relevant to me" and "I don't care enough about the topic."

```{r}
#| label: tbl-prof-consequences-free-response
#| echo: false
#| fig-cap: Table of example free responses that were concerned with professional/workplace consequences that might follow from participating in a conversation about a controversial topic


fr_prof <- free_response_df %>%
  filter(prolific_subject_id == "55ecd6ff748092000daa9f5f" | prolific_subject_id == "5ac440da9534ba0001c736da" | prolific_subject_id == "63469e0a9552017beb3cfb82") %>%
  select("Free response", "Focus topic")

kable(fr_prof, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")

```

Professional consequences: According to ChatGPT, some participants reported avoiding conversations because they could have consequences in the workplace such as employment/HR problems, damaged professional standing, or workplace drama. We originally anticipated that workplace-related concerns would be captured by our social evaluation items; however, because the social risks in a workplace can engender consequences to one's livelihood/career that go beyond the typical repercussions in other social contexts, perhaps professional consequences should be a new item. This could be captured by an item like: "I could experience significant repercussions at work or in other spaces beyond my personal social network." \# potential items to add on:

```{r}
#| label: tbl-boundary-free-response
#| echo: false
#| fig-cap: Table of example free responses that were concerned with personal/family boundaries and the relational contexts of the conversation.


fr_family <- free_response_df %>%
  filter(prolific_subject_id == "63469e0a9552017beb3cfb82" | prolific_subject_id == "66c1c5cb4b460dacc8290b9e" | prolific_subject_id == "58fa3437f718c900016ea5e4") %>%
  select("Free response", "Focus topic")

kable(fr_family, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")

```

Context/setting constraints: ChatGPT reported that some participants identified situational appropriateness as the cause of their reluctance to engage. While situational willingness is indeed a very important addition to dispositional willingness in real-life contexts, we are not tackling situational awareness in this study. Also, it could be argued that situational willingness ultimately goes back to concerns such as straining relationships, not having a productive conversation, and other concerns that we have already included.

```{r}
#| label: tbl-bandwith-free-response
#| echo: false
#| fig-cap: Table of example free responses that were concerned with emotional and cognitive fatigue that might follow from participating in a conversation about a controversial topic


fr_energy <- free_response_df %>%
  filter(prolific_subject_id == "61074ae762b0186a302f1028" | prolific_subject_id == "6648becccd1bf5a6e253577b" | prolific_subject_id == "565bff58c121fe0005fc390d") %>%
  select("Free response", "Focus topic")

kable(fr_energy, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")

```

Emotional/cognitive bandwith: According to ChatGPT-5, many participants reported that it was not worth their energy to engage in these conversations. We originally excluded this concern because we anticipated that lacking the energy for these types of conversations and/or not wanting to expend energy on them is a product of the concerns that we included. For example, in the free response "It's not worth my energy to try to push my ideas or beliefs onto someone else. They will believe what they want," the participant does not want to expend energy because they don't think the others will listen to them. However, many of our nineteen items have the potential to be products of each other (e.g., "I might feel disempowered, unheard, or invalidated" and "The conversation could negatively affect how I feel about myself during and/or afterward (e.g., fear, sadness, anger, vulnerability)"), so the potential inter-relatedness of emotional/cognitive fatigue and our other reasons shouldn't prevent inclusion of this concern. Also, it is very possible that some participants who identify with the concern about emotional/cognitive energy might not identify with the related reasons that we have written, and thus we would not capture their concerns properly without including the energy concern itself. This concern could be captured by an item like "It wouldn't be worth the emotional and/or mental energy."

Through manual review and searching for representative examples of the uncaptured concerns identified by the LLMs, it also became evident that hurting personal relationships, or upsetting people with whom the participant is close, is a large concern for many individuals. This concern was often discussed in a personal or intimate sense, distinguishing it from the more abstract or broad concept of social reprecussions. Because of this, perhaps we should have hurting personal relationships be separate from our item "I could incur social repercussions (e.g., being excluded by others in the future, putting a strain on my relationships, negative changes to my reputation)." We could have one item be "I would not want to hurt my relationship(s) with others or upset them" and another be "I could incur social reprecussions (e.g., being excluded by others in the future, negative changes to my reputation, feeling ostracized)."

Another reason for not engaging that many people expressed was that the conversation would be a waste of time because others' views wouldn't change. This could be considered a subset of the concern that the others wouldn't keep the conversation productive; however, perhaps it should be its own concern because so many people specifically phrased their thoughts in this way. This is probably also a concern that MOCA can't help with; however, for the sake of capturing people's reasons for unwillingness, perhaps it is worth including.

A final item that we could consider adding is "I would be scared for my safety if I expressed my thoughts." This concern probably would not be chosen by many, if any, participants; however, if can be a real concern in various contexts and is a very significant reason for not engaging when it is a factor at play. This is also something that MOCA wouldn't be able to target, though.

```{r}
#| label: tbl-free-response-tally-plot
#| echo: false
#| tbl-cap: Table showing subjects' responses to free response question. [insert the question here for real data analysis]

# CHANGE THIS CODE AS NEEDED BASED ON HOW THE DF IS STRUCTURED 

### NEED TO CREATE A DF WITH ADDITIONAL REASONS FOUND IN THE FREE RESPONSE BEFORE I CAN DO THIS
# 
# free_response_bar_plot <- ggplot(INSERT_DF_NAME_HERE, aes(x = count, y = reorder(statement, count))) +
#   geom_col(fill = "cornflowerblue") +
#   labs(
#     x = "Number of Participants",
#     y = NULL,
#     title = "Number of Participants Endorsing Each Statement"
#   ) +
#   theme_minimal(base_size = 13)
# 
# print(free_response_bar_plot)

```

### Factor analysis

```{r}
#| label: Wrangle data for factor analysis
#| echo: false

# create short labels for each question
question_key_for_fa <- surveys_df %>%
  filter(phase == "follow-up-unwillingness-concerns-survey") %>%
  ungroup() %>%
  select(question) %>%
  distinct(question) %>%
  arrange(question) %>%  # ensure consistent order
  mutate(short_label = paste0("Q0", row_number()))

# join short labels back to the main data
surveys_df_labeled_questions <- surveys_df %>%
  filter(phase == "follow-up-unwillingness-concerns-survey") %>%
  left_join(question_key_for_fa, by = "question") %>%
  select(prolific_subject_id, question, short_label, answer)

concerns_labeled_wide <- surveys_df_labeled_questions %>%
  select(prolific_subject_id, short_label, answer) %>%
  pivot_wider(names_from = short_label, values_from = answer) %>%
  mutate(across(starts_with("Q0"), ~ as.numeric(as.character(.)))) %>%
  ungroup() %>%
 mutate(across(everything(), ~ replace_na(.x, 1))) # making the concerns that participants weren't even worried about into a 1 so that they can be represented in the likert graph as "not at all"

concerns_wide <- concerns_labeled_wide %>%
  select(-prolific_subject_id)
```

```{r}
#| label: tbl-correlation-matrix
#| echo: false
#| tbl-cap: Correlation matrix of reasons for unwillingness

cor_matrix <- cor(concerns_wide, use = "pairwise.complete.obs")

library(corrplot)
corrplot <- corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7)

# the items all seem to show low to moderate correlations. pretty solid
```

```{r}
#| label: Check KMO, Bartlett's test, and Rit values to see if data is well-suited for factor analysis 
#| include: false 

# KMO and Bartlett’s test
kmo <- KMO(concerns_wide)
cortest_bartlett <- cortest.bartlett(cor(concerns_wide), n = nrow(concerns_wide))

#kmo is above 0.8 so our data is well-suited for factor analysis!  

#cortest_bartlett shows that the p-value is VERY small, so correlations are statistically significant 

#check internal consistency
rit.vals <- alpha(concerns_wide)

#raw alpha is 0.88 which is pretty good 
#all the r.cor values are pretty good 

# all indicators suggest that the data is well-suited for factor analysis 

```

```{r}
#| label: Factor analysis on 19 items 
#| include: false

# determine number of factors with parallel analysis. this will output a suggested number of factors and a scree plot 
fa_parallel_results <- fa.parallel(concerns_wide, fa = "fa")

# the results of parallel analysis suggest 3 factors 

# run factor analysis with 3 factors 
## we use oblimin because the factors are probably correlated 
fa_result <- fa(
    concerns_wide,
    nfactors = 3,
    rotate = "oblimin"
    )

#get explained variance for report 
total_var_explained_percent <- round(sum(fa_result$Vaccounted["Proportion Var", ]) * 100, 1)

#get factor correlations 
factor_corrs <- fa_result$r 

# extract min and max off-diagonal correlations
cor_range <- range(factor_corrs[lower.tri(factor_corrs)])
cor_min <- round(cor_range[1], 2)
cor_max <- round(cor_range[2], 2)
```

```{r}
#| label: tbl-factor-loadings-visual
#| echo: false
#| tbl-cap: Table visualizing factor loadings in exploratory factor analysis.

library(tidyverse)

# visualize factor loadings
loadings_df <- as.data.frame(fa_result$loadings[1:19, ])
loadings_df$short_label <- rownames(loadings_df)

loadings_long <- loadings_df %>% 
  pivot_longer(-short_label, names_to = "factor", values_to = "loading")

factor_loadings_plot <- ggplot(loadings_long, aes(x = reorder(short_label, loading), y = loading, fill = factor)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(
    title = "Factor Loadings",
    x = "Question",
    y = "Loading",
    fill = "Factor"
  ) +
  theme_minimal()

# not printing this because i prefer the heat map 
```

```{r}
#| label: tbl-fa-heat-map-df
#| echo: false
#| tbl-cap: Table showing factor analysis heat map.

fa_heat_map <- ggplot(loadings_long, aes(x = factor, y = reorder(short_label, desc(short_label)), fill = loading)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    midpoint = 0, limit = c(-1, 1), name = "Loading"
  ) +
  labs(
    title = "Heat Map of Factor Loadings",
    x = "Factor",
    y = "Question"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10))

print(fa_heat_map)

```

```{r}
#| label: Interpreting the factors
#| echo: false 


```

We conducted an exploratory factor analysis on the 19 items using oblimin rotation to explore potential latent factors reflecting correlated concerns. This analysis yielded a three-factor solution explaining `r total_var_explained_percent`% of the total variance. The factors were moderately correlated (r = `r cor_min`–`r cor_max`) as shown in Figure \@ref(fig:tbl-correlation-matrix). Items tended to load cleanly on one factor, though several cross-loadings were present, suggesting some conceptual overlap among factors. Because the analysis was exploratory, all items were retained to allow full examination of the emerging structure.

The first factor seems to reflect emotional vulnerability or threat to self-concept. This factor consists of: "I might take offense to what someone says," "Someone could challenge ideas or beliefs that play an important role in making me who I am," "I might feel disempowered, unheard, or invalidated," "The conversation could negatively affect how I feel about myself during and/or afterward (e.g., fear, sadness, anger, vulnerability)," "Someone could criticize or show disapproval of my views," and "The conversation could negatively affect how I feel about the world during and/or afterward."

The second factor appears to capture concerns about conversational efficacy and engagement. The focus here is on whether the conversation will be respectful, worthwhile, or productive. This factor consists of: "I might struggle to remain productive and respectful," "I would not trust the others to keep the conversation productive and respectful (e.g., keeping emotions in check, refraining from making hostile remarks, not dominating the conversation)", "I would not want to listen to the others discuss their views", and "It would be a waste of time because my views wouldn't change."

The third factor seems to represent social evaluation or social risk concerns. It emphasizes concern about how others perceive the participants, including fear of embarrassment, judgment, or negative social consequences. This factor consists of: "I could incur social repercussions (e.g., being excluded by others in the future, putting a strain on my relationships, negative changes to my reputation)", "I would feel like my opinion is unpopular amongst the group," "I could feel humiliated or embarrassed in the conversation," "I might struggle to explain my views to the others and/or come across as uninformed," "My opinion is unpopular in general."

Four items did not load above 0.32 on any factor in the 3-factor solution nor in an additional exploratory 4-factor solution. These items were: "Someone might take offense to what I say," "The conversation could become awkward or tense," "The others might not make a full effort to hear and understand me," and "I don’t often participate in conversations like these". It would be expected that these items represent idiosyncratic concerns or situational/contextual items that don't fit neatly into emotional vulnerability, process, or social evaluation; however, these items seem as though they would fit within the social evaluation factor.

It is also interesting that factors 1 and 3 are distinct from each other, as most of the items within these factors seem as though they could very well be interchangeable in the factors. For example, "I could feel humiliated or embarrassed in the conversation" fell within factor 3 with a loading of `r loadings_df$MR3[loadings_df$short_label == "Q01"]`, but conceptually could very well fit within the emotional vulnerability concepts of factor 1 even though it had a loading within factor 1 of `r loadings_df$MR1[loadings_df$short_label == "Q01"]`.

### Correlations

```{r}
#| label: Correlations open-mindedness average vs. willingness concerns 
#| echo: false
#| tbl-cap: Correlation matrix of average open-mindedness vs. willingness concerns 

# look at correlation relationship between mean open-mindedness score and each willingness concern
library(broom)

cor_openmindedness_mean_vs_concerns <- concerns_wide %>% map_dfr(~ tidy(cor.test(.x, openmindedness_df_mean$mean_openmindedness)), .id = "concern")

# view results
cor_openmindedness_mean_vs_concerns_simplified <- cor_openmindedness_mean_vs_concerns %>%
  select(concern, estimate, p.value) %>%
  arrange(p.value)

# extract the relevant correlations and p-values
q05_r <- cor_openmindedness_mean_vs_concerns |> filter(concern == "Q05") |> pull(estimate)
q05_p <- cor_openmindedness_mean_vs_concerns |> filter(concern == "Q05") |> pull(p.value)

q06_r <- cor_openmindedness_mean_vs_concerns |> filter(concern == "Q06") |> pull(estimate)
q06_p <- cor_openmindedness_mean_vs_concerns |> filter(concern == "Q06") |> pull(p.value)

q016_r <- cor_openmindedness_mean_vs_concerns |> filter(concern == "Q016") |> pull(estimate)
q016_p <- cor_openmindedness_mean_vs_concerns |> filter(concern == "Q016") |> pull(p.value)
```

A correlation matrix of average open-mindedness vs. each willingness concern found statistically significant relationships between three concerns and average open-mindedness. These concerns were "I might struggle to explain my views to the others and/or come across as uninformed" (*r* = `r round(q06_r, 3)`, *p* = `r signif(q06_p, 3)`), "I might struggle to remain productive and respectful" (*r* = `r round(q05_r, 3)`, *p* = `r signif(q05_p, 3)`), and "The conversation could become awkward or tense" (*r* = `r round(q016_r, 3)`, *p* = `r signif(q016_p, 3)`). These three concerns showed inverse associations with open-mindedness, meaning people with higher open-mindedness scores tended to express less concern for those specific items. This is very interesting but I'm not sure what to draw from it.

```{r}
#| label: Correlations willingness slider average vs. willingness concerns
#| echo: false

#prepping data
reasons_and_willingness_merged_df <- merge(wide_unwill_reasons_df_1, sliders_df, by = 'prolific_subject_id') %>%
  ungroup() %>%
  select(-prolific_subject_id)

#run cor test
cor_willingness_slider_vs_concerns <- map_dfr(reasons_and_willingness_merged_df[-20], ~
  tidy(cor.test(.x, reasons_and_willingness_merged_df$mean_willingness, use = "pairwise.complete.obs")),
  .id = "reason_item")

#process results 
cor_willingness_slider_vs_concerns <- cor_willingness_slider_vs_concerns %>%
  mutate(
    significance = case_when(
      p.value < 0.001 ~ "< 0.001",
      p.value < 0.01  ~ "< 0.01",
      p.value < 0.05  ~ "< 0.05",
      TRUE            ~ "ns"  # not significant
    ),
    direction = if_else(estimate < 0, "negative", "positive"),
    estimate_rounded = round(estimate, 3),
    pvalue_rounded = signif(p.value, 3)
  ) %>%
  select(reason_item, estimate_rounded, pvalue_rounded, direction, significance)

```

Next, we looked for correlations between willingness concerns and overall willingness as indicated by the slider questions. Most concerns were significantly negatively associated with willingness. The strongest negative associations were observed for "Doesn't participate in these conversations often" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Doesn't participate in these conversations often"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Doesn't participate in these conversations often"]`), "Conversation might be awkward or tense" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Conversation might be awkward or tense"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Conversation might be awkward or tense"]`), "Might be offended" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Might be offended"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Might be offended"]`), "Others might be unproductive or disrespectful" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Others might be unproductive or disrespectful"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Others might be unproductive or disrespectful"]`), "Doesn't want to listen to others" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Doesn't want to listen to others"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Doesn't want to listen to others"]`), "Views wouldn't change" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Views wouldn't change"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Views wouldn't change"]`), and "Might feel invalidated" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Might feel invalidated"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Might feel invalidated"]`).

Other significant negative associations were observed for "Might offend someone" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Might offend someone"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Might offend someone"]`), "Experience negative feelings about the world" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Experience negative feelings about the world"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Experience negative feelings about the world"]`), "Might struggle to explain views or seem uninformed" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Might struggle to explain views or seem uninformed"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Might struggle to explain views or seem uninformed"]`), "Might struggle to remain productive and respectful" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Might struggle to remain productive and respectful"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Might struggle to remain productive and respectful"]`), "Opinion is unpopular amongst group" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Opinion is unpopular amongst group"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Opinion is unpopular amongst group"]`), "Might be criticized" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Might be criticized"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Might be criticized"]`), "Others won't make effort to listen" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Others won't make effort to listen"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Others won't make effort to listen"]`), and "Might experience negative feelings about self" (`r cor_willingness_slider_vs_concerns$estimate_rounded[cor_willingness_slider_vs_concerns$reason_item == "Might experience negative feelings about self"]`, p `r cor_willingness_slider_vs_concerns$significance[cor_willingness_slider_vs_concerns$reason_item == "Might experience negative feelings about self"]`).

### Cluster analysis

```{r}
#| label: Cluster analysis
#| echo: false

# fill in the not at alls
concerns_wide <- concerns_wide %>%
  mutate(across(everything(), ~ replace_na(.x, 1))) 
 
# scale 
concerns_wide_scaled <- scale(concerns_wide) 

# determine optimal number of clusters using wss 
optimal_n_clusters <- fviz_nbclust(concerns_wide_scaled, kmeans, method = "wss")

# K-means with k = 3 (example)
set.seed(1)
kmeans_result <- kmeans(concerns_wide_scaled, centers = 3)

# merge back with subject id df so we can see sub id, cluster, and the questions w ratings
cluster_labeled <- concerns_labeled_wide %>%
  mutate(cluster = factor(kmeans_result$cluster))

# extract cluster info to add to surveys_df becuase the df is structured differently so i'm not sure how to merge otherwise 
cluster_info <- cluster_labeled %>%
  select(prolific_subject_id, cluster)

surveys_df <- surveys_df %>%
  left_join(cluster_info, by = "prolific_subject_id")

#extract the percent of participants in each cluster so i can report
cluster_percentages <- cluster_info %>%
  group_by(cluster) %>%                       # group by cluster
  summarise(n = n_distinct(prolific_subject_id)) %>%  # count unique subjects
  mutate(percent = round(n / sum(n) * 100, 1))       # calculate percentage

```

```{r}
#| label: tbl-cluster-analysis-pca
#| include: false
#| fig-cap: PCA plot with clusters to visualize the three clusters

# visualize clusters with PCA
cluster_visualization <- autoplot(prcomp(concerns_wide_scaled), data = cluster_labeled, colour = 'cluster')

print(cluster_visualization)

```

```{r}
#| label: fig-heat-map-clusters-willingness-concerns
#| echo: false 
#| fig-cap: Heat map of each cluster's average rating of each potential concern

# figure out the average score on each question of a cluster so we can see if certain clusters generally scored higher on certain questions 

cluster_concerns <- cluster_labeled %>%
  group_by(cluster) %>%
  summarise(across(starts_with("Q"), mean, na.rm = TRUE))

#heat map

cluster_long <- cluster_concerns %>%
  pivot_longer(cols = starts_with("Q"), names_to = "question", values_to = "mean_rating")

ggplot(cluster_long, aes(x = question, y = factor(cluster), fill = mean_rating)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(y = "Cluster", x = "Concern", fill = "Mean Rating") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
#| label: Interpreting clusters and concerns 
#| echo: false 

# find each cluster's overall mean rating to see if any clusters generally rate things significantly higher 
cluster_means <- cluster_concerns %>%
  rowwise() %>%
  mutate(overall_mean = round(mean(c_across(Q01:Q019)), 1)) %>%
  select(cluster, overall_mean)

# find which questions had the greatest range in average responses from clusters to see which differ most between clusters 

top_questions <- cluster_concerns %>%
  pivot_longer(cols = c(Q01,Q02,Q03,Q04,Q05,Q06,Q07,Q08,Q09,Q010,Q011,Q012,Q013,Q014,Q015,Q016,Q017,Q018,Q019), names_to = "question", values_to = "mean_rating") %>%
  group_by(question) %>%
  summarize(range = max(mean_rating) - min(mean_rating)) %>%
  arrange(desc(range)) %>%
  slice_head(n = 19) %>%   # top 19 questions with largest differences
  pull(question)

# create a table of clusters × top questions
table_concerns <- cluster_concerns %>%
  select(cluster, all_of(top_questions)) %>%
  pivot_longer(cols = -cluster, names_to = "question", values_to = "mean_rating") %>%
  arrange(question, desc(mean_rating)) %>%
  group_by(question) %>%
  mutate(rank = row_number()) %>%
  select(question, cluster, mean_rating, rank)


### cluster-focused

cluster_long <- cluster_concerns %>%
  pivot_longer(cols = c(Q01,Q02,Q03,Q04,Q05,Q06,Q07,Q08,Q09,Q010,Q011,Q012,Q013,Q014,Q015,Q016,Q017,Q018,Q019), names_to = "question", values_to = "mean_rating")

# for each question, find which cluster had the highest mean
top_by_cluster <- cluster_long %>%
  group_by(question) %>%
  mutate(max_cluster = mean_rating == max(mean_rating)) %>%
  ungroup() %>%
  filter(max_cluster) %>%
  select(cluster, question, mean_rating) %>%
  arrange(cluster, desc(mean_rating))

# join top_by_cluster with questions_key_fa to get full question text
top_by_cluster_full <- top_by_cluster %>%
  left_join(question_key_for_fa, by = c("question" = "short_label")) %>%
  mutate(across(everything(), ~ gsub('unwillingness-reason-', '', .))) 

## for inline report: 
# cluster 1 top questions
cluster1_top <- top_by_cluster_full %>%
  filter(cluster == 1) %>%
  arrange(desc(mean_rating)) %>%
  pull(question.y)

# cluster 2 top questions
cluster2_top <- top_by_cluster_full %>%
  filter(cluster == 2) %>%
  arrange(desc(mean_rating)) %>%
  pull(question.y)

# cluster 3 top questions
cluster3_top <- top_by_cluster_full %>%
  filter(cluster == 3) %>%
  arrange(desc(mean_rating)) %>%
  pull(question.y)

```

```{r}
#| label: fig-clusters-avg-willingness   
#| echo: false 
#| fig-cap: Box plot of average willingness to converse about topic by cluster. On average, cluster 2 appears to be more willing to converse than the other clusters; this would be consistent with cluster 2's lower average rating of how much the 19 items impact willingness.

avg_willingness_cluster_df <- avg_willingness_df %>%
  merge(cluster_info, by = "prolific_subject_id")

ggplot(avg_willingness_cluster_df, aes(x = factor(cluster), y = avg_willingness)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "Average Willingness") +
  theme_minimal()

```

```{r}
#| label: fig-clusters-avg-openmindedness 
#| echo: false 
#| fig-cap: Box plot of average open-mindedness about topic by cluster. 

open_mindedness_w_clusters <- openmindedness_df_mean %>%
  merge(cluster_info, by = "prolific_subject_id")

ggplot(open_mindedness_w_clusters, aes(x = factor(cluster), y = mean_openmindedness)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "Average Open-Mindedness") +
  theme_minimal()

```

We also conducted cluster analysis on the 19 concerns to determine whether participants generally fell into certain categories. The analysis suggested that participants generally fell into one of three clusters, with `r cluster_percentages$percent[cluster_percentages$cluster == 1]`% of participants falling into Cluster 1, `r cluster_percentages$percent[cluster_percentages$cluster == 2]`% of participants falling into Cluster 2, and `r cluster_percentages$percent[cluster_percentages$cluster == 3]`% of participants falling into Cluster 3.

Consistent with Figure \@ref(fig:heat-map-clusters-willingness-concerns), Cluster 1 and Cluster 3, on average, indicated that the 19 concerns overall impact their willingness much more compared to Cluster 3. Cluster 1's average rating of a given concern was `r cluster_means$overall_mean[cluster_means$cluster == 1]`, Cluster 2's average rating of a given concern was `r cluster_means$overall_mean[cluster_means$cluster == 2]`, and Cluster 3's average rating of a given concern was `r cluster_means$overall_mean[cluster_means$cluster == 3]`.

On average, Cluster 1 rated the following concerns the highest: `r paste(cluster1_top, collapse = "; ")`.

Cluster 2 did not rate any concern higher, on average, than the other clusters.

On average, cluster 3's top concerns were `r paste(cluster3_top, collapse = "; ")`.

These patterns suggest that Cluster 3 participants were the most concerned across the set of items, Cluster 1 participants were slightly less concerned, and Cluster 2 participants were much less concerned, which is particularly interesting since Cluster 2 participants made up half of the sample.

This appears to be consistent with the average willingness of each cluster: as shown in Figure \@ref(fig:fig-clusters-avg-willingness), Cluster 2 appears to be more willing to converse than the other two clusters. Interestingly, as shown in Figure \@ref(fig:fig-clusters-avg-openmindedness), Cluster 2 also appears to be more open-minded than the other two clusters. These interpretations have not been rigorously assessed and it should be recalled that these data were self-reported.

```{r}
#| label: fig-demographics-clusters
#| echo: false 
#| fig-cap: Plots of demographics by cluster helps us explore whether the clusters may have significantly different demographics.

ggplot(surveys_df, aes(x = cluster, fill = Sex)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Gender composition by cluster")

ggplot(surveys_df, aes(x = cluster, fill = topicChoiceAsString)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Topic choice by cluster")

ggplot(surveys_df, aes(x = cluster, fill = `Highest education level completed`)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Education level by cluster") # idk why this isn't working

ggplot(surveys_df, aes(x = cluster, fill = Ethnicity)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Ethnicity by cluster")

ggplot(surveys_df, aes(x = cluster, fill = `U.s. political affiliation`)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "U.s. political affiliation")

ggplot(surveys_df, aes(x = cluster, fill = `Political spectrum (us)`)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Political spectrum (us) by cluster")

ggplot(surveys_df, aes(x = cluster, fill = `Religious affiliation`)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Religious affiliation by cluster")

ggplot(surveys_df, aes(x = cluster, y = as.numeric(Age))) +
  geom_boxplot() +
  labs(title = "Age by cluster")

```

```{r}
#| label: fig-topic-choice-clusters
#| echo: false 
#| fig-cap: Plots of topic choice by cluster helps us explore whether the clusters may have significant differences in which topics they focused on. Cluster 2 appears to have had more participants focused on human euthanasia and had fewer participants focused on the role of the U.S. in the Israeli-Palestinian conflict, but the differences are not immense and further analyses should verify whether these differences are significant before drawing conclusions.

# within each cluster, finding percent of subjects on each topic. i tried to do this in one df but kept getting an error that i couldn't figure out 
cluster1_topic_percent <- surveys_df %>%
  filter(cluster == 1) %>%
  distinct(prolific_subject_id, .keep_all = TRUE) %>%
  group_by(topicChoiceAsString) %>%
  summarise(n1 = n(), .groups = "drop") %>%
  mutate(percent1 = n1 / sum(n1))

cluster2_topic_percent <- surveys_df %>%
  filter(cluster == 2) %>%
  distinct(prolific_subject_id, .keep_all = TRUE) %>%
  group_by(topicChoiceAsString) %>%
  summarise(n2 = n(), .groups = "drop") %>%
  mutate(percent2 = n2 / sum(n2))

cluster3_topic_percent <- surveys_df %>%
  filter(cluster == 3) %>%
  distinct(prolific_subject_id, .keep_all = TRUE) %>%
  group_by(topicChoiceAsString) %>%
  summarise(n3 = n(), .groups = "drop") %>%
  mutate(percent3 = n3 / sum(n3))

#join together
cluster_topic_percent <- cluster1_topic_percent %>%
  full_join(cluster2_topic_percent, by = "topicChoiceAsString") %>% 
  full_join(cluster3_topic_percent, by = "topicChoiceAsString")

ggplot(surveys_df, aes(x = cluster, fill = topicChoiceAsString)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Topic choice by cluster")

```

There may be significant differences in the demographic profiles of the clusters as shown in Figure \@ref(fig:fig-demographics-clusters); however, further analyses would need to assess statistical significance. As observed in Figure \@ref(fig:fig-topic-choice-clusters), there may also be some notable differences in the topic choices of each cluster. Particularly, `r round(cluster_topic_percent$percent2[cluster_topic_percent$topicChoiceAsString == "human euthanasia in the U.S."], 2)`% of participants in cluster 2 focused on human euthanasia in the U.S., compared to `r round(cluster_topic_percent$percent1[cluster_topic_percent$topicChoiceAsString == "human euthanasia in the U.S."], 2)`% of participants in cluster 1 and `r round(cluster_topic_percent$percent3[cluster_topic_percent$topicChoiceAsString == "human euthanasia in the U.S."], 2)`% of participants in cluster 3. Also, `r round(cluster_topic_percent$percent2[cluster_topic_percent$topicChoiceAsString == "the role of the U.S. in the Israeli-Palestinian conflict"], 2)`% of participants in cluster 2 focused on the role of the U.S. in the Israeli-Palestinian conflict, compared to `r round(cluster_topic_percent$percent1[cluster_topic_percent$topicChoiceAsString == "the role of the U.S. in the Israeli-Palestinian conflict"], 2)`% of participants in cluster 1 and `r round(cluster_topic_percent$percent3[cluster_topic_percent$topicChoiceAsString == "the role of the U.S. in the Israeli-Palestinian conflict"], 2)`% of participants in cluster 3. These differences have also not been assessed for statistical significance.

## Discussion
