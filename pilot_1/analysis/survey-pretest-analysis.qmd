---
title: "survey-pretest-analysis"
format: html
bibliography: mocareferences.bib
citation-location: document
reference-location: section
editor: visual
---

\*# intended for summer 2025

```{r}
#| label: Data Fetching. Load R packages
#| include: false 

library(osfr) 
library(dplyr)
library(ez) 
library(tidyverse) 
library(gt)
library(gtsummary)
library(kableExtra)
library(stringr)
library(ggridges)
library(lubridate)  # for date handling
library(psych) # for factor analysis 
library(pheatmap) # for factor analysis heat map 
library(corrplot) # for correlation matrix 
library(broom) # for correlation tests
library(purrr) # for correlation tests 
library(likert) # for likert plots 
library(factoextra)
library(ggfortify)
library(car) # for mlr
# library(GGally)
# library(car)
# library(MASS)
# library(glmnet)

```

```{r}
#| label: Prolific demographics data
#| include: false 

## add which round of the pilot it was by reading in demographics files so we know completion dates

#read in prolific demographics files 
pretest_demo_df <- read_csv("google_sheets_fake_demo.csv") %>%
  mutate(Age = as.numeric(Age))

#merge files (if multiple)
merged_prolific_demographics_df <- bind_rows(pretest_demo_df) 

#clean df
cleaned_prolific_demographics_df <- merged_prolific_demographics_df %>%
# make name the same as in data csvs so merging is easier
rename("prolific_subject_id" = "Participant id") %>%
# filter out people who didn't finish
filter(Status != "RETURNED") %>%
#rename "Started at" to make grepl function easier to use later on
rename("start" = "Started at") %>%
# make time more readable but keep the seconds version for analysis stuff later
mutate(`time_taken_overall_minutes` = as.numeric(`Time taken`) / 60) %>%
# name the round of the pilot that the person was in 
  mutate(pilot_iteration = case_when(
    grepl(c("2025-08-14"), start) ~ "Willingness Survey Pretest",
    TRUE ~ NA_character_
  )) 
```

```{r}
#| label: Retrieve data for the specific date of the willingness pretest from our OSF project. Loading into a data folder, which  we manually create
#| include: false 

# list all files
files <- osf_retrieve_node("x32pv") %>%
  osf_ls_files(n_max = Inf)

# filter for the correct date
files_filtered <- files %>%
  mutate( date_created = map_chr(meta, ~ as.character(.x$attributes$date_created)),
    date_created = as_date(date_created)
  ) %>%
  filter(date_created %in% as_date(c("2025-08-14")))

# download files for the correct date 
osf_download(files_filtered, path = "data", conflicts = "skip")

```

```{r}
#| label: Bind csv's
#| include: false 

osf_csv_filenames <- list.files(path = "data/")
raw_dfs_tibbles <- map(osf_csv_filenames, ~read_csv(file.path("data/", .)))
rawwwwwww_df <- bind_rows(raw_dfs_tibbles)

```

```{r}
#| label: Merge prolific demos + create a condition type column
#| include: false 

#REMOVE IN REAL DATA ANALYSIS 
rawwwwwww_df$prolific_subject_id <- 1234

raw_df <- rawwwwwww_df %>%
  # inner join with the demographics df from pilot 2 so we can eliminate pilot 1 subjects
  inner_join(cleaned_prolific_demographics_df, by = "prolific_subject_id") %>%
  # remove unnecessary syntax from topicChoiceAsString for readability later
  mutate(topicChoiceAsString = gsub('\\{"row":"', '', topicChoiceAsString)) %>%
  mutate(topicChoiceAsString = gsub('"}', '', topicChoiceAsString)) %>%
  # fill condition column and row and topic strings
  group_by(prolific_subject_id) %>%
  fill(topicChoiceAsString, .direction = "downup") %>%
  ungroup() 

# number of subjects in data 
total_num_subjects_in_data <- nrow(raw_df %>%
  group_by(prolific_subject_id) %>%
  summarize(total_num_subjects_in_data = n()))

```

```{r}
#| label: Clean the survey df
#| include: false

surveys_cleaning1_df <- raw_df %>%
  # Filter for only surveys
  filter(trial_type == "survey") %>%
  # Remove control interventions (these are coded as surveys)
  filter(phase != "control-intervention") %>%
  # Remove the matrix question titles because they mess with separating questions from answers
  # openmindedness and unwillingness reason
  mutate(across(everything(), ~ gsub(',"openmindedness":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"openmindedness":', '', .))) %>%
  mutate(across(everything(), ~ gsub(',"unwillingness-reason":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"unwillingness-reason":', '', .))) %>%
  # republican/democrat (pre-intervention)
  mutate(across(everything(), ~ gsub(',"rating-republicans":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub(',"rating-democrats":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-republicans":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans":null', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats":null', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-republicans":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats":', '', .))) %>%
  # republican/democrat (post-intervention)
  mutate(across(everything(), ~ gsub('"rating-republicans-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-republicans-post":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats-post":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans-post":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats-post":', '', .))) %>%
  # the name of the question in 2.1 run of the experiment
  mutate(across(everything(), ~ gsub('topicChoice":', '', .))) %>%
  # the name of the question in future runs of the experiment
  mutate(across(everything(), ~ gsub('topic":', '', .))) %>%
  mutate(across(everything(), ~ gsub(',null,', '', .))) 

  
surveys_cleaning2_df <- surveys_cleaning1_df %>%
  # Remove weird characters but don't remove quotations because we need them for separating questions
  mutate(across(everything(), ~ gsub('}', '', .))) %>%
  mutate(across(everything(), ~ gsub('\\{', '', .))) %>%
  mutate(across(everything(), ~ gsub('\\[', '', .))) %>%
  mutate(across(everything(), ~ gsub('\\]', '', .))) %>%
  # Make each question its own row
  separate_rows(response, sep = ',"') %>%
  # Remove any leftover quotation marks
  mutate(across(everything(), ~ gsub('"', '', .))) %>%
  # Move answers to a new column
  separate(response, into = c("question", "answer"), sep = ":", extra = "merge", fill = "right") %>%
  # Remove all remaining questions that were answered with null (like the slider questions, because the slider 'placeholder' questions are where people actually answer the slider questions)
  filter(!is.na(answer) & answer != 'null' & answer != 'null,null') %>%
  mutate(prolific_subject_id = as.numeric(prolific_subject_id))

```

```{r}
#| label: Reverse code necessary items 
#| include: false 

# reverse code open-mindedness items 

surveys_cleaning3_df <- surveys_cleaning2_df %>%
   mutate(answer = ifelse(
         grepl("@R@", question)  | 
         grepl("openmindedness...believe it is a waste of time to pay attention to certain political ideas about", question), # CAN REMOVE IN REAL DATA ANALYSIS. i forgot to put an @R@ next to that one in the experiment code. fixed it now 
     8 - as.numeric(answer),
     answer
   )) 

```

```{r}
#| label: Remove unnecessary columns 
#| include: false 

surveys_df <- surveys_cleaning3_df %>%
  select(-c(stimulus, trial_type, trial_index, time_elapsed, internal_node_id, study_id, session_id, unwillingness_yes_rows, "Submission id", Status, "Custom study tncs accepted at", "Completed at", "Reviewed at", "Archived at", "Completion code", "Total approvals"))

```

```{r}
# REMOVE IN REAL ANALYSIS 

surveys_df <- surveys_df %>%
  mutate(prolific_subject_id = if_else(row_number() > 50, 5678, prolific_subject_id))
         
```

```{r}
#| label: Create a df that flags people who failed the attention check
#| include: false

#see who passes the check
attention_check <- surveys_df %>%
  filter(question == "attention-check-slider-pre-placeholder") %>%
  mutate(flag = case_when(
    sum(answer == "0.00") == 0 ~ "fail (single)", # single fail means we exclude their data but we still have to pay them on prolific 
    sum(answer == "0.00") == 1 ~ "pass", #good subject :3
    TRUE ~ NA_character_
  )) %>%
  ungroup() %>%
  select(flag, question, answer, prolific_subject_id) 

#get list of all subject id's so we can see which subjects didn't even answer the question (the sliders aren't recorded in our df if the subject doesn't move them. they naturally sit at 50.) 
all_ids <- surveys_df %>%
  distinct(prolific_subject_id)

#filter for the question of interest
attention_ids <- surveys_df %>%
  filter(question == "attention-check-slider-pre-placeholder") %>%
  distinct(prolific_subject_id) 

#see who's missing
missing_ids <- setdiff(all_ids$prolific_subject_id, attention_ids$prolific_subject_id)

#make a table for these missing people 
missing_rows <- tibble(
  flag = "fail (single)",
  question = NA_character_,
  answer = NA_character_,
  prolific_subject_id = missing_ids,
  time_taken_overall_minutes = NA_real_
)

# combine those who answered and those who didn't answer
attention_check <- bind_rows(attention_check, missing_rows)

# extract value for text 
total_failed_attention_checks <- attention_check %>%
  # excluding people for both fails
  filter(flag == 'fail (single)') %>%
  summarize(total_failed_attention_checks = n_distinct(prolific_subject_id)) %>%
  pull(total_failed_attention_checks)

#extract list of ids so we can remove the data
failed_attention_check_ids <- attention_check %>%
  filter(flag == "fail (single)") %>%
  distinct(prolific_subject_id) %>%
  pull(prolific_subject_id)
```

```{r}
#| label: Remove subs who failed attention check 
#| include: false

# remove bad subs
raw_df <- raw_df %>%
  filter(!(prolific_subject_id %in% failed_attention_check_ids))
    
surveys_df <- surveys_df %>%
  filter(!prolific_subject_id %in% failed_attention_check_ids) 
  
total_num_clean_subjects_in_data <- nrow(raw_df %>%
  group_by(prolific_subject_id) %>%
  summarize(total_num_clean_subjects_in_data = n()))

```

## Introduction

Existing literature indicates that a complex interplay of psychological factors—including fear of social isolation (Spiral of Silence), identity protection (Social Identity Theory), communication anxiety (Communication Apprehension), and emotional dysregulation—converge with sociocultural influences like opinion climate, misinformation, and cognitive biases to deter open dialogue. People avoid controversial discussions primarily due to perceived threats to their social standing, personal identity, and emotional well-being. This avoidance is often a coping mechanism, reinforced by group dynamics and the spread of misinformation, ultimately hindering constructive discourse and critical thinking. Understanding this phenomenon is critical for fostering democratic citizenship, promoting critical thinking, and cultivating inclusive learning environments.

These problems can be combated on systemic and individual levels. Based on the information that will follow in this report, interventions on the individual level can include reducing communication apprehension, providing tools for difficult conversations, and building skills such as emotional regulation, respectful dissent, sensitivity towards others, and open-minded perspective-taking. We are focusing on the individual level through one-on-one human-chatbot conversations. These conversations are about controversial topics with strong moral components, which are thought to evoke many of the factors that play into individual unwillingness to converse. By curating a space where users are led to cultivate and practice the individual-level interventions which should increase users’ competencies in navigating difficult conversations, we hope to increase users’ willingness to participate in these difficult conversations.

### Limitations

The literature clearly indicates that while individual skills like emotional regulation, communication competence, and conflict management are vital for engaging in difficult conversations, their effective application is deeply intertwined with the social environment. Thus, by not directly intervening on the systemic level, which includes aspects such as creating a psychologically safe environment, challenging negative norms, and promoting inclusivity, we are missing pieces of the puzzle that significantly influence individual willingness, especially “psychological safety.”

Psychological safety is the belief that one can speak openly and truthfully about problems without fear of reprisal; it is built upon three core pillars: care, consistency, and normalizing mistakes. Psychological safety is suggested to largely relate to group culture and norms in a conversation. Through developing greater abilities to navigate difficult conversations, users of our intervention may influence future group climates, which, in turn, could contribute to each member’s sense of psychological safety. This idea goes hand in hand with the finding that widespread individual avoidance can contribute to the formation of a silent, unsupportive opinion climate. However, the literature suggests that psychological safety is largely curated through systemic changes, especially for creating an environment where people from marginalized communities can experience the same sense of psychological safety as conventionally celebrated voices. Future research could explore applications of chatbots for systemic interventions.

Our primary goal, though, is to focus on the direct effects of individual interventions. By focusing solely on the individual level, perhaps users will grow in their abilities to navigate these difficult conversations even in spaces with poor group cultures or norms.

## Methods

### Materials

We used custom software written with the jsPsych framework (@jspsychcite) to create the study, which participants viewed and responded to via their personal laptops or desktops. The javascript code for the experiment and RStudio code for data analysis can both be found at (<https://github.com/jodeleeuw/URSI-LLM-Convo-Bot/tree/main/pilot_1).>

Pre-registrations and all data for this study and previous pilots are available on the Open Science Framework at <https://osf.io/x32pv.> The experiment Web page can be found at \[webpage - insert for real data analysis\].

### Participants and Procedure

Through Prolific, we gathered data from `r total_num_subjects_in_data` subjects. To remain consistent with the demographic that our main experiment will target, we restricted our study to Prolific users who currently reside in the states, fluently speak English, and reported the United States as their nationality and country of birth. Per Prolific's harmful content criteria, we also restricted our study to participants who have consented to participating in studies with harmful content. Participants received a compensation of one dollar and fifty cents. We excluded data from `r total_failed_attention_checks` subjects for failing our attention check question. After applying this exclusion criteria, `r total_num_clean_subjects_in_data` subjects remain.

Upon agreeing to participate in this experiment, participants were brought to the experiment webpage, \[insert webpage for real experiment\], where they were asked to complete our questionnaire. The questionnaire was composed of questions to ascertain which of our six topics the participant was most unwilling to discuss, the participant's topic-specific open-mindedness, the participant's willingness to converse with others about the topic, and the concerns that negatively impact the user's willingness. After completing the questionnaire, participants were sent back to Prolific.

```{r}
#| label: Find time taken specifically on the surveys (by subject and overall)
#| echo: false 

#finding each subject's time taken on the survey specifically
surveys_df <- surveys_df %>%
  group_by(prolific_subject_id) %>%
  mutate(
    time_minutes_survey_only = sum(
      unique(as.numeric(rt)) / 60000),
      na.rm = TRUE
    )

#finding overall mean, min, max of how long participants took 
time_stats <- surveys_df %>%
  ungroup() %>%
  summarise(
    Mean = round(mean(time_minutes_survey_only, na.rm = TRUE), 1),
    Min = round(min(time_minutes_survey_only, na.rm = TRUE), 1),
    Max = round(max(time_minutes_survey_only, na.rm = TRUE), 1)
  ) 

mean_survey_time <- time_stats$Mean
min_survey_time <- time_stats$Min
max_survey_time <- time_stats$Max
```

```{r}
#| label: WIP - tbl-basic-demographics-table
#| echo: false
#| tbl-cap: Table of basic demographics.

### CHECK THAT THIS WORKS IN REAL DATA ANALYSIS 

#we already read in demographics earlier 

#create a demographics table from the demographics questions in our survey. turn the demographics questions from question column into their own columns with answer as the entries within the column
survey_demographics_df <- surveys_df %>%
filter(question == 'ethnicity' | question == 'political-affiliation' | question == 'gender-identification' | question == 'education-level' | question == 'religious-affiliation' | question == 'political-ideology') %>%
select(question, answer, prolific_subject_id) %>%
pivot_wider(names_from = question, values_from = answer)

#merge our two demographics tables
demo_table_df <- survey_demographics_df %>%
  left_join(cleaned_prolific_demographics_df, by = "prolific_subject_id", suffix = c("_prolific", ""))

#ensure that these are factors
demo_table_df$`education-level` <- factor(demo_table_df$`education-level`)
demo_table_df$`political-affiliation` <- factor(demo_table_df$`political-affiliation`)
demo_table_df$`political-ideology` <- factor(demo_table_df$`political-ideology`)

# define the levels in categorical variables so that they are organized in the table 
demo_table_df$`political-affiliation` <- fct_relevel(
  demo_table_df$`political-affiliation`,
  "Republican",
  "Democrat",
  "Independent",
)


demo_table_df$`education-level` <- fct_relevel(
  demo_table_df$`education-level`,
  "High school diploma or GED",
   "Some college; no degree",
  "Associate degree",
  "Bachelor's degree",
   "Master's degree",
)


demo_table_df$`political-ideology` <- fct_relevel(
  demo_table_df$`political-ideology`,
"Very conservative",
"Conservative",
"Somewhat conservative",
"Moderate",
"Somewhat liberal",
"Liberal",
"Very liberal"
)

demo_table_df <- demo_table_df %>%
  mutate(across(where(is.factor), ~ droplevels(.)))

demographics_table <- tbl_summary(demo_table_df,
 by = NULL,
  label = list(
    `time_taken_overall_minutes` = "Time taken (minutes)",
    Age = "Age (years)",
    Sex = "Sex",
    `Ethnicity simplified` = "Ethnicity",
    `political-affiliation` = "Political affiliation",
    `education-level` = "Education level",
    `religious-affiliation` = "Religious affiliation",
    `political-ideology` = "Political ideology"
  ),
  statistic = NULL,
#list(
  #  all_continuous() ~ "{mean} ({sd})",
  #  all_categorical() ~ "{n} ({p}%)"
#),
  type = list(
    `time_taken_overall_minutes` = "continuous",
    `Age` = "continuous",
    `gender-identification` = "categorical",
   `Sex` = "categorical",
    `Ethnicity simplified` = "categorical",
    `political-affiliation` = "categorical",
    `education-level` = "categorical",
    `religious-affiliation` = "categorical",
    `political-ideology` = "categorical"
  ),
  digits = list(
    all_continuous() ~ 2,
    all_categorical() ~ 0
  ),
  value = NULL,
  missing = NULL,
  missing_text = NULL,
  sort = NULL,
  percent = NULL,
 include = c(
'time_taken_overall_minutes', 
'Age', 
'Sex', 
'Ethnicity simplified', 
'political-affiliation', 
'education-level', 
'religious-affiliation', 
'political-ideology'
))
 # %>%  modify_header(label = "**whatever label i want**")

# Print the table
as_gt(demographics_table)

```

### Survey design

#### Supporting Literature

We developed our willingness questions based on existing literature about the concerns reasons that contribute to a person's unwillingness to participate in conversations, especially conversations about divisive topics.

Elisabeth Noelle-Neumann's Spiral of Silence theory, developed in the 1960s and '70s, offers a foundational understanding of how individuals' willingness to express opinions on controversial public issues is influenced by their perception of those opinions as popular or unpopular. This theory suggests that the prevailing opinion held by the majority in a specific social setting, such as a classroom or public forum, profoundly influences an individual's willingness to participate in discussions. If one's view is perceived as opposed to the majority, individuals are less likely to express it, a phenomenon directly linked to the Spiral of Silence. Individual psychological factors, such as communication apprehension and fear of conflict, are not isolated but are significantly influenced by the prevailing opinion climate.

The theory's key elements revolve around a fundamental human apprehension: the fear of social isolation, which is triggered by the belief that others will consider the individual not merely mistaken, but morally deficient. Individuals thus tend to refrain from publicly stating their views on controversial matters when they perceive that doing so would attract criticism, scorn, laughter, or other signs of disapproval.

The Social Identity Theory (SIT) suggests that when controversial issues are discussed, individuals may experience "identity threat" if their ideas and feelings about these issues are deeply connected to their identity or in-group, such as their race. Identity threat is defined as "experiences appraised as indicating potential harm to the value, meanings, or enactment of an identity.”

The accentuation of differences between in-groups and out-groups, coupled with the desire for positive distinctiveness, can intensify intergroup hostility and conflict, especially when competition or perceived threats exist. This can lead to negative perceptions of out-groups, even bordering on dehumanization, where less of certain human attributes are ascribed to the "other". Such vilifying views, often marked by beliefs that opponents are "wicked and untrustworthy", make constructive dialogue exceedingly difficult, as the "other" is seen as an adversary rather than a conversational partner.

This strong in-group cohesion, while providing social support and validation, can paradoxically foster communication siloing. SIT highlights that people identify with in-groups to enhance their self-esteem and perceive their in-group favorably. This desire for "positive distinctiveness" motivates them to accentuate differences with out-groups. When controversial topics arise, challenging an in-group's perspective can be perceived as an "identity threat", leading to defensive behaviors like derogating or distancing from out-group views. This process also contributes to "group polarization," where discussions with like-minded peers strengthen original beliefs and lead to more extreme stances.

Communication Apprehension (CA) is a broad term referring to an individual's "fear or anxiety associated with either real or anticipated communication with another person or persons". It is thought to be a fundamental psychological response to evaluation. Research indicates that approximately 70% of individuals in the US experience CA when giving a speech, with 20% having severe CA that leads to avoidance of oral communication in personal, public, and professional settings. A significant driver of communication avoidance is the fear of conflict. People often avoid communication when it is likely to lead to an unpleasant outcome, such as offending someone or entering into conflict. This avoidance is primarily driven by a desire to minimize perceived threats to self-esteem and well-being, including the fear of rejection, disapproval, criticism, humiliation, or loss of security.

CA suggests that the fear of conflict can lead people to systematically avoid "zero-sum situations"—where one party's gains are offset by another's losses—even when such avoidance is costly. This belief can erode interpersonal trust and increase hostility. Moreover, avoiding necessary difficult conversations leads to "conflict debt," which breeds resentment and allows unaddressed concerns to escalate into crises.

Individuals with high CA experience significant fear and anxiety when faced with communication. This internal discomfort, along with a fear of negative outcomes like conflict or rejection, leads them to actively avoid communication. This avoidance, while providing immediate relief from anxiety, prevents the individual from experiencing that the feared outcomes might not occur or that they can cope with the anxiety. This creates a negative reinforcement loop: avoidance reduces immediate discomfort, but simultaneously strengthens the underlying fear and the avoidance habit, making it harder to engage in future difficult conversations.

The concepts of emotional avoidance, emotional dysregulation, and affective forecasting, reveal that avoidance is not just about external threats, but also an internal struggle. Emotional avoidance refers to the deliberate effort to suppress, ignore, or distract oneself from experiencing unpleasant feelings such as fear, sadness, anger, or vulnerability. Common tactics include denial, suppression, distraction, busyness, and steering clear of situations, people, or places that might evoke uncomfortable emotions.

Emotional dysregulation (ED) is defined as the inability to regulate the intensity and quality of emotions to generate an appropriate emotional response. Emotional regulation strategies involve the ability to recognize, evaluate, modify, and manage emotions in a personal and socially acceptable way to maintain mental control over strong feelings and achieve adaptive functioning.

Affective forecasting, or hedonic forecasting, is the prediction of how one will feel in the future. Research consistently demonstrates that people are poor predictors of their future well-being. They tend to overestimate the impact and duration of negative emotions in response to loss or difficult situations. This inaccuracy occurs because individuals often focus more on what they will lose than on what will stay the same (focalism), fail to envision how their own coping skills will lessen their unhappiness (immune neglect), and fail to envision how they might develop new values (adaptation).

Thus, avoidance is also an internal struggle to manage uncomfortable feelings and a flawed prediction of future emotional states, making the perceived "cost" of engagement seem higher than it might be.

If a significant portion of a population struggles with emotional regulation and relies on avoidance as a primary coping mechanism for unpleasant feelings, this has implications beyond individual mental health. When collective emotional dysregulation is widespread, it can hinder a society's capacity for constructive dialogue on complex, emotionally charged issues. Individuals less equipped to handle the inherent discomfort of disagreement may resort to withdrawal, aggression, or rigid adherence to existing beliefs, rather than engaging in nuanced discussion. This suggests that the inability to manage emotions constructively during controversial discussions can lead to societal fragmentation, increased polarization, and a diminished capacity to find common ground or collectively problem-solve issues that require navigating diverse, often conflicting, emotional responses.

Based on this existing research, we developed a Likert-scale questionnaire which lists 19 concerns that might contribute to users' unwillingness to engage in a conversation about a topic that they are unwilling to discuss with others. We also created two questions which ask users to rate, on a scale from 0 to 100, their willingness to "have a conversation with someone who strongly disagrees with \[them\] about \[conversation topic\]" and "to have a conversation with someone who would push against \[their\] views about \[conversation topic\]," where a score of 0 is complete unwillingness, a score of 50 is neutral, and a score of 100 is absolute willingness to converse.

In our full-scale experiment, we intend to administer this questionnaire both before and after our intervention to assess which concerns, if any, our intervention can mitigate. To ensure that this survey captures all of the primary reasons that contribute to unwillingness and can be targeted on the individual level, this pretest study administered the questionnaire once and with a free-response question at the end: "\[insert the exact question for real data analysis\]

## Results

### Basic stats

On average, participants completed the questionnaire in `r mean_survey_minutes` minutes (`r min_survey_minutes`, `r max_survey_minutes`).

```{r}
#| label: WIP - fig-topic-ratings
#| echo: false
#| fig-cap: In the pre-intervention survey, each subject used a 7-pt scale to rate how comfortable they would be discussing each of the following topics. One of the subject's most strongly rated topics were randomly selected as the topic that the subject would focus on for the remainder of the experiment. The scale went from strongly uncomfortable (1), to moderately uncomfortable, a little uncomfortable, neutral (4), a little comfortable, moderately comfortable, and strongly comfortable (7).

### EDIT THIS FOR REAL DATA ANALYSIS BECAUSE WE'RE CHANGING OUR TOPICS 

# create figure/table showing distribution of ratings on topics
topic_ratings_df <- surveys_df %>%
    filter(grepl("mandating vaccines in the U.S.|human euthanasia in the U.S.|the criminal justice system in the U.S.|same-sex marriage in the U.S.|the role of the U.S. government in healthcare|gender equality in the U.S.", question)) %>%
  mutate(answer = as.numeric(answer)) %>%
  select(prolific_subject_id, question, answer, topicChoiceAsString)
  
topic_labels <- c( 
  "mandating vaccines in the U.S." = 
    "Mandating vaccines",
  "human euthanasia in the U.S." = 
    "Human euthanasia",
  "the criminal justice system in the U.S." = 
    "Criminal justice system",
  "same-sex marriage in the U.S." = 
    "Same-sex marriage",
  "the role of the U.S. government in healthcare" = 
    "Government in healthcare",
  "gender equality in the U.S." = 
    "Gender equality"
)

topic_ratings_plot <- ggplot(topic_ratings_df, aes(x = answer, y = question, fill = question)) +
  geom_density_ridges() +
  # give the qualitative meanings of 1-7
  scale_x_continuous(breaks = seq(1, 7, by = 1)) +
   scale_y_discrete(labels = topic_labels) +
  # make it look nicer
  theme_ridges() +      
  # remove the legend
  theme(legend.position = "none") +
  labs(title = "Ridgeline Plot of Ratings by Topic",
       x = "Rating",
       y = "Contentious Topic")

print(topic_ratings_plot)
```

```{r}
#| label: fig-unwillingness-reason-likert-bar-plot
#| echo: false
#| fig-cap: Likert bar plot displaying subjects' ratings of how much certain concerns contribute to their unwillingness to converse. The nineteen potential concerns were derived from existing literature and pretested in a pilot experiment. On a scale from 0 to 7, where a rating of 0 meant subject strongly disagreed and a rating of 7 meant strongly agree, subjects rated each potential concerns effect on their willingness to converse.

# only works though if all "1" to "7" ratings are in the df

wide_for_likert_unwillingness_reasons_df_with_sub_labels <- surveys_df %>%
  filter(grepl("unwillingness-reason", question)) %>%
  filter(phase == "follow-up-unwillingness-concerns-survey") %>%
  select(question, answer, prolific_subject_id) %>%
  group_by(prolific_subject_id) %>%
  mutate(answer = as.numeric(answer)) %>%
  pivot_wider(id_cols = prolific_subject_id, names_from = question, values_from = answer) %>%
  ungroup() %>%
  rename("Might experience negative feelings about self" = "unwillingness-reason-The conversation could negatively affect how I feel about myself during and/or afterward (e.g.,  fear, sadness, anger, vulnerability)", "Might be offended" = "unwillingness-reason-I might take offense to what someone says",
  "Opinion is unpopular amongst group" = "unwillingness-reason-I would feel like my opinion is unpopular amongst the group.",
  "Doesn't participate in these conversations often" = "unwillingness-reason-I don’t often participate in conversations like these",
  "Might struggle to explain views or seem uninformed" = "unwillingness-reason-I might struggle to explain my views to the others and/or come across as uninformed",
  "Others might be unproductive or disrespectful" = "unwillingness-reason-I would not trust the others to keep the conversation productive and respectful (e.g., keeping emotions in check, refraining from making hostile remarks, not dominating the conversation)",
  "Views wouldn't change" = "unwillingness-reason-It would be a waste of time because my views wouldn't change",
  "Doesn't want to listen to others" = "unwillingness-reason-I would not want to listen to the others discuss their views",
  "Might be criticized" = "unwillingness-reason-Someone could criticize or show disapproval of my views",
  "Experience negative feelings about the world" = "unwillingness-reason-The conversation could negatively affect how I feel about the world during and/or afterward",
  "Conversation might be awkward or tense" = "unwillingness-reason-The conversation could become awkward or tense",
  "Might feel embarrassed" = "unwillingness-reason-I could feel humiliated or embarrassed in the conversation",
  "Might offend someone" = "unwillingness-reason-Someone might take offense to what I say",
  "Might struggle to remain productive and respectful" = "unwillingness-reason-I might struggle to remain productive and respectful",
  "Others won't make effort to listen" = "unwillingness-reason-The others might not make a full effort to hear and understand me",
  "Identity might be challenged" = "unwillingness-reason-Someone could challenge ideas or beliefs that play an important role in making me who I am",
  "Might feel invalidated" = "unwillingness-reason-I might feel disempowered, unheard, or invalidated",
  "Might incur social reprecussions" = "unwillingness-reason-I could incur social repercussions (e.g., being excluded by others in the future, putting a strain on my relationships, negative changes to my reputation)",  
  "Opinion is unpopular in general" = "unwillingness-reason-My opinion is unpopular in general."
)

wide_for_likert_unwillingness_reasons_df <- wide_for_likert_unwillingness_reasons_df_with_sub_labels %>%
  select(-prolific_subject_id) %>%
  mutate(across(everything(), ~ factor(.x, levels = 1:7,
     labels = c("Strongly Disagree", "Moderately Disagree", "Somewhat Disagree",
                "Neutral", "Somewhat Agree", "Moderately Agree", "Strongly Agree"),
     ordered = TRUE)))


### REMOVE DURING ACTUAL DATA ANALYSIS. I'M MANUALLY ADDING ALL LIKERT VALUES FOR EACH unwillingness REASON SO THAT LIKERT PLOT WORKS
### create a vector of the all 1-7 values
all_likert_values <- c("Strongly Disagree", "Moderately Disagree", "Somewhat Disagree", "Neutral", "Somewhat Agree", "Moderately Agree", "Strongly Agree")
### get the column names from  existing df
unwillingness_reasons_names <- colnames(wide_for_likert_unwillingness_reasons_df)
### create 7 new rows
new_likert_rows <- lapply(all_likert_values, function(response) {
  as.list(setNames(rep(response, length(unwillingness_reasons_names)), unwillingness_reasons_names))
}) %>%
  bind_rows()
### add the new rows to the existing dataframe
wide_for_likert_unwillingness_reasons_df <- bind_rows(wide_for_likert_unwillingness_reasons_df, new_likert_rows)



# standardize all columns to use the same factor object
wide_for_likert_unwillingness_reasons_df <- wide_for_likert_unwillingness_reasons_df %>%
  mutate(across(everything(), ~ factor(.x, levels = all_likert_values, ordered = TRUE)))

unwillingness_reason_likert <- likert(as.data.frame(wide_for_likert_unwillingness_reasons_df))

# for the future when comparing pre and post, might want to use this example line: df1_likert <- likert(items=df1[,3:4], grouping=df1[,2]) 

plot(unwillingness_reason_likert, legend.position="right")

```

```{r}
#| label: fig-sliders-density
#| echo: false
#| fig-cap: Density plot displaying subjects' self-reported willingness to converse about their topic with somebody who disagrees with them and/or will challenge their views. Subjects used a scale from 0 to 100, where a rating of 0 meant subject was absolutely unwilling and a rating of 100 meant subject was absolutely willing. Subjects were asked this question in two ways; this density plot shows the mean of the two ratings. 

sliders_df <- surveys_df %>%
  filter(question == "slider1-placeholder" | question == "slider2-placeholder") %>%
  group_by(prolific_subject_id) %>%
  summarize(mean_willingness = round(mean(as.numeric(answer), na.rm = TRUE), 1))
  
sliders_density <- ggplot(sliders_df, aes(x = mean_willingness)) + #, color = condition.x)) +
  geom_density() +  
  labs(x = "Mean Willingness Rating", y = "Density of Rating", title = "Average Willingness to Converse about Selected Topic")  #, color = "Group") +
 # scale_color_manual(values = c("control free reflection" = "darkseagreen", "selection bot" = "cornflowerblue", "combination bot" = "navyblue"), 
 #                    labels = c("control free reflection" = "Control group", "selection bot" = "Experimental group: Selection bot", "combination bot" = "Experimental group: Combination bot"))

print(sliders_density)

```

```{r}
#| label: fig-openmindedness-density
#| echo: false
#| fig-cap: Scatterplot with line of equality of pre- and post-conversation mean ratings of subjects' open-mindedness when discussing their topic. Subjects rated their open-mindedness on a scale from 0 to 7, where a rating of 0 meant subject was not open-minded and a rating of 7 meant subject was open-minded.

openmindedness_df <- surveys_df %>%
  filter(grepl("openmindedness", question)) %>%
  mutate(answer = as.numeric(answer)) 

openmindedness_df_mean <- openmindedness_df %>%
  group_by(prolific_subject_id) %>%
  summarize(mean_openmindedness = mean(as.numeric(answer))) %>%
  ungroup()

openmindedness_density <- ggplot(openmindedness_df_mean, aes(x = mean_openmindedness)) + #, color = condition.x)) +
  geom_density() +  
  labs(x = "Mean Open-mindedness Rating", y = "Density of Rating", title = "Average Open-mindedness about Selected Topic")  #, color = "Group") +
 # scale_color_manual(values = c("control free reflection" = "darkseagreen", "selection bot" = "cornflowerblue", "combination bot" = "navyblue"), 
 #                    labels = c("control free reflection" = "Control group", "selection bot" = "Experimental group: Selection bot", "combination bot" = "Experimental group: Combination bot"))

print(openmindedness_density)

```

```{r}
#| label: fig-openmindedness-likert-bar-plot
#| echo: false
#| fig-cap: Likert bar plot displaying subjects' ratings of five questions about their open-mindedness. The five questions were derived from @elnakouricite and modified for application to this experiment. On a scale from 0 to 7, where a rating of 0 meant subject strongly disagreed and a rating of 7 meant strongly agree, subjects rated how true the five statements were for them. Three of the statements were negatively worded in the survey but have been worded positively and reverse-coded in this plot for clarity.

# only works though if all "1" to "7" ratings are in the df

wide_for_likert_openmindedness_df_with_sub_labels <- openmindedness_df %>%
  select(question, answer, prolific_subject_id) %>%
  pivot_wider(names_from = question, values_from = answer) %>%
  ungroup() %>%
  rename("Has patience for opposing arguments" = "openmindedness...have little patience for arguments that I disagree with @R@", 
"Does not avoid opposing messages" = "openmindedness...avoid messages that I disagree with @R@", 
"Believes it is important to pay attention to all politics ideas" = "openmindedness...believe it is a waste of time to pay attention to certain political ideas about $topic",
"Is open to other political viewpoints" = "openmindedness...am open to considering other political viewpoints about $topic",
"Considers as many different opinions as possible" = "openmindedness...consider as many different opinions as possible about $topic") 

wide_for_likert_openmindedness_df <- wide_for_likert_openmindedness_df_with_sub_labels %>%
  select(-prolific_subject_id) %>%
  mutate(across(everything(), ~ factor(.x, levels = 1:7,
     labels = c("Strongly Disagree", "Moderately Disagree", "Somewhat Disagree",
                "Neutral", "Somewhat Agree", "Moderately Agree", "Strongly Agree"),
     ordered = TRUE)))

### REMOVE DURING ACTUAL DATA ANALYSIS. I'M MANUALLY ADDING ALL LIKERT VALUES FOR EACH UNWILLINGNESS REASON SO THAT LIKERT PLOT WORKS
### create a vector of the all 1-7 values. defining again here in case stuff gets moved around 
all_likert_values <- c("Strongly Disagree", "Moderately Disagree", "Somewhat Disagree", "Neutral", "Somewhat Agree", "Moderately Agree", "Strongly Agree")
### get the column names from  existing df
openmindedness_column_names <- colnames(wide_for_likert_openmindedness_df)
### create 7 new rows
new_openmindedness_likert_rows <- lapply(all_likert_values, function(response) {
  as.list(setNames(rep(response, length(openmindedness_column_names)), openmindedness_column_names))
}) %>%
  bind_rows()
### add the new rows to the existing dataframe
wide_for_likert_openmindedness_df <- bind_rows(wide_for_likert_openmindedness_df, new_openmindedness_likert_rows)



# standardize all columns to use the same factor object
wide_for_likert_openmindedness_df <- wide_for_likert_openmindedness_df %>%
  mutate(across(everything(), ~ factor(.x, levels = all_likert_values, ordered = TRUE)))

openmindedness_likert <- likert(as.data.frame(wide_for_likert_openmindedness_df))

# for the future when comparing pre and post, might want to use this example line: df1_likert <- likert(items=df1[,3:4], grouping=df1[,2]) 

plot(openmindedness_likert, legend.position="right")
```

### Free response

We conducted two primary analyses. First, we assessed whether our 19-item survey question accurately captures Americans' most pressing willingness concerns. Towards this end, we had CHAT-GPT 4o analyze the free-response data to determine whether users felt that there were concerns contributing to their unwillingness other than the 19 items that we provided and whether any of these additional concerns were common in the data. Specifically, we prompted CHAT-GPT to extract and list all distinct concerns mentioned across responses, to group similar concerns (e.g., “Fear of confrontation,” “Lack of trust in others,” “Feeling unqualified to speak,” etc.), and to accompany these groupings with summaries and representative examples. We also manually analyzed ten percent of the data to validate CHAT-GPT's assessment.

We found that \[results\]

```{r}
#| label: tbl-free-response-examples-table
#| echo: false
#| tbl-cap: Table showing examples of subjects' responses to free response question, which asked subjects "INSERT EXACT QUESTION HERE FOR REAL DATA ANALYSIS"

free_response <- surveys_df %>%
  filter(question == "willingness-free-response") %>%
  select(answer, topicChoiceAsString) %>%
  rename(
    #Condition = condition,
    'Topic Choice' = 'topicChoiceAsString',
    'Other reasons for unwillingness?' = 'answer'
    )

kable(free_response, "html") 
# %>%
#   kable_styling() %>%
#   column_spec(2, width = "5em") %>%
#   column_spec(c(3,4,5,5), width = "40em")
```

```{r}
#| label: tbl-free-response-tally-plot
#| echo: false
#| tbl-cap: Table showing subjects' responses to free response question. [insert the question here for real data analysis]

# CHANGE THIS CODE AS NEEDED BASED ON HOW THE DF IS STRUCTURED 

### NEED TO CREATE A DF WITH ADDITIONAL REASONS FOUND IN THE FREE RESPONSE BEFORE I CAN DO THIS

free_response_bar_plot <- ggplot(INSERT_DF_NAME_HERE, aes(x = count, y = reorder(statement, count))) +
  geom_col(fill = "cornflowerblue") +
  labs(
    x = "Number of Participants",
    y = NULL,
    title = "Number of Participants Endorsing Each Statement"
  ) +
  theme_minimal(base_size = 13)

print(free_response_bar_plot)

```

### Factor analysis

Secondly, we conducted a factor analysis on the 19 items to explore potential latent factors reflecting correlated concerns. We found that \[list results\].

```{r}
#| label: WIP - KMO, Bartlett's test, parallel analysis on 19-item matrix
#| echo: false

# create short labels for each question
question_key_for_fa <- surveys_df %>%
  distinct(question) %>%
  filter(grepl("unwillingness-reason", question)) %>%
  arrange(question) %>%  # ensure consistent order
  mutate(short_label = paste0("Q0", row_number()))

# join short labels back to the main data
surveys_df_labeled_questions <- surveys_df %>%
  left_join(question_key_for_fa, by = "question") %>%
  filter(grepl("unwillingness-reason", question))

concerns_labeled_wide_with_sub_labels <- surveys_df_labeled_questions %>%
  select(prolific_subject_id, short_label, answer) %>%
  pivot_wider(names_from = short_label, values_from = answer) %>%
  mutate(across(starts_with("Q0"), ~ as.numeric(as.character(.)))) %>%
  ungroup()

concerns_labeled_wide <- concerns_labeled_wide_with_sub_labels %>%
  select(-prolific_subject_id)

# KMO and Bartlett’s test
KMO(concerns_labeled_wide)
cortest.bartlett(cor(concerns_labeled_wide), n = nrow(concerns_labeled_wide))

# Parallel analysis
fa.parallel(concerns_labeled_wide, fa = "fa")

```

```{r}
#| label: WIP - Factor analysis on 19 items 
#| echo: false

# determine number of factors with parallel analysis
fa.parallel(concerns_labeled_wide, 
            fa = "fa", 
            n.iter = 100)

# run factor analysis with the chosen number of factors
fa_result <- fa(
  concerns_labeled_wide, 
  nfactors = [INSERT_NUMBER_OF_FACTORS], 
  rotate = "oblimin", 
  fm = "ml"
  )

# print results
print(fa_result, cutoff = 0.3)  # show loadings above .3 NOT SURE IF THIS IS A GOOD CHOICE OF CUTOFF 
```

```{r}
#| label: WIP - tbl-correlation-matrix
#| echo: false
#| tbl-cap: Correlation matrix of reasons for unwillingness

cor_matrix <- cor(concerns_labeled_wide, use = "pairwise.complete.obs")
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7)

```

```{r}
#| label: WIP - Check internal consistency 
#| echo: false

psych::alpha(concerns_labeled_wide)

```

```{r}
#| label: WIP - tbl-factor-loadings-visual
#| echo: false
#| tbl-cap: Table visualizing factor loadings.

# visualize factor loadings
loadings_df <- as.data.frame(fa_result$loadings[1:19, ])
loadings_df$short_label <- rownames(loadings_df)

loadings_long <- loadings_df %>%
  pivot_longer(-question, names_to = "factor", values_to = "loading")

factor_loadings_plot <- ggplot(loadings_long, aes(x = reorder(question, loading), y = loading, fill = factor)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(
    title = "Factor Loadings",
    x = "Question",
    y = "Loading",
    fill = "Factor"
  ) +
  theme_minimal()

print(factor_loadings_plot)

```

```{r}
#| label: WIP - tbl-fa-heat-map-df
#| echo: false
#| tbl-cap: Table showing factor analysis heat map.

fa_heat_map <- ggplot(loadings_long, aes(x = factor, y = reorder(question, desc(question)), fill = loading)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    midpoint = 0, limit = c(-1, 1), name = "Loading"
  ) +
  labs(
    title = "Heat Map of Factor Loadings",
    x = "Factor",
    y = "Question"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10))

print(fa_heat_map)

```

### Additional analyses

Additionally, we looked for any correlations between open-mindedness and willingness concerns.

```{r}
#| label: WIP - Correlations open-mindedness average vs. willingness concerns 
#| echo: false
#| tbl-cap: Correlation matrix of average open-mindedness vs. willingness concerns 

# look at correlation relationship between mean open-mindedness score and each willingness concern
cor_openmindedness_mean_vs_concerns <- concerns_labeled_wide_with_sub_labels %>%
   map_dfr(~ tidy(cor.test(.x, openmindedness_df_mean$mean_openmindedness)), .id = "prolific_subject_id")

# view results (i don't really understand this code. i had chat generate it to help me)
cor_openmindedness_mean_vs_concerns %>%
  select(item, estimate, p.value) %>%
  arrange(p.value)

```

```{r}
#| label: WIP - correlation between average open-mindedness and unwillingness concern factors
#| echo: false 
#| tbl-cap: Correlation matrix of average open-mindedness vs. willingness concern factors 


```

```{r}
#| label: WIP - Correlations willingness slider average vs. willingness concerns 
#| echo: false

reasons_and_willingness_merged_df <- merge(wide_for_likert_unwillingness_reasons_df_with_sub_labels, sliders_df, by = 'prolific_subject_id') %>%
  ungroup() %>%
  select(-prolific_subject_id)

cor_willingness_slider_vs_concerns <- map_dfr(reasons_and_willingness_merged_df[-20], ~ 
  tidy(cor.test(.x, reasons_and_willingness_merged_df$mean_willingness, use = "pairwise.complete.obs")),
  .id = "reason_item")

cor_willingness_slider_vs_concerns %>%
  select(item, estimate, p.value) %>%
  arrange(p.value)

```

Next, we looked for correlations between willingness concerns and overall willingness as indicated by the slider questions.

```{r}
#| label: WIP - Look at concerns as predictors of overall willingness 
#| echo: false 

model <- lm(mean_willingness ~ ., data = reasons_and_willingness_merged_df)
summary(model)

# Multicollinearity check
vif(model)

```

Then, we assessed whether any willingness concerns are significant predictors of overall willingness through the use of a multiple linear regression model. We found that [results].

```{r}
#| label: WIP - Cluster analysis
#| echo: false 

# Scale data
concerns_labeled_wide_scaled <- scale(concerns_labeled_wide)

# Determine optimal number of clusters
fviz_nbclust(concerns_labeled_wide_scaled, kmeans, method = "wss")

# K-means with k = 3 (example)
set.seed(1)
kmeans_result <- kmeans(concerns_labeled_wide_scaled, centers = 3)
survey_df$cluster <- kmeans_result$cluster

# Visualize clusters with PCA
autoplot(prcomp(concerns_labeled_wide_scaled), data = survey_df, colour = 'cluster')
```

We also conducted cluster analysis to determine whether participants generally fell into certain categories of concerns.

```{r}
#| label: WIP - ANOVAs 
#| include: false

# If you have a 'topic' variable
manova_result <- manova(as.matrix(concerns_labeled_wide_with) ~ survey_df$topic)
summary(manova_result)

# Follow-up ANOVAs
aov_results <- lapply(concerns_labeled_wide, function(item) aov(item ~ survey_df$topic))
lapply(aov_results, summary)

```

Finally, we ran ANOVAs to determine whether there were significant differences in how participants with different conversation topics responded to our survey.

## Discussion

## References
