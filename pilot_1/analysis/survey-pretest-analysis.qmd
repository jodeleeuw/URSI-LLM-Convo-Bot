---
title: "survey-pretest-analysis"
format: html
bibliography: mocareferences.bib
citation-location: document
reference-location: section
editor: visual
---

```{r}
#| label: Data Fetching. Load R packages
#| include: false 

library(osfr) 
library(dplyr)
library(ez) 
library(tidyverse) 
library(gt)
library(gtsummary)
library(kableExtra)
library(stringr)
library(ggridges)
library(lubridate)  # for date handling
library(psych) # for factor analysis 
library(pheatmap) # for factor analysis heat map 
library(corrplot) # for correlation matrix 
library(broom) # for correlation tests
library(purrr) # for correlation tests 
library(likert) # for likert plots 
library(factoextra)
library(ggfortify)
library(car) 
library(GPArotation) # for factor analysis 
# for mlr
# library(GGally)
# library(car)
# library(MASS)
# library(glmnet)

```

```{r}
#| label: Prolific demographics data
#| include: false 

## add which round of the pilot it was by reading in demographics files so we know completion dates

#read in prolific demographics files 
pretest_demo_df <- read_csv("prolific_export_685c48bc2d211110b035b997.csv") %>%
  mutate(Age = as.numeric(Age))

#merge files (if multiple)
merged_prolific_demographics_df <- pretest_demo_df
  #bind_rows(pretest_demo_df) 

#clean df
cleaned_prolific_demographics_df <- merged_prolific_demographics_df %>%
# make name the same as in data csvs so merging is easier
rename("prolific_subject_id" = "Participant id") %>%
# filter out people who didn't finish
filter(Status != "RETURNED") %>%
#rename "Started at" to make grepl function easier to use later on
rename("start" = "Started at") %>%
# make time more readable but keep the seconds version for analysis stuff later
mutate(`time_taken_overall_minutes` = as.numeric(`Time taken`) / 60) %>%
#remove the no code and unknown code people 
  filter(`Completion code` == "CVWBBA5S")

str(cleaned_prolific_demographics_df)

```

```{r}
#| label: Retrieve data for the specific date of the willingness pretest from our OSF project. Loading into a data folder, which  we manually create
#| include: false 
# 
# # list all files
# files <- osf_retrieve_node("x32pv") %>%
#   osf_ls_files(n_max = 1000)
# 
# # originally it wasn't downloading: 
# #csv 1ftd0hhjnr.csv which has subject id 5f82567f1e720a3842580d23
# #csv fn1vmsaka7 which has subject id 66d8837
# #csv zxynjls3ql.csv which has subject id 5ffb8c5b6... 
# 
# # filter for the correct date
# files_filtered <- files %>%
#   mutate(date_created = map_chr(meta, ~ as.character(.x$attributes$date_created)),
#     date_created = as_date(date_created)
#   ) %>%
#   filter(date_created %in% as_date(c("2025-09-16", "2025-09-29", "2025-09-30")))

# download files for the correct date 
#osf_download(files_filtered, path = "data", conflicts = "skip")
```

```{r}
#| label: Bind csv's
#| include: false 
# 
#  osf_csv_filenames <- list.files(path = "data/")
#  osf_csv_filenames_filtered <- osf_csv_filenames[osf_csv_filenames %in% files_filtered$name]

# LOAD IN DATA FROM FOLDER IN THIS R PROJ SINCE RETRIEVAL NODE IS WEIRD 

file_names <- list.files("osfstorage-archive/", pattern = "\\.csv$", full.names = FALSE)

 raw_dfs_tibbles <- map(file_names, ~read_csv(file.path("osfstorage-archive/", .)))
 rawwwwwww_df <- bind_rows(raw_dfs_tibbles)

```

```{r}
#| label: Verifying that we have all data 
#| include: false 

# verify that we have the data for all the people by making sure that the data df has as many subs as the demo df

# difference <- setdiff(
#   cleaned_prolific_demographics_df$prolific_subject_id,
#   rawwwwwww_df$prolific_subject_id
# )
# 
# if (length(difference) > 0) {
#   print("ALERT: MISSING DATA FROM PARTICIPANTS:")
#   print(difference)
# }

#664287debe4750708ea0ecf7 has our url as their completion code 
#6173c0bb92e149b4127563fe has their submission id as completion code 
# the rest are completely normal, idk why they're not downloading:
# not sure what is going on with "5dceca2541afe801a32e27ac", 

#qeg3nvqqbc.csv which has "646792a49a85f35e7a7f5169",
#omkd7wc2nm.csv which has "5e07d174da6bad1342bafab2", 
#08yq5tsl91.csv which has "67ac21373a76570ef0a5c0d3" , 
#4y1akeuxu9.csv which has "5dac4dad5e53cc001499f1f6", 
#9vtg72o38k.csv which has "6296bef3248bf76cc113926b"

```

```{r}
#| label: Merge prolific demos + create a condition type column
#| include: false 

raw_df <- rawwwwwww_df %>%
  # inner join with the demographics df from pilot 2 so we can eliminate pilot 1 subjects
  right_join(cleaned_prolific_demographics_df, by = "prolific_subject_id") %>%
  # remove unnecessary syntax from topicChoiceAsString for readability later
  mutate(topicChoiceAsString = gsub('\\{"row":"', '', topicChoiceAsString)) %>%
  mutate(topicChoiceAsString = gsub('"}', '', topicChoiceAsString)) %>%
  # fill condition column and row and topic strings
  group_by(prolific_subject_id) %>%
  fill(topicChoiceAsString, .direction = "downup") %>%
  ungroup() 

# number of subjects in data 
total_num_subjects_in_data <- nrow(raw_df %>%
  group_by(prolific_subject_id) %>%
  summarize(total_num_subjects_in_data = n()))

```

```{r}
#| label: Clean the survey df
#| include: false

surveys_cleaning1_df <- raw_df %>%
  # Filter for only surveys
  filter(trial_type == "survey") %>%
  # Remove control interventions (these are coded as surveys)
  filter(phase != "control-intervention") %>%
  # Remove the matrix question titles because they mess with separating questions from answers
  # openmindedness and unwillingness reason
  mutate(across(everything(), ~ gsub(',"openmindedness":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"openmindedness":', '', .))) %>%
  mutate(across(everything(), ~ gsub(',"unwillingness-reason":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"unwillingness-reason":', '', .))) %>%
  # republican/democrat (pre-intervention)
  mutate(across(everything(), ~ gsub(',"rating-republicans":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub(',"rating-democrats":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-republicans":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans":null', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats":null', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-republicans":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats":', '', .))) %>%
  # republican/democrat (post-intervention)
  mutate(across(everything(), ~ gsub('"rating-republicans-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats-post":null,', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-republicans-post":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"rating-democrats-post":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-republicans-post":', '', .))) %>%
  mutate(across(everything(), ~ gsub('"interacting-with-democrats-post":', '', .))) %>%
  # the name of the question in 2.1 run of the experiment
  mutate(across(everything(), ~ gsub('topicChoice":', '', .))) %>%
  # the name of the question in future runs of the experiment
  mutate(across(everything(), ~ gsub('topic":', '', .))) %>%
  mutate(across(everything(), ~ gsub(',null,', '', .))) 

  
surveys_cleaning2_df <- surveys_cleaning1_df %>%
  # Remove weird characters but don't remove quotations because we need them for separating questions
  mutate(across(everything(), ~ gsub('}', '', .))) %>%
  mutate(across(everything(), ~ gsub('\\{', '', .))) %>%
  mutate(across(everything(), ~ gsub('\\[', '', .))) %>%
  mutate(across(everything(), ~ gsub('\\]', '', .))) %>%
  # Make each question its own row
  separate_rows(response, sep = ',"') %>%
  # Remove any leftover quotation marks
  mutate(across(everything(), ~ gsub('"', '', .))) %>%
  # Move answers to a new column
  separate(response, into = c("question", "answer"), sep = ":", extra = "merge", fill = "right") %>%
  # Remove all remaining questions that were answered with null (like the slider questions, because the slider 'placeholder' questions are where people actually answer the slider questions)
  filter(!is.na(answer) & answer != 'null' & answer != 'null,null')

```

```{r}
#| label: Reverse code necessary items 
#| include: false 

# reverse code open-mindedness items 

surveys_cleaning3_df <- surveys_cleaning2_df %>%
    mutate(answer = ifelse(
     grepl("@R@", question),
     8 - as.numeric(answer),
     (answer)
   ))

```

```{r}
#| label: Remove unnecessary columns 
#| include: false 

surveys_df <- surveys_cleaning3_df %>%
  select(-c(stimulus, trial_type, trial_index, time_elapsed, internal_node_id, study_id, session_id, unwillingness_yes_rows, "Submission id", Status, "Custom study tncs accepted at", "Reviewed at", "Archived at", "Completion code", "Total approvals"))

willingness <- surveys_df %>%
  filter(question == "commitment-check")
```

```{r}
#| label: Create a df that flags people who failed the attention check
#| include: false

#see who passes the check
attention_check <- surveys_df %>%
  filter(question == "attention-check-slider-pre-placeholder") %>%
  mutate(flag = case_when(
    sum(answer == "0.00") == 0 ~ "fail (single)", # single fail means we exclude their data but we still have to pay them on prolific 
    sum(answer == "0.00") == 1 ~ "pass", #good subject :3
    TRUE ~ NA_character_
  )) %>%
  ungroup() %>%
  select(flag, question, answer, prolific_subject_id) 

#get list of all subject id's so we can see which subjects didn't even answer the question (the sliders aren't recorded in our df if the subject doesn't move them. they naturally sit at 50.) 
all_ids <- surveys_df %>%
  distinct(prolific_subject_id)

#filter for the question of interest
attention_ids <- surveys_df %>%
  filter(question == "attention-check-slider-pre-placeholder") %>%
  distinct(prolific_subject_id) 

#see who's missing
missing_ids <- setdiff(all_ids$prolific_subject_id, attention_ids$prolific_subject_id)

#make a table for these missing people 
missing_rows <- tibble(
  flag = "fail (single)",
  question = NA_character_,
  answer = NA_character_,
  prolific_subject_id = missing_ids,
  time_taken_overall_minutes = NA_real_
)

# combine those who answered and those who didn't answer
attention_check <- bind_rows(attention_check, missing_rows)

# extract value for text 
total_failed_attention_checks <- attention_check %>%
  # excluding people for both fails
  filter(flag == 'fail (single)') %>%
  summarize(total_failed_attention_checks = n_distinct(prolific_subject_id)) %>%
  pull(total_failed_attention_checks)

#extract list of ids so we can remove the data
failed_attention_check_ids <- attention_check %>%
  filter(flag == "fail (single)") %>%
  distinct(prolific_subject_id) %>%
  pull(prolific_subject_id)
```

```{r}
#| label: Remove subs who failed attention check 
#| include: false

# remove bad subs
raw_df <- raw_df %>%
  filter(!(prolific_subject_id %in% failed_attention_check_ids))
    
surveys_df <- surveys_df %>%
  filter(!prolific_subject_id %in% failed_attention_check_ids) 

total_num_clean_subjects_in_data <- nrow(raw_df %>%
  group_by(prolific_subject_id) %>%
  summarize(total_num_clean_subjects_in_data = n()))

```

```{r}
#| label: Print surveys_df csv (and txt for Claude)
#| echo: false 

write.csv(surveys_df, "pretest_survey_data.csv", row.names = FALSE)


write.table(surveys_df, file = "pretest_survey_data.txt", sep = "\t", row.names = FALSE, quote = FALSE)

```

## Introduction

Existing literature indicates that a complex interplay of psychological factors—including fear of social isolation (@NoelleNeumann1974; @Chen2018; @Haug2025), identity protection (@Tajfel1979; @Hunter1996), communication anxiety (@Beatty1987; @Schulenberg2023), and emotional dysregulation (@Masters2019; @Aldao2010)—converge with sociocultural influences to deter open dialogue (@Haug2025; @Kahneman2011; @Lewandowsky2017). This avoidance is a significant barrier to constructive discourse. Understanding this phenomenon is critical for fostering democratic citizenship and cultivating inclusive learning environments.

We are developing an individual-level intervention to help increase Americans' willingness to engage in conversations on divisive topics. To properly measure the barriers to engagement and assess the effectiveness of our intervention, we first designed a 19-item questionnaire to capture the core concerns that contribute to an individual's reluctance to engage in conversations on controversial topics. This pretest study aims to assess the effectiveness of this 19-item questionnaire through addressing two primary preregistered research questions:

Comprehensiveness: Do the 19 items in our questionnaire capture the primary concerns that contribute to Americans' unwillingness to engage in productive, open-minded discussions about divisive topics?

Structure: Are there any distinct, underlying factors that organize these 19 concerns?

## Methods

### Materials

We used custom software written with the jsPsych framework (@jspsychcite) to create the study, which participants viewed and responded to via their personal laptops or desktops. The javascript code for the experiment and RStudio code for data analysis can both be found at (<https://github.com/jodeleeuw/URSI-LLM-Convo-Bot/tree/main/pilot_1).> Pre-registrations and all data for this study and previous pilots are available on the Open Science Framework at <https://osf.io/x32pv.>

### Participants and Procedure

```{r}
#| label: Find time taken specifically on the surveys (by subject and overall)
#| echo: false 

#finding each subject's time taken on the survey specifically
surveys_df <- surveys_df %>%
  group_by(prolific_subject_id) %>%
  mutate(
    time_minutes_survey_only = sum(
      unique(as.numeric(rt)) / 60000),
      na.rm = TRUE
    )

#finding overall mean, min, max of how long participants took 
time_stats <- surveys_df %>%
  ungroup() %>%
  summarise(
    Mean = round(mean(time_minutes_survey_only, na.rm = TRUE), 1),
    Min = round(min(time_minutes_survey_only, na.rm = TRUE), 1),
    Max = round(max(time_minutes_survey_only, na.rm = TRUE), 1)
  ) 

mean_survey_time <- time_stats$Mean
min_survey_time <- time_stats$Min
max_survey_time <- time_stats$Max
```

```{r}
#| label: WIP - tbl-basic-demographics-table
#| echo: false
#| tbl-cap: Table of basic demographics.

#reloading package because functions from other packages were overriding??
library(gtsummary) 

#we already read in demographics earlier 

#create a demographics table from prolific screening demo data which we loaded onto the raw df arlier
survey_demographics_df <- surveys_df %>%
filter(question == "commitment-check") %>% # picking random question so there's only one row per subject
select(prolific_subject_id, Age, Sex, Ethnicity, "Ethnicity simplified", "Highest education level completed", "U.s. political affiliation", "Political spectrum (us)", "Religious affiliation", "Employment status", time_taken_overall_minutes) %>%
  mutate(Age = as.numeric(Age))

#ensure that these are factors
survey_demographics_df$`Highest education level completed` <- factor(survey_demographics_df$`Highest education level completed`)
survey_demographics_df$`U.s. political affiliation` <- factor(survey_demographics_df$`U.s. political affiliation`)
survey_demographics_df$`Political spectrum (us)` <- factor(survey_demographics_df$`Political spectrum (us)`)

# define the levels in categorical variables so that they are organized in the table
survey_demographics_df$`U.s. political affiliation` <- fct_relevel(
  survey_demographics_df$`U.s. political affiliation`,
  "Republican",
  "Democrat",
  "Independent",
)


survey_demographics_df$`Highest education level completed` <- fct_relevel(
  survey_demographics_df$`Highest education level completed`,
  "High school diploma/A-levels",
  "Technical/community college",
  "Undergraduate degree (BA/BSc/other)",
  "Graduate degree (MA/MSc/MPhil/other)",
)


survey_demographics_df$`Political spectrum (us)` <- fct_relevel(
  survey_demographics_df$`Political spectrum (us)`,
"Conservative",
"Liberal",
)

survey_demographics_df <- survey_demographics_df %>%
  mutate(across(where(is.factor), ~ droplevels(.)))

survey_demographics_df$time_taken_overall_minutes <- round(
  as.numeric(survey_demographics_df$time_taken_overall_minutes), 
  1
)

demographics_table <- tbl_summary(survey_demographics_df,
 by = NULL,
  label = list(
    `time_taken_overall_minutes` = "Time taken (minutes)",
    Age = "Age (years)",
    Sex = "Sex",
    `Ethnicity simplified` = "Ethnicity",
    `U.s. political affiliation` = "Political affiliation",
    `Highest education level completed` = "Education level",
    `religious-affiliation` = "Religious affiliation",
    `Political spectrum (us)` = "Political ideology"
  ),
  statistic = NULL,
#list(
  #  all_continuous() ~ "{mean} ({sd})",
  #  all_categorical() ~ "{n} ({p}%)"
#),
  type = list(
    `time_taken_overall_minutes` = "continuous",
    `Age` = "continuous",
    `gender-identification` = "categorical",
   `Sex` = "categorical",
    `Ethnicity simplified` = "categorical",
    `U.s. political affiliation` = "categorical",
    `Highest education level completed` = "categorical",
    `religious-affiliation` = "categorical",
    `Political spectrum (us)` = "categorical"
  ),
  digits = list(
    all_continuous() ~ 2,
    all_categorical() ~ 0
  ),
 include = c(
'time_taken_overall_minutes',
'Age',
'Sex',
'Ethnicity simplified',
'U.s. political affiliation',
'Highest education level completed',
'Religious affiliation',
'Political spectrum (us)'
))
 # %>%  modify_header(label = "**whatever label i want**")

# Print the table
as_gt(demographics_table)

```

Through Prolific, we initially gathered data from `r total_num_subjects_in_data` subjects. Participants were restricted to U.S. residents who fluently speak English. We excluded data from `r total_failed_attention_checks` subjects for failing the attention check question. The final cleaned sample consisted of `r total_num_clean_subjects_in_data` subjects. Participants completed the questionnaire in an average of `r mean_survey_time` minutes (Range: `r min_survey_time` – `r max_survey_time`).

Participants were first asked to rate their comfort level with 10 controversial topics. The topic that they were least willing to discuss was selected as their focus topic for the remainder of the survey. The full questionnaire included self-reported willingness to converse (via two slider questions from 0-100), self-reported open-mindedness (a 5-item Likert scale from 1-7), the 19-item concern inventory (a Likert scale from 1-7, along with an initial binary "Yes/No" endorsement). The session concluded with a free-response question about participants' reasons for reluctance, which we will assess for any uncaptured reasons.

## Results

### Basic results

```{r}
#| label: fig-topic-ratings
#| include: false
#| fig-cap: In the pre-intervention survey, each subject used a 7-pt scale to rate how comfortable they would be discussing each of the following topics. One of the subject's most strongly rated topics were randomly selected as the topic that the subject would focus on for the remainder of the experiment. The scale went from strongly uncomfortable (1), to moderately uncomfortable, a little uncomfortable, neutral (4), a little comfortable, moderately comfortable, and strongly comfortable (7).

# create figure/table showing distribution of ratings on topics
topic_ratings_df <- surveys_df %>%
  filter(phase == "pre-convo-survey-initial") %>%
  filter(question != "commitment-check") %>%
  mutate(answer = as.numeric(answer)) %>%
  select(prolific_subject_id, question, answer, topicChoiceAsString)

topic_labels <- c( 
  "mandating vaccines in the U.S." = 
    "Vaccine Mandates",
  "human euthanasia in the U.S." = 
    "Human euthanasia",
  "deportation policies for undocumented immigrants in the U.S." = 
    "Deportation policies",
  "transgender athletes in U.S. sports" = 
    "Transgender athletes in sports",
  "the role of the U.S. government in healthcare" = 
    "Government in healthcare",
  "U.S. public policy on climate change" = 
    "Climate change policies",
  "the use of A.I. in traditionally human-run spaces (e.g., medicine, art, driving)" = 
    "A.I. in human-run spaces",
  "the role of capitalism in the U.S. economy" = 
    "Capitalism in economy",
  "the role of the U.S. in the Israeli-Palestinian conflict" = 
    "U.S. in Israeli-Palestinian conflict",
  "police conduct and accountability in the U.S." =
    "Police conduct and accountability"
)


really_short_topic_labels <- c( 
  "mandating vaccines in the U.S." = 
    "Vaccines",
  "human euthanasia in the U.S." = 
    "Euthanasia",
  "deportation policies for undocumented immigrants in the U.S." = 
    "Deportation",
  "transgender athletes in U.S. sports" = 
    "Trans athletes",
  "the role of the U.S. government in healthcare" = 
    "Healthcare",
  "U.S. public policy on climate change" = 
    "Climate change",
  "the use of A.I. in traditionally human-run spaces (e.g., medicine, art, driving)" = 
    "A.I.",
  "the role of capitalism in the U.S. economy" = 
    "Capitalism",
  "the role of the U.S. in the Israeli-Palestinian conflict" = 
    "Israel-Palestine",
  "police conduct and accountability in the U.S." =
    "Police"
)

topic_ratings_plot <- ggplot(topic_ratings_df, aes(x = answer, y = question, fill = question)) +
  geom_density_ridges(bandwidth = 0.931) + # using the bandwidth that it automatically picks 
  # give the qualitative meanings of 1-7
  scale_x_continuous(breaks = seq(1, 7, by = 1)) +
   scale_y_discrete(labels = topic_labels) +
  # make it look nicer
  theme_ridges() +      
  # remove the legend
  theme(legend.position = "none") +
  labs(title = "Ridgeline Plot of Ratings by Topic",
       x = "Rating (1 = Strongly uncomfortable, 7 = Strongly comfortable)",
       y = "Contentious Topic")

print(topic_ratings_plot)
```

```{r}
#| label: fig-topic-ratings-only-chosen-topics
#| echo: false
#| fig-cap: In the pre-intervention survey, each participant used a 7-pt scale (where 1 = Strongly uncomfortable and 7 = Strongly comfortable) to rate how willing they would be to discuss each of the ten divisive topics. For the remainder of the survey, participants were asked questions about the topic that they rated themselves as least willing to discuss. 

# create figure/table showing distribution of ratings on chosen topics only

topic_choice_rating <- topic_ratings_df %>%
  group_by(prolific_subject_id) %>%
  filter(question == topicChoiceAsString) %>%
   mutate(topicChoiceAsString = factor(topicChoiceAsString, 
                                     levels = names(sort(table(topicChoiceAsString), decreasing = TRUE))))


topic_choice_ratings_plot <- ggplot(topic_choice_rating, aes(x = answer, y = question, fill = question)) +
  geom_density_ridges(bandwidth = 0.931) + # using the bandwidth that it automatically picks 
  # give the qualitative meanings of 1-7
  scale_x_continuous(breaks = seq(1, 7, by = 1)) +
   scale_y_discrete(labels = topic_labels) +
  # make it look nicer
  theme_ridges() +      
  # remove the legend
  theme(legend.position = "none") +
  labs(title = "Ridgeline Plot of Ratings by Topic",
       x = "Rating (1 = Strongly uncomfortable, 7 = Strongly comfortable)",
       y = "Contentious Topic")

print(topic_choice_ratings_plot)
```

```{r}
#| label: Looking at high willingness topics more 
#| echo: false

# I want to see the percent of people who selected climate change policy, A.I. in human-run spaces, and government in healthcare with a rating of 5 or higher

topics_rating_high_will_df <- topic_ratings_df %>%
  filter(topicChoiceAsString == "U.S. public policy on climate change" | topicChoiceAsString == "human euthanasia in the U.S." | topicChoiceAsString == "the use of A.I. in traditionally human-run spaces (e.g., medicine, art, driving)") %>%
  filter(question == topicChoiceAsString) 


count_AI <- nrow(filter(topics_rating_high_will_df, topicChoiceAsString == "the use of A.I. in traditionally human-run spaces (e.g., medicine, art, driving)"))
count_climate_change <- nrow(filter(topics_rating_high_will_df, topicChoiceAsString == "U.S. public policy on climate change"))
count_euthanasia <- nrow(filter(topics_rating_high_will_df, topicChoiceAsString == "human euthanasia in the U.S."))

count_AI_high_willing <- nrow(filter(topics_rating_high_will_df, topicChoiceAsString == "the use of A.I. in traditionally human-run spaces (e.g., medicine, art, driving)" & answer %in% c(5, 6, 7)))
count_climate_change_high_willing <- nrow(filter(topics_rating_high_will_df, topicChoiceAsString == "U.S. public policy on climate change" & answer %in% c(5, 6, 7)))
count_euthanasia_high_willing <- nrow(filter(topics_rating_high_will_df, topicChoiceAsString == "human euthanasia in the U.S." & answer %in% c(5, 6, 7)))

percent_highly_willing_ai= count_AI_high_willing/count_AI
  
percent_highly_willing_climate_change = count_climate_change_high_willing/count_climate_change

percent_highly_willing_euthanasia = count_euthanasia_high_willing/count_euthanasia


```

```{r}
#| label: fig-bar-chart-topic-choice
#| include: false 
#| fig-cap: Bar chart showing the number of participants in each topic.

topic_votes_plot <- ggplot(topic_choice_rating, aes(x = answer, fill = topicChoiceAsString)) + 
  geom_bar() + 
  xlab("Willingness (1 = Strongly Unwilling, 7 = Strongly Willing)") +
  ylab("Number of participants")

topic_votes_plot
```

```{r}
#| label: fig-bar-topic-counts
#| echo: false 
#| fig-cap: Bar plot showing the number of participants in each topic.

topic_hist <- ggplot(topic_choice_rating, aes(x = topicChoiceAsString)) + 
  geom_bar() + 
  xlab("Focus topic") +
  scale_x_discrete(labels = really_short_topic_labels) +
  ylab("Number of participants") #+ 
  #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

topic_hist
```

Our topic assignment method successfully generated a sample for whom engagement with their focus topic was a genuine concern. As shown in Figure \@ref(fig: fig-topic-ratings-only-chosen-topics), the distribution of comfort ratings for the chosen topics was generally skewed below the neutral point of 4, validating the selection mechanism. Three of the ten topics---climate change policy, A.I. in human-run spaces, and government in healthcare---garnered many participants who were more willing than we had intended; however, these topics were also less popular (Figure \@ref(fig: fig-bar-topic-counts).

Specifically, `r count_climate_change_high_willing` participants were least willing to discuss climate change policies but still rated themselves as slightly to strongly comfortable discussing the topic, `r count_euthanasia_high_willing` participants were least willing to discuss human euthanasia but still rated themselves as slightly to strongly comfortable discussing the topic, and `r count_AI_high_willing` participants were least willing to discuss A.I. in human-run spaces but still rated themselves as slightly to strongly comfortable discussing the topic. Overall, most participants focused on a topic that they were slightly to strongly uncomfortable discussing.

As shown in Figure \@ref(fig: fig-bar-topic-counts), the majority of participants were least willing to discuss the role of the U.S. in the Israeli-Palestinian conflict, human euthanasia, and transgender athletes in sports.

```{r}
#| label: fig-avg-willingness-by-topic
#| include: false # this is such a fugly graph
#| fig-cap: Ridgeline plot of willingness to engage in conversation about topic by topic. Participants only rated their willingness for their topic choice. Participants' willingness values were calculated through averaging the responses to the questions, "Using the slider, please rate your willingness to have a conversation with someone who strongly disagrees with you about [topic], where a score of 0 is absolute unwillingness to converse and 100 is absolute willingness to converse. A score of 50 is neutral," and "Using the slider, please rate your willingness to have a conversation with someone who strongly disagrees with you about [topic], where a score of 0 is absolute unwillingness to converse and 100 is absolute willingness to converse. A score of 50 is neutral."

#make avg willingness df by taking mean of both slides for each participant 

avg_willingness_df <- surveys_df %>%
  filter(question == "slider1-placeholder" | question == "slider2-placeholder") %>% 
  group_by(prolific_subject_id, topicChoiceAsString) %>%
  summarise(avg_willingness = mean(as.numeric(answer), na.rm = TRUE)) %>%
  ungroup()

#merge back with surveys_df in case i want all info in one spot in the future 
surveys_df <- avg_willingness_df %>%
  select(-topicChoiceAsString) %>%
  full_join(surveys_df, by = "prolific_subject_id")

topic_choice_ratings_plot <- ggplot(avg_willingness_df, aes(x = avg_willingness, y = topicChoiceAsString, fill = topicChoiceAsString)) +
  geom_density_ridges(bandwidth = 0.931) + # using the bandwidth that it automatically picks 
  scale_x_continuous() +
   scale_y_discrete(labels = topic_labels) +
  # make it look nicer
  theme_ridges() +      
  # remove the legend
  theme(legend.position = "none") +
  labs(title = "Ridgeline Plot of Willingness to Engage by Topic",
       x = "Rating (0 = Strongly unwilling, 100 = Strongly willing)",
       y = "Contentious Topic")

topic_choice_ratings_plot

```

```{r}
#| label: fig-box-plot-willingness 
#| echo: false
#| fig-cap: Distribution of average willingness ratings by topic from 0 (completely unwilling) to 100 (completely willing), where 50 is neutral.

avg_willingness_box_plot <- ggplot(avg_willingness_df, aes(x = avg_willingness, y = as.factor(topicChoiceAsString), fill = as.factor(topicChoiceAsString))) +
  geom_boxplot() + 
  labs(title = "Boxplots of Willingness to Engage by Topic",
       x = "Rating (0 = Strongly unwilling, 100 = Strongly willing)",
       y = "Contentious Topic") + 
  guides(fill = "none")

avg_willingness_box_plot

```

Separately from assigning focus topics, we asked participants to rate their willingness to discuss their focus topic with someone who strongly disagrees with them and with someone who would push back against their views. Participants used a slider ranging from 0 (absolute unwillingness) to 100 (absolute willingness) to respond, and we averaged the two responses to find participants' average willingness. This assessment was quite similar to that of the topic selection question; however, we asked these questions separately to ensure quality of data (should probably expand on what I mean by quality data) and to use alternative phrasing that emphasized the potential divisiveness of the discussion.

Participants' average willingness, shown in Figure \@ref(fig:fig-box-plot-willingness), was widely spread with a notable density of participants reporting scores at or above the neutral point of 50. This is an unexpected contrast from participants' responses in the earlier Likert-scale question; we had anticipated that the Likert-scale indicator of willingness and the average willingness seen here would indicate similar overall trends.

```{r}
#| label: Diff btwn two avg willingness questions 
#| echo: false 

will_df <- surveys_df %>%
  filter(question == "slider1-placeholder" | question == "slider2-placeholder") %>%
  pivot_wider(
    names_from = question,
    values_from = answer
  ) %>%
  mutate(
    diff_willingness = as.numeric(`slider1-placeholder`) - as.numeric(`slider2-placeholder`)
  ) %>%
  select(prolific_subject_id, diff_willingness)

# there are some p big differences for a handful of people, so it's good that we have both questions

```

```{r}
#| label: fig-sliders-density
#| include: false
#| fig-cap: Density plot displaying subjects' self-reported willingness to converse about their topic with somebody who disagrees with them and/or will challenge their views. Subjects used a scale from 0 to 100, where a rating of 0 meant subject was absolutely unwilling and a rating of 100 meant subject was absolutely willing. Subjects were asked this question in two ways; this density plot shows the mean of the two ratings. 

sliders_df <- surveys_df %>%
  filter(question == "slider1-placeholder" | question == "slider2-placeholder") %>%
  group_by(prolific_subject_id) %>%
  summarize(mean_willingness = round(mean(as.numeric(answer), na.rm = TRUE), 1))
  
sliders_density <- ggplot(sliders_df, aes(x = mean_willingness)) + #, color = condition.x)) +
  geom_density() +  
  labs(x = "Mean Willingness Rating", y = "Density of Rating", title = "Average Willingness to Converse about Selected Topic")  #, color = "Group") +
 # scale_color_manual(values = c("control free reflection" = "darkseagreen", "selection bot" = "cornflowerblue", "combination bot" = "navyblue"), 
 #                    labels = c("control free reflection" = "Control group", "selection bot" = "Experimental group: Selection bot", "combination bot" = "Experimental group: Combination bot"))

print(sliders_density)

```

```{r}
#| label: Most frequently endorsed concerns 
#| echo: false

concern_map <- c("Might experience negative feelings about self" = "unwillingness-reason-The conversation could negatively affect how I feel about myself during and/or afterward (e.g.,  fear, sadness, anger, vulnerability)", "Might be offended" = "unwillingness-reason-I might take offense to what someone says",
  "Opinion is unpopular amongst group" = "unwillingness-reason-I would feel like my opinion is unpopular amongst the group.",
  "Doesn't participate in these conversations often" = "unwillingness-reason-I don’t often participate in conversations like these",
  "Might struggle to explain views or seem uninformed" = "unwillingness-reason-I might struggle to explain my views to the others and/or come across as uninformed",
  "Others might be unproductive or disrespectful" = "unwillingness-reason-I would not trust the others to keep the conversation productive and respectful (e.g., keeping emotions in check, refraining from making hostile remarks, not dominating the conversation)",
  "Views wouldn't change" = "unwillingness-reason-It would be a waste of time because my views wouldn't change",
  "Doesn't want to listen to others" = "unwillingness-reason-I would not want to listen to the others discuss their views",
  "Might be criticized" = "unwillingness-reason-Someone could criticize or show disapproval of my views",
  "Experience negative feelings about the world" = "unwillingness-reason-The conversation could negatively affect how I feel about the world during and/or afterward",
  "Conversation might be awkward or tense" = "unwillingness-reason-The conversation could become awkward or tense",
  "Might feel embarrassed" = "unwillingness-reason-I could feel humiliated or embarrassed in the conversation",
  "Might offend someone" = "unwillingness-reason-Someone might take offense to what I say",
  "Might struggle to remain productive and respectful" = "unwillingness-reason-I might struggle to remain productive and respectful",
  "Others won't make effort to listen" = "unwillingness-reason-The others might not make a full effort to hear and understand me",
  "Identity might be challenged" = "unwillingness-reason-Someone could challenge ideas or beliefs that play an important role in making me who I am",
  "Might feel invalidated" = "unwillingness-reason-I might feel disempowered, unheard, or invalidated",
  "Might incur social reprecussions" = "unwillingness-reason-I could incur social repercussions (e.g., being excluded by others in the future, putting a strain on my relationships, negative changes to my reputation)",
  "Opinion is unpopular in general" = "unwillingness-reason-My opinion is unpopular in general."
)

percent_concerned_df <- surveys_df %>%
  filter(phase == "willingness-concerns-first-survey") %>%
    # Keep only the columns needed
    select(question, answer) %>%
    # Group by the question text
    group_by(question) %>%
    # Summarize the data for each question
    summarise(
        # Calculate the count of "Yes" answers for this question
        N_yes = sum(answer == "2", na.rm = TRUE),
        # Calculate the percentage
        percent_yes = (N_yes / n()) * 100,
        .groups = 'drop' # Ungroup after summarizing
    ) %>%
    # Select and rename the final columns
    select(
        concern = question, 
        percent_yes
    ) %>%
    # Format to 1 decimal place
    mutate(
        percent_yes = round(percent_yes, 1)
    ) %>%
    # Order the results from highest to lowest percentage
    arrange(desc(percent_yes))


# select the top 5 most frequently endorsed 
top_5_concerns <- percent_concerned_df %>%
  slice(1:5) %>%
  mutate(across(everything(), ~ gsub('unwillingness-reason-', '', .))) %>%
 pull(concern)  

```

```{r}
#| label: Most severely impacting willingness concerns 
#| echo: false

percent_severe_df <- surveys_df %>%
  filter(phase == "follow-up-unwillingness-concerns-survey") %>%
    # Keep only the columns needed
    select(question, answer) %>%
    # Group by the question text
    group_by(question) %>%
    mutate(across(everything(), ~ replace_na(.x, 1))) %>% # making the concerns that participants weren't even worried about into a 1 so they can be represented properly 
    # Summarize the data for each question
    summarise(
        # Calculate the count of "Yes" answers for this question
        N_severe = sum(answer == "7" | answer == "6", na.rm = TRUE),
        # Calculate the percentage
        percent_severe = (N_severe / n()) * 100,
        .groups = 'drop' # Ungroup after summarizing
    ) %>%
    # Select and rename the final columns
    select(
        concern = question, 
        percent_severe
    ) %>%
    # Format to 1 decimal place
    mutate(
        percent_severe = round(percent_severe, 1)
    ) %>%
    # Order the results from highest to lowest percentage
    arrange(desc(percent_severe))


# select the top 5 most frequently endorsed 
top_5_severe <- percent_severe_df %>%
  slice(1:5) %>%
  mutate(across(everything(), ~ gsub('unwillingness-reason-', '', .))) %>%
 pull(concern)  

```

```{r}
#| label: fig-openmindedness-density
#| include: false
#| fig-cap: Scatterplot with line of equality of pre- and post-conversation mean ratings of subjects' open-mindedness when discussing their topic. Subjects rated their open-mindedness on a scale from 0 to 7, where a rating of 0 meant subject was not open-minded and a rating of 7 meant subject was open-minded.

openmindedness_df <- surveys_df %>%
  filter(grepl("openmindedness", question)) %>%
  mutate(answer = as.numeric(answer)) 

openmindedness_df_mean <- openmindedness_df %>%
  group_by(prolific_subject_id) %>%
  summarize(mean_openmindedness = mean(as.numeric(answer))) %>%
  ungroup()

openmindedness_density <- ggplot(openmindedness_df_mean, aes(x = mean_openmindedness)) + #, color = condition.x)) +
  geom_density() +  
  labs(x = "Mean Open-mindedness Rating", y = "Density of Rating", title = "Average Open-mindedness about Selected Topic")  #, color = "Group") +
 # scale_color_manual(values = c("control free reflection" = "darkseagreen", "selection bot" = "cornflowerblue", "combination bot" = "navyblue"), 
 #                    labels = c("control free reflection" = "Control group", "selection bot" = "Experimental group: Selection bot", "combination bot" = "Experimental group: Combination bot"))

print(openmindedness_density)

```

```{r}
#| label: fig-openmindedness-likert-bar-plot
#| echo: false
#| fig-cap: Likert bar plot displaying subjects' ratings of five questions about their open-mindedness. The five questions were derived from @elnakouri2024 and modified for application to this experiment. On a scale from 0 to 7, where a rating of 0 meant subject strongly disagreed and a rating of 7 meant strongly agree, subjects rated how true the five statements were for them. Three of the statements were negatively worded in the survey but have been worded positively and reverse-coded in this plot for clarity.

# only works though if all "1" to "7" ratings are in the df

wide_for_likert_openmindedness_df_with_sub_labels <- openmindedness_df %>%
  select(question, answer, prolific_subject_id) %>%
  pivot_wider(names_from = question, values_from = answer) %>%
  ungroup() %>%
  rename("Has patience for opposing arguments" = "openmindedness...have little patience for arguments that I disagree with @R@", 
"Does not avoid opposing messages" = "openmindedness...avoid messages that I disagree with @R@", 
"Believes it is important to pay attention to all politics ideas" = "openmindedness...believe it is a waste of time to pay attention to certain political ideas about $topic @R@",
"Is open to other political viewpoints" = "openmindedness...am open to considering other political viewpoints about $topic",
"Considers as many different opinions as possible" = "openmindedness...consider as many different opinions as possible about $topic") 

wide_for_likert_openmindedness_df <- wide_for_likert_openmindedness_df_with_sub_labels %>%
  select(-prolific_subject_id) %>%
  mutate(across(everything(), ~ factor(.x, levels = 1:7,
     labels = c("Strongly Disagree", "Moderately Disagree", "Somewhat Disagree",
                "Neutral", "Somewhat Agree", "Moderately Agree", "Strongly Agree"),
     ordered = TRUE)))

### create a vector of the all 1-7 values. defining again here in case stuff gets moved around 

all_likert_values <- c("Strongly Disagree", "Moderately Disagree", "Somewhat Disagree", "Neutral", "Somewhat Agree", "Moderately Agree", "Strongly Agree")
### get the column names from  existing df
openmindedness_column_names <- colnames(wide_for_likert_openmindedness_df)
### create 7 new rows
new_openmindedness_likert_rows <- lapply(all_likert_values, function(response) {
  as.list(setNames(rep(response, length(openmindedness_column_names)), openmindedness_column_names))
}) %>%
  bind_rows()
### add the new rows to the existing dataframe
wide_for_likert_openmindedness_df <- bind_rows(wide_for_likert_openmindedness_df, new_openmindedness_likert_rows)


# standardize all columns to use the same factor object
wide_for_likert_openmindedness_df <- wide_for_likert_openmindedness_df %>%
  mutate(across(everything(), ~ factor(.x, levels = all_likert_values, ordered = TRUE)))

openmindedness_likert <- likert(as.data.frame(wide_for_likert_openmindedness_df))

# for the future when comparing pre and post, might want to use this example line: df1_likert <- likert(items=df1[,3:4], grouping=df1[,2]) 

plot(openmindedness_likert, legend.position="right")
```

Subjects generally rated their open-mindedness quite high, as observed in Figure @ref(fig-openmindedness-likert-bar-plot).

```{r}
#| label: Making free response csv for chat analysis 
#| echo: false

free_response_df <- surveys_df %>%
  filter(question == "willingness-free-response") %>%
  ungroup() %>%
  select(prolific_subject_id, answer, topicChoiceAsString, Age, Sex, Ethnicity, "Ethnicity simplified", "Highest education level completed", "U.s. political affiliation", "Political spectrum (us)", "Religious affiliation", "Employment status", "Student status") %>%
  rename(
    #Condition = condition,
    'Focus topic' = 'topicChoiceAsString',
    'Free response' = 'answer'
    )

write.csv(free_response_df, "pretest_free_response_data.csv", row.names = FALSE)
```

```{r}
#| label: Df for 20 manually reviewed free responses 
#| echo: false 

# I used random.org to randomly select 20 numbers from 1 to 202, which will correspond to the row number in the free_response df

# 1. 129 
# 2. 179
# 3. 164
# 4. 199
# 5. 184
# 6. 116
# 7. 160
# 8. 38
# 9. 60
# 10. 76
# 11. 165
# 12. 1
# 13. 83
# 14. 106
# 15. 70
# 16. 33
# 17. 15
# 18. 46
# 19. 82
# 20. 81

free_response_manual_df <- free_response_df %>%
  filter(prolific_subject_id == "662938efd711c3639bd68aa3" |# 1. 129 
           prolific_subject_id == "675de3bd64119cef93edfc75" |# 2. 179
           prolific_subject_id == "672eda41e28ba46f845249df" |# 3. 164
           prolific_subject_id == "65c10e1bba59f80070501946" |# 4. 199
           prolific_subject_id == "64012a2623351894f2917605" |# 5. 184
           prolific_subject_id == "667610590a062036fb381ccd" |# 6. 116
           prolific_subject_id == "60712d937752fb8780e89951" |# 7. 160
           prolific_subject_id == "6624638357b56eb340c8f109" |# 8. 38
           prolific_subject_id == "660460162a2b3e5c554cf140" |# 9. 60
           prolific_subject_id == "609f58e8af0f5f3acedd9806" |# 10. 76
           prolific_subject_id == "646e4572cbed17f6bbacec12" |# 11. 165
           prolific_subject_id == "5dd86784dcec8782b85cb5bb" |# 12. 1
           prolific_subject_id == "66b979589d208d3ec691d89b" |# 13. 83
           prolific_subject_id == "66961b0829c07b563688c86d" |# 14. 106
           prolific_subject_id == "678ff81c8c6b56f93355e0bb" |# 15. 70
           prolific_subject_id == "63671bfc81a5eca4218b6417" |# 16. 33
           prolific_subject_id == "666efed91a31f3c82021f11d" |# 17. 15
           prolific_subject_id == "67335e8a302aac96120cc104" |# 18. 46
           prolific_subject_id == "671bf57baf2c2374636a8c4d" |# 19. 82
           prolific_subject_id == "6724260700cede758eb0c062" # 20. 81
           )

```

```{r}
#| label: tbl-free-response-examples-table
#| echo: false
#| tbl-cap: Table showing examples of uncaptured concerns in subjects' responses to free response question, which asked subjects "Think back to the last time you chose to not participate in a conversation about [topic] or another topic you found uncomfortable to discuss. In the space below, please describe why you decided to not engage. Consider whether your reasons align with any of the potential concerns listed in this survey, or if you had different concerns."

# This table should show a couple very typical responses, which, based on LLM analysis, should probably be consistent with our nineteen items 

free_response_for_table <- free_response_df %>%
   filter(prolific_subject_id == "5d3695969749cc00165c222f"
          | prolific_subject_id == "66293ab7b323f5f7ce785c38"
          | prolific_subject_id == "60712d937752fb8780e89951"
          | prolific_subject_id == "61074ae762b0186a302f1028"
          | prolific_subject_id == "565bff58c121fe0005fc390d"
          | prolific_subject_id == "666efed91a31f3c82021f11d") %>%
   select("Free response", "Focus topic")

kable(free_response_for_table, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")
```

### Comprehensiveness of Concerns

To assess the comprehensiveness of the inventory, we analyzed the free-response data using LLMs with manual review on 10% of the sample for validation. We found that topic complexity/lack of knowledge, perceived lack of relevance, professional consequences, and emotional/cognitive bandwidth were not captured by our nineteen items. Additionally, we found that while the concerns that others' views wouldn't change or that personal relationships might get damaged were both nested within our items, their prevalence among the data may warrant us turning these concerns into their own items.

```{r}
#| label: tbl-free-response
#| include: false
#| fig-cap: Table of example free responses in which participants expressed avoidance of conversations because they felt that the topic was too complex or that they did not know enough about the topic to engage in a conversation about it.


fr_knowledge <- free_response_df %>%
  filter(prolific_subject_id == "67ac21373a76570ef0a5c0d3" | prolific_subject_id == "66b979589d208d3ec691d89b" | prolific_subject_id == "5d3695969749cc00165c222f") %>%
  select("Free response", "Focus topic")

kable(fr_knowledge, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")

```

Topic complexity/lack of knowledge: These participants reported that they avoided conversations because they did not know enough about the topic to engage in a conversation about it. We have items that can capture fear or embarrassment about appearing to be uninformed, but not the pragmatic concern that an individual does not know enough. We plan to capture this with the new item: "I would feel like the topic is too complex or requires more knowledge than I have."

```{r}
#| label: tbl-relevance-free-response
#| include: false
#| fig-cap: Table of example free responses in which participants expressed avoidance of conversations because they felt that they did not know enough about the topic to engage in a productive conversation about it with people who hold differing opinions. 

#6101d290cc9593237a346123

fr_relevance <- free_response_df %>%
  filter(prolific_subject_id == "5d3695969749cc00165c222f" | prolific_subject_id == "66293ab7b323f5f7ce785c38" | prolific_subject_id == "5b5a822e1ad8270001c4f28d") %>%
  select("Free response", "Focus topic")

kable(fr_relevance, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")

```

Perceived lack of relevance: Some participants reported avoiding conversations because they felt that the topic was not personally relevant to them or that it was not their place to comment because they did not have first-hand experiences or were not affected by the topic. We do not have items that capture disinterest or an intentional ethical boundary, so we plan to capture this new area with the item: "I don't have first-hand experience with the topic and/or it isn't relevant to me."

```{r}
#| label: tbl-prof-consequences-free-response
#| include: false
#| fig-cap: Table of example free responses that were concerned with professional/workplace consequences that might follow from participating in a conversation about a controversial topic


fr_prof <- free_response_df %>%
  filter(prolific_subject_id == "55ecd6ff748092000daa9f5f" | prolific_subject_id == "5ac440da9534ba0001c736da" | prolific_subject_id == "63469e0a9552017beb3cfb82") %>%
  select("Free response", "Focus topic")

kable(fr_prof, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")

```

Professional consequences: Some participants reported avoiding conversations because they could have consequences in the workplace such as employment/HR problems, damaged professional standing, or workplace drama. We originally anticipated that workplace-related concerns would be captured by our social evaluation items; however, it is clear that consequences in the workplace can extend beyond the typical repercussions in other social contexts. We plan to capture this concern with the item: "I could experience significant repercussions at work or in other spaces beyond my personal social network."

```{r}
#| label: tbl-bandwith-free-response
#| include: false
#| fig-cap: Table of example free responses that were concerned with emotional and cognitive fatigue that might follow from participating in a conversation about a controversial topic


fr_energy <- free_response_df %>%
  filter(prolific_subject_id == "61074ae762b0186a302f1028" | prolific_subject_id == "6648becccd1bf5a6e253577b" | prolific_subject_id == "565bff58c121fe0005fc390d") %>%
  select("Free response", "Focus topic")

kable(fr_energy, "html") %>%
  kable_styling() %>%
  column_spec(2, width = "5em")

```

Emotional/cognitive bandwith: Many participants reported that it was not worth their energy to engage in these conversations. We originally excluded this concern because we anticipated that not wanting to expend energy is a product of other concerns that we may be able to more adequately target with our future intervention. However, the prevalence of this concern warrants its own item. We plan to capture this with the item: "It wouldn't be worth the emotional and/or mental energy."

Others' views not changing: Some participants expressed that the conversation would be a waste of time because others' views wouldn't change. We have items about productivity, but this concern seems to be a more specific and commonly anticipated reason for why a conversation may be unproductive, so it may be worth turning this concern into its own item. We plan to capture it with the item: "It would be a waste of time because the others' views wouldn't change."

Additionally, while we nested the concern about damaging personal relationships within an item about social repercussions, this concern was often discussed in a personal or intimate sense, distinguishing it from the more abstract or broad concept of social repercussions. Because of this, we plan to separate damaging personal relationships from our item about social repercussions. We will have one item be "I would not want to hurt my relationship(s) with others or upset them" and another be "I could incur social repercussions (e.g., being excluded by others in the future, negative changes to my reputation, feeling ostracized)."

In sum, we plan to add six news items to our questionnaire: "I would feel like the topic is too complex or requires more knowledge than I have," "I don't have first-hand experience with the topic and/or it isn't relevant to me," "I could experience significant repercussions at work or in other spaces beyond my personal social network," "It wouldn't be worth the emotional and/or mental energy," "I would not want to hurt my relationship(s) with others or upset them", "I could incur social repercussions (e.g., being excluded by others in the future, negative changes to my reputation, feeling ostracized)."

```{r}
#| label: fig-unwillingness-reason-likert-bar-plot
#| echo: false
#| fig-cap: Likert bar plot displaying subjects' ratings of how much certain concerns contribute to their unwillingness to converse. The nineteen potential concerns were derived from existing literature and pretested in a pilot experiment. On a scale from 0 to 7, where a rating of 0 meant participant's willingness is not at all impacted by the concern and a rating of 7 meant participant's willingness is strongly impacted by the concern, participants rated the potential concerns' effects on their willingness to converse.

# only works though if all "1" to "7" ratings are in the df

wide_unwill_reasons_df_1 <- surveys_df %>%
  filter(grepl("unwillingness-reason", question)) %>%
  filter(phase == "follow-up-unwillingness-concerns-survey") %>%
  select(question, answer, prolific_subject_id) %>%
  group_by(prolific_subject_id) %>%
  mutate(answer = as.numeric(answer)) %>%
  pivot_wider(id_cols = prolific_subject_id, names_from = question, values_from = answer) %>%
  ungroup() %>%
  rename("Might experience negative feelings about self" = "unwillingness-reason-The conversation could negatively affect how I feel about myself during and/or afterward (e.g.,  fear, sadness, anger, vulnerability)", "Might be offended" = "unwillingness-reason-I might take offense to what someone says",
  "Opinion is unpopular amongst group" = "unwillingness-reason-I would feel like my opinion is unpopular amongst the group.",
  "Doesn't participate in these conversations often" = "unwillingness-reason-I don’t often participate in conversations like these",
  "Might struggle to explain views or seem uninformed" = "unwillingness-reason-I might struggle to explain my views to the others and/or come across as uninformed",
  "Others might be unproductive or disrespectful" = "unwillingness-reason-I would not trust the others to keep the conversation productive and respectful (e.g., keeping emotions in check, refraining from making hostile remarks, not dominating the conversation)",
  "Views wouldn't change" = "unwillingness-reason-It would be a waste of time because my views wouldn't change",
  "Doesn't want to listen to others" = "unwillingness-reason-I would not want to listen to the others discuss their views",
  "Might be criticized" = "unwillingness-reason-Someone could criticize or show disapproval of my views",
  "Experience negative feelings about the world" = "unwillingness-reason-The conversation could negatively affect how I feel about the world during and/or afterward",
  "Conversation might be awkward or tense" = "unwillingness-reason-The conversation could become awkward or tense",
  "Might feel embarrassed" = "unwillingness-reason-I could feel humiliated or embarrassed in the conversation",
  "Might offend someone" = "unwillingness-reason-Someone might take offense to what I say",
  "Might struggle to remain productive and respectful" = "unwillingness-reason-I might struggle to remain productive and respectful",
  "Others won't make effort to listen" = "unwillingness-reason-The others might not make a full effort to hear and understand me",
  "Identity might be challenged" = "unwillingness-reason-Someone could challenge ideas or beliefs that play an important role in making me who I am",
  "Might feel invalidated" = "unwillingness-reason-I might feel disempowered, unheard, or invalidated",
  "Might incur social reprecussions" = "unwillingness-reason-I could incur social repercussions (e.g., being excluded by others in the future, putting a strain on my relationships, negative changes to my reputation)",
  "Opinion is unpopular in general" = "unwillingness-reason-My opinion is unpopular in general."
) %>%
  mutate(across(everything(), ~ replace_na(.x, 1))) # making the concerns that participants weren't even worried about into a 1 so that they can be represented in the likert graph as "not at all"


wide_unwill_reasons_df_2 <- wide_unwill_reasons_df_1 %>%
  select(-prolific_subject_id) %>%
  mutate(across(everything(), ~ factor(.x, levels = 1:7,
     labels = c("Not at all", "Slightly", "Somewhat",
                "Moderately", "Quite a bit", "Very much", "Extremely"),
     ordered = TRUE))) 

unwillingness_reason_likert <- likert(as.data.frame(wide_unwill_reasons_df_2))

# for the future when comparing pre and post, might want to use this example line: df1_likert <- likert(items=df1[,3:4], grouping=df1[,2])

suppressWarnings(plot(unwillingness_reason_likert, legend.position="right"))

```

```{r}
#| label: Wrangle data for factor analysis
#| echo: false

# create short labels for each question
question_key_for_fa <- surveys_df %>%
  filter(phase == "follow-up-unwillingness-concerns-survey") %>%
  ungroup() %>%
  select(question) %>%
  distinct(question) %>%
  arrange(question) %>%  # ensure consistent order
  mutate(short_label = paste0("Q0", row_number()))

# join short labels back to the main data
surveys_df <- surveys_df %>%
  filter(phase == "follow-up-unwillingness-concerns-survey") %>%
  full_join(question_key_for_fa, by = "question") 

concerns_labeled_wide <- surveys_df %>%
  select(prolific_subject_id, short_label, answer) %>%
  pivot_wider(names_from = short_label, values_from = answer) %>%
  mutate(across(starts_with("Q0"), ~ as.numeric(as.character(.)))) %>%
  ungroup() 

concerns_labeled_wide_na_replaced <- concerns_labeled_wide %>%
   mutate(across(everything(), ~ replace_na(.x, 1))) # making the concerns that participants weren't even worried about into a 1 so that they can be represented in the likert graph as "not at all"

concerns_wide_na_replaced <- concerns_labeled_wide_na_replaced %>%
  select(-prolific_subject_id)

concerns_wide <- concerns_labeled_wide %>%
  select(-prolific_subject_id)

row_na_counts <- rowSums(is.na(concerns_labeled_wide))
total_na <- sum(row_na_counts)
avg_na_per_row <- mean(row_na_counts)

```

```{r}
#| label: tbl-correlation-matrix
#| echo: false
#| tbl-cap: Correlation matrix of reasons for unwillingness

cor_matrix <- cor(concerns_wide_na_replaced, use = "pairwise.complete.obs")

library(corrplot)
corrplot <- corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7)

# the items all seem to show low to moderate correlations. pretty solid
```

```{r}
#| label: Check KMO, Bartlett's test, and Rit values to see if data is well-suited for factor analysis 
#| include: false 

# KMO and Bartlett’s test
kmo <- KMO(concerns_wide_na_replaced)
cortest_bartlett <- cortest.bartlett(cor(concerns_wide_na_replaced), n = nrow(concerns_wide_na_replaced))

#kmo is above 0.8 so our data is well-suited for factor analysis!  

#cortest_bartlett shows that the p-value is VERY small, so correlations are statistically significant 

#check internal consistency
rit.vals <- alpha(concerns_wide_na_replaced)

#raw alpha is 0.88 which is pretty good 
#all the r.cor values are pretty good 

# all indicators suggest that the data is well-suited for factor analysis 

```

```{r}
#| label: Factor analysis on 19 items 
#| include: false

# determine number of factors with parallel analysis. this will output a suggested number of factors and a scree plot 
fa_parallel_results <- fa.parallel(concerns_wide_na_replaced, fa = "fa")

# the results of parallel analysis suggest 3 factors 

# run factor analysis with 3 factors 
## we use oblimin because the factors are probably correlated 
fa_result <- fa(
    concerns_wide_na_replaced,
    nfactors = 3,
    rotate = "oblimin"
    )

#get explained variance for report 
total_var_explained_percent <- round(sum(fa_result$Vaccounted["Proportion Var", ]) * 100, 1)

#get factor correlations 
factor_corrs <- fa_result$r 

# extract min and max off-diagonal correlations
cor_range <- range(factor_corrs[lower.tri(factor_corrs)])
cor_min <- round(cor_range[1], 2)
cor_max <- round(cor_range[2], 2)
```

```{r}
#| label: tbl-factor-loadings-visual
#| echo: false
#| tbl-cap: Table visualizing factor loadings in exploratory factor analysis.

library(tidyverse)

# visualize factor loadings
loadings_df <- as.data.frame(fa_result$loadings[1:19, ])
loadings_df$short_label <- rownames(loadings_df)

loadings_long <- loadings_df %>% 
  pivot_longer(-short_label, names_to = "factor", values_to = "loading")

factor_loadings_plot <- ggplot(loadings_long, aes(x = reorder(short_label, loading), y = loading, fill = factor)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(
    title = "Factor Loadings",
    x = "Question",
    y = "Loading",
    fill = "Factor"
  ) +
  theme_minimal()

# not printing this because i prefer the heat map 
```

```{r}
#| label: tbl-fa-heat-map-df
#| echo: false
#| tbl-cap: Table showing factor analysis heat map.

fa_heat_map <- ggplot(loadings_long, aes(x = factor, y = reorder(short_label, desc(short_label)), fill = loading)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red",
    midpoint = 0, limit = c(-1, 1), name = "Loading"
  ) +
  labs(
    title = "Heat Map of Factor Loadings",
    x = "Factor",
    y = "Question"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10))

print(fa_heat_map)

```

### Factor Analysis on Concerns

We examined the initial binary endorsement ratings of our nineteen items where each participant simply indicated whether an item concerned them at all, as well as the Likert-scale ratings of how much each of the endorsed items impact the participant's willingness. The top five most frequently endorsed concerns (answered "Yes") were: `r paste0(shQuote(top_5_concerns[1:5]), collapse=", ")`. The top five concerns most severely impacting willingness were `r paste0(shQuote(top_5_severe[1:5]), collapse=", ")`. The full distribution of severity ratings can be observed in Figure \@ref(fig:fig-unwillingness-reason-likert-bar-plot).

We conducted an exploratory factor analysis (EFA) on the 19 concern items using oblimin rotation to identify latent factors that could reflect correlated concerns. Parallel analysis suggested a three-factor solution, which accounted for `r total_var_explained_percent`% of the total variance. The factors were moderately correlated ($r$ = `r cor_min`–`r cor_max`) as shown in Figure \@ref(fig:tbl-correlation-matrix). Many cross-loadings were present, suggesting some conceptual overlap among factors. We retained all nineteen items to allow full examination of the emerging structure.

We interpret the the three factors, which are visualized in Figure @ref(fig:tbl-fa-heat-map-df), as:

Emotional Vulnerability/Identity Threat (Factor 1): This first factor consists of items focusing on internal psychological distress and threats to the participant's self-concept or core beliefs. The items in this factor are: "I might take offense to what someone says," "Someone could challenge ideas or beliefs that play an important role in making me who I am," "I might feel disempowered, unheard, or invalidated," "The conversation could negatively affect how I feel about myself during and/or afterward (e.g., fear, sadness, anger, vulnerability)," "Someone could criticize or show disapproval of my views," and "The conversation could negatively affect how I feel about the world during and/or afterward."

Conversational Efficacy/Unproductive Process (Factor 2): This factor captures concerns about the quality, utility, and worth of the interaction. The focus here is on whether the conversation will be respectful, worthwhile, or productive. This factor consists of: "I might struggle to remain productive and respectful," "I would not trust the others to keep the conversation productive and respectful (e.g., keeping emotions in check, refraining from making hostile remarks, not dominating the conversation)", "I would not want to listen to the others discuss their views", and "It would be a waste of time because my views wouldn't change."

Social Evaluation/Reputation Risk (Factor 3): This factor emphasizes fear of external perception and broader social consequences. It emphasizes concern about how others perceive the participants, including fear of embarrassment, judgment, or negative social consequences. This factor consists of: "I could incur social repercussions (e.g., being excluded by others in the future, putting a strain on my relationships, negative changes to my reputation)", "I would feel like my opinion is unpopular amongst the group," "I could feel humiliated or embarrassed in the conversation," "I might struggle to explain my views to the others and/or come across as uninformed," "My opinion is unpopular in general."

Four items did not load above 0.32 on any factor in the 3-factor solution nor in an additional exploratory 4-factor solution. These items were: "Someone might take offense to what I say," "The conversation could become awkward or tense," "The others might not make a full effort to hear and understand me," and "I don’t often participate in conversations like these". It would be expected that these items represent idiosyncratic concerns or situational/contextual items that don't fit neatly into emotional vulnerability, process, or social evaluation; however, these items seem as though they would fit within the social evaluation factor \[not sure what to make of this...\].

It is also interesting that factors 1 and 3 are distinct from each other, as most of the items within these factors seem as though they could very well be interchangeable in the factors. For example, "I could feel humiliated or embarrassed in the conversation" fell within factor 3 with a loading of `r loadings_df$MR3[loadings_df$short_label == "Q01"]`, but conceptually could very well fit within the emotional vulnerability concepts of factor 1 even though it had a loading within factor 1 of `r loadings_df$MR1[loadings_df$short_label == "Q01"]`.

### Correlations

```{r}
#| label: Function for printing correlation results
#| echo: false

print_significant_correlations <- function(df) {
  formatted_items <- df %>%
    filter(significance != "ns") %>%
    mutate(
      formatted = paste0(
       '"', question, '"',
        " ($r$ = ", estimate_rounded,
        ", $p$ = ", significance, ")"
      )
    ) %>%
    pull(formatted)
  
  n <- length(formatted_items)
  
  if (n == 0) {
    return("No significant correlations found.")
  } else if (n == 1) {
    return(formatted_items)
  } else if (n == 2) {
    return(paste(formatted_items, collapse = " and "))
  } else {
    return(paste(
      paste(formatted_items[-n], collapse = ", "),
      "and", formatted_items[n]
    ))
  }
}


```

```{r}
#| label: fig-EDA-correlation-openminded-relationships 
#| echo: false 
#| fig-cap: Relationships between each willingness concern and average open-mindedness. Using a 7-pt Likert-scale, participants rated the impact of each willingness concern on their overall willingness. Also using a 7-pt Likert-scale, participants rated 5 statements about their open-mindedness, which we averaged to find each participant's average open-mindedness.  

library(gridExtra) # so we can print the plots in columns 

cor_openmindedness_mean_vs_concerns <- concerns_wide_na_replaced %>% map_dfr(~ tidy(cor.test(.x, openmindedness_df_mean$mean_openmindedness)), .id = "concern")

concerns_and_avg_openmindedness_df <- openmindedness_df_mean %>%
  select(mean_openmindedness, prolific_subject_id) %>%
  full_join(concerns_labeled_wide_na_replaced, by = "prolific_subject_id") %>%
  select(-"prolific_subject_id")

# see the average number of concerns that people indicated as affecting them 

# Create all the individual plots
p1.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q01, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p2.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q02, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p3.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q03, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p4.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q04, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p5.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q05, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p6.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q06, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p7.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q07, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p8.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q08, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p9.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q09, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p10.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q010, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p11.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q011, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p12.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q012, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p13.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q013, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p14.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q014, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p15.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q015, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p16.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q016, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p17.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q017, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p18.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q018, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p19.o <- ggplot(concerns_and_avg_openmindedness_df, aes(x = Q019, y = mean_openmindedness)) + geom_point() + geom_jitter(width = 0.2, height = 0)

# arrange the plots in a grid
suppressWarnings(grid.arrange(p1.o, p2.o, p3.o, p4.o, p5.o, p6.o, p7.o, p8.o, p9.o, p10.o, p11.o, p12.o, p13.o, p14.o, p15.o, p16.o, p17.o, p18.o, p19.o, ncol = 2))


### the results seem to pretty much show no relationship, but if you squint your eyes, there is vaguely a linear relationship in some of the variables. i'm going to stick with pearson because, to my knowledge, the data doesn't meet the standards for any non-parametric methods 

```

```{r}
#| label: Correlations open-mindedness average vs. willingness concerns (NA is not replaced)
#| echo: false
#| tbl-cap: Correlation matrix of average open-mindedness vs. willingness concerns 

# look at correlation relationship between mean open-mindedness score and each willingness concern
library(broom)

cor_openmindedness_mean_vs_concerns <- concerns_wide_na_replaced %>% map_dfr(~ tidy(cor.test(.x, openmindedness_df_mean$mean_openmindedness)), .id = "concern")

# view results
cor_openmindedness_mean_vs_concerns_simplified <- cor_openmindedness_mean_vs_concerns %>%
  select(concern, estimate, p.value) %>%
  arrange(p.value) %>% 
  full_join(question_key_for_fa, by = c("concern" = "short_label"))
```

```{r}
#| label: Correlations open-mindedness average vs. willingness concerns NA replaced
#| echo: false


# using the df that replaced empty entries with "not at all" because this is conceptually correct
cor_openmindedness_mean_vs_concerns_na_replaced <- concerns_wide_na_replaced %>% map_dfr(~ tidy(cor.test(.x, openmindedness_df_mean$mean_openmindedness)), .id = "concern") 

# view results
cor_openmindedness_mean_vs_concerns_simplified_na_replaced <- cor_openmindedness_mean_vs_concerns_na_replaced %>% 
    mutate(
    significance = case_when(
      p.value < 0.001 ~ "< 0.001",
      p.value < 0.01  ~ "< 0.01",
      p.value < 0.05  ~ "< 0.05",
      TRUE            ~ "ns"  # not significant
    ),
    direction = if_else(estimate < 0, "negative", "positive"),
    estimate_rounded = round(estimate, 3),
    pvalue_rounded = signif(p.value, 3)
  ) %>%
  select(concern, estimate_rounded, pvalue_rounded, direction, significance) %>%
  full_join(question_key_for_fa, by = c("concern" = "short_label")) %>%
    mutate(across(everything(), ~ gsub('unwillingness-reason-', '', .))) 

```

Accounting for the concerns that participants did not endorse as "not at all" impacting participants' willingness, we found correlations between most willingness concerns and average open-mindedness. These relationships suggests that, for most willingness concerns, a greater impact of the concern on an American's willingness is associated with a decrease in willingness to converse.

These concerns were `r print_significant_correlations(cor_openmindedness_mean_vs_concerns_simplified_na_replaced)`.

```{r}
#| label: fig-EDA-correlation-relationships 
#| echo: false 
#| fig-cap: Relationships between each willingness concern and average willingness to converse. Using a 7-pt Likert-scale, participants rated the impact of each willingness concern on their overall willingness. Using a slider from 0 to 100, participants rated 2 statements about their willingness to converse, which we averaged to find each participant's average willingness to converse about their focus topic. 

library(gridExtra) # so we can print the plots in columns 

concerns_and_avg_willingness_df <- concerns_labeled_wide %>%
  full_join(avg_willingness_df, by = "prolific_subject_id")

# see the average number of concerns that people indicated as affecting them 

# Create all the individual plots
p1 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q01, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p2 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q02, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p3 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q03, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p4 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q04, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p5 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q05, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p6 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q06, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p7 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q07, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p8 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q08, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p9 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q09, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p10 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q010, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p11 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q011, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p12 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q012, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p13 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q013, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p14 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q014, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p15 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q015, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p16 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q016, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p17 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q017, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p18 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q018, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)
p19 <- ggplot(concerns_and_avg_willingness_df, aes(x = Q019, y = avg_willingness)) + geom_point() + geom_jitter(width = 0.2, height = 0)

# arrange the plots in a 2x8 grid
suppressWarnings(grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p14, p15, p16, p17, p18, p19, ncol = 2))


### the results seem to pretty much show no relationship, but if you squint your eyes, there is vaguely a linear relationship in some of the variables. i'm going to stick with pearson because, to my knowledge, the data doesn't meet the standards for any non-parametric methods 

```

```{r}
#| label: Correlations willingness slider average vs. willingness concerns (NA is not replaced) 
#| echo: false

#prepping data
reasons_and_willingness_merged_df <- merge(concerns_labeled_wide, avg_willingness_df, by = 'prolific_subject_id') %>%
  ungroup() %>%
  select(-prolific_subject_id, -topicChoiceAsString)

#run cor test
cor_willingness_slider_vs_concerns <- reasons_and_willingness_merged_df %>% select(-avg_willingness) %>%
  map_dfr( ~ tidy(cor.test(.x, reasons_and_willingness_merged_df$avg_willingness)),
  .id = "reason_item"
)

#process results 
cor_willingness_slider_vs_concerns <- cor_willingness_slider_vs_concerns %>%
  mutate(
    significance = case_when(
      p.value < 0.001 ~ "< 0.001",
      p.value < 0.01  ~ "< 0.01",
      p.value < 0.05  ~ "< 0.05",
      TRUE            ~ "ns"  # not significant
    ),
    direction = if_else(estimate < 0, "negative", "positive"),
    estimate_rounded = round(estimate, 3),
    pvalue_rounded = signif(p.value, 3)
  ) %>%
  select(reason_item, estimate_rounded, pvalue_rounded, direction, significance)

```

```{r}
#| label: Correlations willingness slider average vs. willingness concerns NA replaced 
#| echo: false

#prepping data
reasons_and_willingness_merged_df_na_replaced <- merge(concerns_labeled_wide_na_replaced, avg_willingness_df, by = 'prolific_subject_id') %>%
  ungroup() %>%
  select(-prolific_subject_id, -topicChoiceAsString)

#run cor test
cor_willingness_slider_vs_concerns_na_replaced_d <- reasons_and_willingness_merged_df_na_replaced %>% select(-avg_willingness) %>%
  map_dfr( ~ tidy(cor.test(.x, reasons_and_willingness_merged_df_na_replaced$avg_willingness)),
  .id = "reason_item"
)

#process results  ### LEFT OFF PRINTING THE SIGNIFICANT ONES 
cor_willingness_slider_vs_concerns_na_replaced <- cor_willingness_slider_vs_concerns_na_replaced_d %>%
  mutate(
    significance = case_when(
      p.value < 0.001 ~ "< 0.001",
      p.value < 0.01  ~ "< 0.01",
      p.value < 0.05  ~ "< 0.05",
      TRUE            ~ "ns"  # not significant
    ),
    direction = if_else(estimate < 0, "negative", "positive"),
    estimate_rounded = round(estimate, 3),
    pvalue_rounded = signif(p.value, 3)
  ) %>%
  select(reason_item, estimate_rounded, pvalue_rounded, direction, significance) %>%
  full_join(question_key_for_fa, by = c("reason_item" = "short_label")) %>%
    mutate(across(everything(), ~ gsub('unwillingness-reason-', '', .))) 

```

Next, we looked for correlations between each willingness concerns and average willingness as indicated by the two slider questions. Most concerns were negatively associated with willingness, suggesting that, for most of the willingness concerns, a greater impact of the concern on an American's willingness is associated with lower willingness to engage.

These concerns were `r print_significant_correlations(cor_willingness_slider_vs_concerns_na_replaced)`.

### Cluster analysis

```{r}
#| label: Cluster analysis
#| echo: false

# scale 
concerns_wide_na_replaced_scaled <- scale(concerns_wide_na_replaced) 

# determine optimal number of clusters using wss 
optimal_n_clusters <- fviz_nbclust(concerns_wide_na_replaced_scaled, kmeans, method = "wss")

# K-means with k = 3 (example)
set.seed(1)
kmeans_result <- kmeans(concerns_wide_na_replaced_scaled, centers = 3)

# merge back with subject id df so we can see sub id, cluster, and the questions w ratings
cluster_labeled <- concerns_labeled_wide_na_replaced %>%
  mutate(cluster = factor(kmeans_result$cluster))

# extract cluster info to add to surveys_df becuase the df is structured differently so i'm not sure how to merge otherwise 
cluster_info <- cluster_labeled %>%
  select(prolific_subject_id, cluster)

surveys_df <- surveys_df %>%
  left_join(cluster_info, by = "prolific_subject_id")

#extract the percent of participants in each cluster so i can report
cluster_percentages <- cluster_info %>%
  group_by(cluster) %>%                       # group by cluster
  summarise(n = n_distinct(prolific_subject_id)) %>%  # count unique subjects
  mutate(percent = round(n / sum(n) * 100, 1))       # calculate percentage

```

```{r}
#| label: tbl-cluster-analysis-pca
#| include: false
#| fig-cap: PCA plot with clusters to visualize the three clusters

# visualize clusters with PCA
cluster_visualization <- autoplot(prcomp(concerns_wide_na_replaced_scaled), data = cluster_labeled, colour = 'cluster')

print(cluster_visualization)

```

```{r}
#| label: fig-heat-map-clusters-willingness-concerns
#| echo: false 
#| fig-cap: Heat map of each cluster's average rating of each potential concern

# figure out the average score on each question of a cluster so we can see if certain clusters generally scored higher on certain questions 

suppressWarnings(cluster_concerns <- cluster_labeled %>%
  group_by(cluster) %>%
  summarise(across(starts_with("Q"), mean, na.rm = TRUE)))

#heat map

cluster_long <- cluster_concerns %>%
  pivot_longer(cols = starts_with("Q"), names_to = "question", values_to = "mean_rating")

ggplot(cluster_long, aes(x = question, y = factor(cluster), fill = mean_rating)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(y = "Cluster", x = "Concern", fill = "Mean Rating") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
#| label: Interpreting clusters and concerns 
#| echo: false 

# find each cluster's overall mean rating to see if any clusters generally rate things significantly higher 
cluster_means <- cluster_concerns %>%
  rowwise() %>%
  mutate(overall_mean = round(mean(c_across(Q01:Q019)), 1)) %>%
  select(cluster, overall_mean)

# find which questions had the greatest range in average responses from clusters to see which differ most between clusters 

top_questions <- cluster_concerns %>%
  pivot_longer(cols = c(Q01,Q02,Q03,Q04,Q05,Q06,Q07,Q08,Q09,Q010,Q011,Q012,Q013,Q014,Q015,Q016,Q017,Q018,Q019), names_to = "question", values_to = "mean_rating") %>%
  group_by(question) %>%
  summarize(range = max(mean_rating) - min(mean_rating)) %>%
  arrange(desc(range)) %>%
  slice_head(n = 19) %>%   # top 19 questions with largest differences
  pull(question)

# create a table of clusters × top questions
table_concerns <- cluster_concerns %>%
  select(cluster, all_of(top_questions)) %>%
  pivot_longer(cols = -cluster, names_to = "question", values_to = "mean_rating") %>%
  arrange(question, desc(mean_rating)) %>%
  group_by(question) %>%
  mutate(rank = row_number()) %>%
  select(question, cluster, mean_rating, rank)


### cluster-focused

cluster_long <- cluster_concerns %>%
  pivot_longer(cols = c(Q01,Q02,Q03,Q04,Q05,Q06,Q07,Q08,Q09,Q010,Q011,Q012,Q013,Q014,Q015,Q016,Q017,Q018,Q019), names_to = "question", values_to = "mean_rating")

# for each question, find which cluster had the highest mean
top_by_cluster <- cluster_long %>%
  group_by(question) %>%
  mutate(max_cluster = mean_rating == max(mean_rating)) %>%
  ungroup() %>%
  filter(max_cluster) %>%
  select(cluster, question, mean_rating) %>%
  arrange(cluster, desc(mean_rating))

# join top_by_cluster with questions_key_fa to get full question text
top_by_cluster_full <- top_by_cluster %>%
  left_join(question_key_for_fa, by = c("question" = "short_label")) %>%
  mutate(across(everything(), ~ gsub('unwillingness-reason-', '', .))) 

## for inline report: 
# cluster 1 top questions
cluster1_top <- top_by_cluster_full %>%
  filter(cluster == 1) %>%
  arrange(desc(mean_rating)) %>%
  pull(question.y)

# cluster 2 top questions
cluster2_top <- top_by_cluster_full %>%
  filter(cluster == 2) %>%
  arrange(desc(mean_rating)) %>%
  pull(question.y)

# cluster 3 top questions
cluster3_top <- top_by_cluster_full %>%
  filter(cluster == 3) %>%
  arrange(desc(mean_rating)) %>%
  pull(question.y)

```

We also conducted cluster analysis on the 19 concerns to determine whether participants generally fell into certain categories. The analysis suggested that participants generally fell into one of three clusters, with `r cluster_percentages$percent[cluster_percentages$cluster == 1]`% of participants falling into Cluster 1, `r cluster_percentages$percent[cluster_percentages$cluster == 2]`% of participants falling into Cluster 2, and `r cluster_percentages$percent[cluster_percentages$cluster == 3]`% of participants falling into Cluster 3.

As shown in Figure \@ref(fig:heat-map-clusters-willingness-concerns), the clusters appear to arrange participants by the overall impact of the concerns on willingness. Cluster 2 is the low impact group, Cluster 1 is the moderate impact group, and Cluster 3 is the high impact group. It is notable that half of our sample fell into low impact group, meaning about half of our sample felt that most to all of the concerns did not strongly impact their willingness. This could reflect weakness of our questionnaire or it could mean that half of our participants are simply not as concerned as the other half of our sample. The latter would be consistent with Cluster 2's greater average willingness and open-mindedness; although, recalling that willingness and open-mindedness were also self-reported, the trio might also reflect an overall weakness of gathering this data through self reports.

Cluster 1's average rating of a given concern was `r cluster_means$overall_mean[cluster_means$cluster == 1]`. On average, Cluster 1 rated the following concerns the highest: `r paste(shQuote(cluster1_top), collapse = "; ")`.

Cluster 2's average rating of a given concern was `r cluster_means$overall_mean[cluster_means$cluster == 2]`. On average, cluster 2's top concerns were `r paste(shQuote(cluster2_top), collapse = "; ")`.

Cluster 3's average rating of a given concern was `r cluster_means$overall_mean[cluster_means$cluster == 3]`. On average, cluster 3's top concerns were `r paste(shQuote(cluster3_top), collapse = "; ")`.

```{r}
#| label: fig-clusters-avg-willingness   
#| echo: false 
#| fig-cap: Box plot of average willingness to converse about topic by cluster. On average, cluster 2 appears to be more willing to converse than the other clusters; this would be consistent with cluster 2's lower average rating of how much the 19 items impact willingness.

avg_willingness_cluster_df <- avg_willingness_df %>%
  merge(cluster_info, by = "prolific_subject_id")

ggplot(avg_willingness_cluster_df, aes(x = factor(cluster), y = avg_willingness)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "Average Willingness") +
  theme_minimal()

```

```{r}
#| label: fig-clusters-avg-openmindedness 
#| echo: false 
#| fig-cap: Box plot of average open-mindedness about topic by cluster. 

open_mindedness_w_clusters <- openmindedness_df_mean %>%
  merge(cluster_info, by = "prolific_subject_id")

ggplot(open_mindedness_w_clusters, aes(x = factor(cluster), y = mean_openmindedness)) +
  geom_boxplot() +
  labs(x = "Cluster", y = "Average Open-Mindedness") +
  theme_minimal()

```

As shown in Figure \@ref(fig:fig-clusters-avg-willingness), the willingness of each cluster aligns with the impact of concerns, with Cluster 2 reporting the greatest willingness and Cluster 3 reporting the least willingness. Similarly, as shown in Figure \@ref(fig:fig-clusters-avg-openmindedness), Cluster 2 reported the highest open-mindedness and Cluster 3 reported the least open-mindedness. These interpretations have not been rigorously assessed and, again, it should be recalled that these data were self-reported.

```{r}
#| label: fig-demographics-clusters
#| echo: false 
#| fig-cap: Plots of demographics by cluster helps us explore whether the clusters may have significantly different demographics.

sex_cluster_plot <- ggplot(surveys_df, aes(x = cluster, fill = Sex)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Gender composition by cluster")

education_cluster_plot <- ggplot(surveys_df, aes(x = cluster, fill = `Highest education level completed`)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Education level by cluster") # idk why this isn't working

ethnicity_cluster_plot <- ggplot(surveys_df, aes(x = cluster, fill = Ethnicity)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Ethnicity by cluster")

polaffiliation_cluster_plot <- ggplot(surveys_df, aes(x = cluster, fill = `U.s. political affiliation`)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "U.s. political affiliation")

polspectrum_cluster_plot <- ggplot(surveys_df, aes(x = cluster, fill = `Political spectrum (us)`)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Political spectrum (us) by cluster")

surveys_df_recoded_religion <- surveys_df %>%
  mutate(`Religious affiliation` = fct_recode(`Religious affiliation`, "Christianity" = "Christianity (e.g. Baptist, Church of England, Roman Catholic, Methodist, Jehovah Witness,  etc.)", "Non religious" = "Non Religious (e.g. Agnostic, Atheist, No Religion)")) 
  

religion_cluster_plot <- ggplot(surveys_df_recoded_religion, aes(x = cluster, fill = `Religious affiliation`)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Religious affiliation by cluster")

age_cluster_plot <- ggplot(surveys_df, aes(x = cluster, y = as.numeric(Age))) +
  geom_boxplot() +
  labs(title = "Age by cluster")

sex_cluster_plot
age_cluster_plot
ethnicity_cluster_plot
religion_cluster_plot
polaffiliation_cluster_plot
polspectrum_cluster_plot
education_cluster_plot
```

```{r}
#| label: fig-topic-choice-clusters
#| echo: false 
#| fig-cap: Plots of topic choice by cluster helps us explore whether the clusters may have significant differences in which topics they focused on.

# within each cluster, finding percent of subjects on each topic. i tried to do this in one df but kept getting an error that i couldn't figure out 
cluster1_topic_percent <- surveys_df %>%
  filter(cluster == 1) %>%
  distinct(prolific_subject_id, .keep_all = TRUE) %>%
  group_by(topicChoiceAsString) %>%
  summarise(n1 = n(), .groups = "drop") %>%
  mutate(percent1 = n1 / sum(n1))

cluster2_topic_percent <- surveys_df %>%
  filter(cluster == 2) %>%
  distinct(prolific_subject_id, .keep_all = TRUE) %>%
  group_by(topicChoiceAsString) %>%
  summarise(n2 = n(), .groups = "drop") %>%
  mutate(percent2 = n2 / sum(n2))

cluster3_topic_percent <- surveys_df %>%
  filter(cluster == 3) %>%
  distinct(prolific_subject_id, .keep_all = TRUE) %>%
  group_by(topicChoiceAsString) %>%
  summarise(n3 = n(), .groups = "drop") %>%
  mutate(percent3 = n3 / sum(n3))

#join together
cluster_topic_percent <- cluster1_topic_percent %>%
  full_join(cluster2_topic_percent, by = "topicChoiceAsString") %>% 
  full_join(cluster3_topic_percent, by = "topicChoiceAsString")

ggplot(surveys_df, aes(x = cluster, fill = topicChoiceAsString)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", title = "Topic choice by cluster")

```

```{r}
#| label: Diff in religion btwn clusters?
#| include: false

# looking for statistical significance 

# create a contingency table
table_cluster_religion <- table(surveys_df$cluster, surveys_df$`Religious affiliation`)

# View the table
table_cluster_religion 

# Chi-square test of independence
res_religion <- chisq.test(table_cluster_religion) 

res_df_religion <- tibble(
  statistic = unname(res_religion$statistic),
  df = unname(res_religion$parameter),
  p_value = res_religion$p.value
) %>%
  mutate(
    significance = case_when(
      p_value < 0.001 ~ "< 0.001",
      p_value < 0.01  ~ "< 0.01",
      p_value < 0.05  ~ "< 0.05",
      TRUE            ~ "ns"  # not significant
    )
  )

residuals_religion <- res_religion$stdres
residuals_religion

library(corrplot)
corrplot(residuals_religion, is.cor = FALSE)

```

```{r}
#| label: Diff in political affiliation btwn clusters?
#| include: false

# looking for statistical significance 

# create a contingency table
table_cluster_polaffiliation <- table(surveys_df$cluster, surveys_df$`U.s. political affiliation`)

# View the table
table_cluster_polaffiliation 

# Chi-square test of independence
res_polaffiliation <- chisq.test(table_cluster_polaffiliation) 

res_df_polaffiliation <- tibble(
  statistic = unname(res_polaffiliation$statistic),
  df = unname(res_polaffiliation$parameter),
  p_value = res_polaffiliation$p.value
) %>%
  mutate(
    significance = case_when(
      p_value < 0.001 ~ "< 0.001",
      p_value < 0.01  ~ "< 0.01",
      p_value < 0.05  ~ "< 0.05",
      TRUE            ~ "ns"  # not significant
    )
  )

residuals_polaffiliation <- res_polaffiliation$stdres
residuals_polaffiliation

library(corrplot)
corrplot(residuals_polaffiliation, is.cor = FALSE)

```

```{r}
#| label: Diff in political spectrum btwn clusters?
#| include: false

# looking for statistical significance 

# create a contingency table
table_cluster_polspectrum <- table(surveys_df$cluster, surveys_df$`Political spectrum (us)`)

# View the table
table_cluster_polspectrum

# Chi-square test of independence
res_polspectrum <- chisq.test(table_cluster_polspectrum) 

res_df_polspectrum <- tibble(
  statistic = unname(res_polspectrum$statistic),
  df = unname(res_polspectrum$parameter),
  p_value = res_polspectrum$p.value
) %>%
  mutate(
    significance = case_when(
      p_value < 0.001 ~ "< 0.001",
      p_value < 0.01  ~ "< 0.01",
      p_value < 0.05  ~ "< 0.05",
      TRUE            ~ "ns"  # not significant
    )
  )

residuals_polspectrum <- res_polspectrum$stdres
residuals_polspectrum

library(corrplot)
corrplot(residuals_polspectrum, is.cor = FALSE)

```

```{r}
#| label: Diff in sex btwn clusters?
#| include: false

# looking for statistical significance 

# create a contingency table
table_cluster_sex <- table(surveys_df$cluster, surveys_df$Sex)

# View the table
table_cluster_sex

# Chi-square test of independence
res_sex <- chisq.test(table_cluster_sex) 

res_df_sex <- tibble(
  statistic = unname(res_sex$statistic),
  df = unname(res_sex$parameter),
  p_value = res_sex$p.value
) %>%
  mutate(
    significance = case_when(
      p_value < 0.001 ~ "< 0.001",
      p_value < 0.01  ~ "< 0.01",
      p_value < 0.05  ~ "< 0.05",
      TRUE            ~ "ns"  # not significant
    )
  )

residuals_sex <- res_sex$stdres
residuals_sex

library(corrplot)
corrplot(residuals_sex, is.cor = FALSE)

```

```{r}
#| label: Diff in topic choice btwn clusters?
#| include: false

# looking for statistical significance 

# create a contingency table
table_cluster_topic <- table(surveys_df$cluster, surveys_df$topicChoiceAsString)

# View the table
table_cluster_topic 

# Chi-square test of independence
chisq.test(table_cluster_religion)

res <- chisq.test(table_cluster_religion) 

res_df <- tibble(
  statistic = unname(res$statistic),
  df = unname(res$parameter),
  p_value = res$p.value
) %>%
  mutate(
    significance = case_when(
      p_value < 0.001 ~ "< 0.001",
      p_value < 0.01  ~ "< 0.01",
      p_value < 0.05  ~ "< 0.05",
      TRUE            ~ "ns"  # not significant
    )
  )

residuals <- res$stdres
residuals

library(corrplot)
corrplot(residuals, is.cor = FALSE)

```

There may be significant differences in the demographic profiles of the clusters as shown in Figure \@ref(fig:fig-demographics-clusters); however, further analyses would need to assess statistical significance. \[i ran some out of curiosity but didn't bother to report here because it's probably not useful enough to be worth digesting the extra information\]

Consistent with Figure @ref(fig: fig-topic-choice-clusters), a chi-square test of cluster and focus topic revealed a significant association, χ²(`r res$parameter`, `r total_num_clean_subjects_in_data`) = `r res$statistic`, p \< `r res_df$significance`.

Standardized residuals indicated that participants in Cluster 1 disproportionately focused on human euthanasia, healthcare, police conduct, A.I., and mandating vaccines. These participants were significantly less likely to focus on deportation policies, capitalism, transgender athletes in sports, climate change, or the Israeli-Palestinian conflict. Cluster 2 disproportionately focused on transgender athletes in sports, climate change, capitalism, and deportation policies. These participants were significantly less likely to focus on human euthanasia, healthcare, police conduct, A.I., or mandating vaccines. Participants in Cluster 3 disproportionately focused on capitalism, deportation policies, and the role of the U.S. in the Israeli-Palestinian conflict. These participants were significantly less likely to focus on human euthanasia, healthcare, transgender athletes in sports, police conduct, or climate change.

## Discussion

This pretest study aimed to assess the comprehensiveness and underlying structure of a newly developed 19-item questionnaire designed to capture the core concerns contributing to Americans' unwillingness to engage in open, productive discussions on divisive topics. Our findings validate the instrument's relevance and offer a preliminary structural framework, while also highlighting key areas for refinement.

### Comprehensiveness of Concerns

Our first objective was to determine if the 19-item inventory comprehensively captured the reasons for reluctance to engage. Through a qualitative analysis of participants' free-response explanations for avoiding difficult conversations, we found that our items captured the majority of concerns but failed to capture pragmatic barriers and concerns about energy and relevance.

Specifically, participants frequently cited topic complexity/lack of knowledge as a barrier, not just fearing embarrassment, but a practical inability to engage due to insufficient information. We also found concerns about the perceived lack of relevance of the topic to the participant's personal life or ethical boundaries against speaking on issues outside of their firsthand experience. Furthermore, many expressed that it simply wouldn't be worth their emotional and/or mental energy.

In addition to these new constructs, our analysis revealed that two existing concepts warranted separation from the items in which they had been nested due to their high prevalence and distinct nature in the free responses. The first, damaging personal relationships, was often discussed in an intimate, personal sense that distinguished it from the broader concept of "social repercussions". The second, others' views not changing, was common  enough to be separated from the general item about others causing the conversation to not be productive.

Based on these results, we plan to add six new items to enhance the questionnaire's comprehensiveness and construct validity. These items are 

### Structure of Concerns

Our second objective was to explore the latent structure of the 19 concerns. An Exploratory Factor Analysis (EFA) suggested a three-factor solution, which accounted for `r total_var_explained_percent`% of the total variance. These factors were conceptually labeled as:

Emotional Vulnerability/Identity Threat (Factor 1): Concerns centered on internal psychological distress, such as negative feelings about the self or the world, and direct challenges to core beliefs or identity.

Conversational Efficacy/Unproductive Process (Factor 2): Concerns focusing on the quality, respectfulness, and utility of the interaction, particularly whether the process would be a waste of time or devolve into disrespect.

Social Evaluation/Reputation Risk (Factor 3): Concerns emphasizing external social perception, fear of embarrassment, public disapproval, and broader social consequences.

The factors were moderately correlated ($r$ = 0.17–0.60) and exhibited some cross-loadings, suggesting conceptual overlap and the utility of an oblique rotation (oblimin). Importantly, four items failed to load strongly on any factor, indicating they may represent highly idiosyncratic or situational concerns, though some conceptually belong to the Social Evaluation factor (e.g., "The conversation could become awkward or tense").

The structural distinction between Factor 1 (internal, emotional threat) and Factor 3 (external, social risk) is noteworthy, as many items seem conceptually close (e.g., "I could feel humiliated or embarrassed in the conversation" loaded on Factor 3 but relates to emotional vulnerability).

### Relationships to Willingness and Open-Mindedness

Our correlation analyses provide initial construct validation for our survey. A greater impact of most concerns was significantly associated with lower self-reported willingness to converse about the chosen topic, as measured by our slider questions.

Furthermore, most concerns were also negatively correlated with participants' open-mindedness. This suggests that individuals who feel more impacted by the various barriers to engagement also tend to rate themselves as less open-minded, which aligns with the literature suggesting that psychological and social anxieties inhibit constructive discourse.

### Cluster Analysis 

Our $k$-means cluster analysis suggested that participants' willingness concerns generally fell into one of three distinct clusters that were differentiated by the strength of the concerns' impacts on their willingness.

Cluster 2, the low impact group, comprised `r cluster_percentages$percent[cluster_percentages$cluster == 2]`% of our sample. Correspondingly, they also reported the highest average willingness and open-mindedness. Cluster 1, the moderate impact group, comprised `r cluster_percentages$percent[cluster_percentages$cluster == 1]`% of our sample. Cluster 3, the high impact group, comprised `r cluster_percentages$percent[cluster_percentages$cluster == 3]`% of our sample and reported the lowest average willingness and open-mindedness.

This clear differentiation by willingness and open-mindedness provides further evidence of the questionnaire's ability to segment individuals based on their barriers to engagement.

A particularly interesting finding is the significant association between cluster membership and the participant's chosen focus topic. For example, Cluster 2 (Low Impact) was disproportionately focused on topics like transgender athletes in sports and climate change, whereas Cluster 3 (High Impact) disproportionately focused on issues like capitalism, deportation policies, and the Israeli-Palestinian conflict. This suggests that the topic itself may be linked to the level of perceived threat and concern, which should be taken under consideration in the development of interventions for increasing willingness.

### Limitations and Future Directions 

The primary limitation of this study is the reliance on self-report measures for all key variables (concerns, willingness, and open-mindedness). The discrepancy between the Likert-scale comfort ratings and the slider-based willingness ratings (both asked about the focus topic), for example, highlights the inherent difficulties in measuring these constructs. Future work should integrate behavioral or implicit measures to provide a more objective assessment.

The EFA, while providing a clear structure, did not result in perfectly clean factors, as evidenced by cross-loadings and items that failed to load strongly. The six new items proposed for inclusion must be validated through a second factor analysis to confirm the underlying structure and assess the potential for developing subscales from the resulting factors.

In conclusion, this pretest confirms that the 19-item questionnaire, particularly with the proposed additions, is a valuable tool for measuring the multifaceted barriers to productive discourse on divisive topics. The identified factor structure and participant clusters will be crucial for tailoring our individual-level intervention to specifically address the emotional, efficacy, and social concerns that drive an American's unwillingness to engage.

## References
